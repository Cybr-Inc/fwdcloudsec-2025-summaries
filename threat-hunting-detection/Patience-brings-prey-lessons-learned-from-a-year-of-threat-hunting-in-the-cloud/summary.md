# Patience brings prey: lessons learned from a year of threat hunting in the cloud

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=cugwipMXHp0)

- **Author**: Greg Foss and Anthony Amran (Datadog)
- **Talk Type**: Security

## Summary

This presentation shares lessons learned from Datadog's internal threat hunting operations program and how their findings drive detection engineering initiatives. The speakers detail their approach to cloud threat hunting, methodologies for identifying advanced persistent threats, and practical techniques for building effective detection rules that eventually make their way into Datadog's security products.

## Key Points

- Datadog's threat hunting program directly feeds detection engineering efforts that improve their security products
- Focus on cloud-native environments and understanding attacker tactics, techniques, and procedures (TTPs)
- Emphasis on patience and persistence in threat hunting - "bringing prey" requires sustained effort over time
- Collaboration between threat hunters and detection engineers creates a feedback loop for improving security coverage
- Real-world hunting scenarios provide valuable insights for building robust detection logic
- Cloud environments present unique challenges and opportunities for both attackers and defenders
- Importance of understanding normal vs abnormal behavior patterns in cloud infrastructure
- Threat hunting helps identify gaps in existing detection capabilities

## Technical Details

**Threat Hunting Methodology:**
- Systematic approach to proactively searching for threats in cloud environments
- Focus on hypothesis-driven investigations based on known attack patterns
- Integration of threat intelligence to guide hunting priorities
- Continuous refinement of hunting techniques based on findings

**Detection Engineering Process:**
- Translation of hunting findings into automated detection rules
- Validation of detection logic against real-world attack scenarios
- Balancing detection coverage with false positive rates
- Iterative improvement based on threat hunter feedback

**Cloud-Specific Considerations:**
- Understanding cloud service logs and telemetry sources
- Identifying abnormal API usage patterns and privilege escalations
- Monitoring cross-service interactions and data flows
- Leveraging cloud-native security controls and monitoring capabilities

**Tools and Techniques:**
- Data analysis platforms for processing large volumes of cloud logs
- Custom hunting queries and detection logic development
- Automation frameworks for scaling hunting operations
- Integration with security orchestration and response platforms

**Key Lessons Learned:**
- Patience and persistence are essential for effective threat hunting
- Collaboration between teams improves both hunting effectiveness and detection quality
- Real-world experience is invaluable for building practical security controls
- Continuous learning and adaptation are necessary to keep pace with evolving threats

## Full Transcript

This talk is sponsored by Rock Steady and it's titled Patience Brings Prey: Lessons Learned from a Year of Threat Hunting in the Cloud. Uh, welcome Greg and Anthony. All right. Oh, slides are up. Perfect. Well, thank you all for uh for joining us for our for our talk today. We're going to kind of speed through a little bit. We have a lot to cover and in 20 minutes we may go a little over just forewarning but with that so uh so we're going to talk to you today about is kind of like data dog's internal threat hunting operations program a lot of like what we do to drive our uh detection engineering initiatives that ultimately end up in the product. So without further ado I'll introduce myself and then pass over to to Anthony. So um my name is Greg Foss. Uh I manage the uh product detection engineering team uh over at Data Dog. We kind of focus on workload security SAM uh and a little bit of threat hunting in between which kind of ties it all together. So I've been in the space for around 15 years. Seen a lot of different companies how they do threat hunting over the years. And so a lot of what we're going to talk about is some stuff that Anthony I have kind of seen and tried to bring into our overall detection engineering process. All right. Um, Anthony Renzazzo. I lead our detection engineering org uh at Data Dog for our security products. Uh, been in cyber security for about 20 years doing mostly thready threat stuff um, and security operations. Got my started as a a sock analyst and DoD, dabbled in a little bit of everything, incident response, uh, cyber threat intelligence, manage detection response, and I've been at Data Dog for last three years. All right. Um, just a quick few caveats up front. Um, our thread hunting is probably a little bit different than yours. Uh, we're doing it on a a pretty massive scale, looking at a lot of data across a lot of different organizations and environments. Uh, which means we're probably using a little bit different tooling. Uh, so what works well for us may not work as well for you, especially at the scale. Um, we'll get into some of our findings over the last year um, a little bit later. I just want to call out some of our metrics are a little bit biased. U naturally, very focused on cloud threats, greatest risk to our customer base. All right, next slide. So, uh, starting with why, uh, why do we thread hunt? Uh, first and foremost, protect our customers. Uh, you know, if we can find something that they don't know about and help them, uh, then that's great. Uh, you know, as a result of that, uh, we get to identify a lot of new attacker techniques, uh, package that up, communicate it out to all of you, uh, in the community. Um, and that, uh, beyond that, we get to develop a lot of new detections, uh, catch this stuff in a little bit more real time, and and improve our security products. So, uh, I think traditional thread hunting, uh, a vast majority of us would think of this as like hypothesisdriven hunting, right? Um, you know, ask a question, uh, get our arms around a bunch of data and try to find something interesting. Uh, we take a little bit of a different approach. Um, in in our work uh, and we'll talk about that uh, a little bit more. So, uh, what is cloud threat hunting? Uh, I think naturally this is, you know, really focusing on that that that cloud attack surface and and the various associated threats. uh you know some of the things that we're really focused on is you know fishing adversary in the middle compromises um particularly against the cloud identity providers uh cloud control plane compromise and then um you know some runtime compromise on on cloud compute um but again uh we're not really limiting ourselves to that hypothesis based hunting uh we'll give you a look at our um our our greater uh threat hunting program here in a second okay so what what does this look like uh you know that that attack surface those data sources that we're using uh you know four C's of cloudnative security to build you know web applications uh enabled by all those SAS applications that help us do that which are powered by those cloud identity providers uh one maybe many uh you know one thing to call out is you know don't discount you know the value of a lot of the cloud native uh or even BYOD security vendors and and the telemetry and signal that they provide we actually use that as an input to our thread hunting in many occasions so uh a pretty busy slide. Um but you know the one thing to focus on is is is our threat hunting inputs there in the middle. Um so first and foremost uh critical signals uh you know we we get a lot of security signal from across uh you know many different environments many customers uh where we can kind of take those back and sample a lot of lot of security signals to to to find bad things and then from there we can you know kind of deep dive investigate find new techniques. Um uh same with dete detection maintenance uh detection maintenance cycle we're you know continuously looking at improving detections uh in many instances uh you know we'll actually cover one later where you know we're performing detection maintenance and and and uncover some new activity. Um but of course we we have kind of those targeted thread hunts as well where we have sort of this backlog of uh different uh you know hypothesis driven hunts that we'll uh we'll run on occasion. Well, so with that like um one of the primary focuses of our team is really looking at threat detection. So the number one output for everything that we do is ultimately what can we bring into the product to improve our coverage to cover the things that we know about. Right? So when we look at threat detection, we're identifying known activities, known things that have been documented uh and really focusing on like you know things that have high efficacy in terms of like when you see this this is something that maybe you should respond to or at least investigate further. Whereas threat hunting, it's kind of a different approach. We're really looking for the unknowns, things we haven't seen, things we might not have observed before. We're trying to find uh kind of like interesting behaviors and capture these behaviors in a wide net and then distill that data down so that we can start to analyze and really kind of focus in on what may be interesting. Are there things that stand out? Uh you know, things of that nature. So really more behavioral contextual kind of focused approach. um trying to bring in all of these different elements that may give us an inkling of like okay this is interesting this is not and when we should move on uh and whatnot. So kind of an example to highlight this uh a little better. uh looking at the threat detection side, you know, pretty static sort of things we're looking at here like, oh, specific user agent strings. Um, and so this example, we're looking at like a compromised AWS access key, uh, for example. Um, or specific event names or maybe multiple combinations of these things happening in a way that has been observed by adversaries in the past or or things like that. Uh when we look at threat hunting for the same type of activity, we're really starting with this very wide set of data, very diverse set of data. Maybe looking at like general user agent strings for one example. Maybe uh specific patterns of access uh that we're observing. Uh maybe even looking at unusual regions like uh some of the times we'll find things because a region is being used that has never been used before uh by this customer, you know. Uh so all of these things where you know you all kind of in here have a lot of power because you know your environment inside and out much better than us from like a vendor standpoint. So taking that context in is is like such a key component here. So uh that's one of those things where we try and work very closely with our customers as well when we're both developing hunts and then also figuring out what rules are going to make for good detections that we can ultimately push into the product. So that's kind of our process of like uh threat hunting driven detection development essentially. And so you know what tools and techniques do we use? Uh a lot of stuff of course we use the data dog platform. Uh we try and use everything that we have available not just our our security products but the observability stuff is fascinating as well especially when we're looking at like honeypotss and stuff. Seeing what attackers are clicking on uh maybe requests made that go all the way down into database calls. super cool stuff that I'd never been able to see until like uh coming to Data Dog honestly. Uh but then of course for our back-end data analysis, we used a lot of Jupyter notebooks. We have a bunch of various Python uh kind of libraries that we use that allow us to uh look at a large amount of data at scale. So a lot of templates that we've built out and and things of that nature that really help us kind of do a lot of what I talked about on the last slide in a more automated and streamlined uh fashion. And then of course we work with our internal thread intel team. We have external thread intel partners and and things like that where we gather a lot of intelligence from to help us uh with areas that we should focus on. Um but then one of my favorite things that we've been doing for about the past year is these quarterly uh we we call them hunting parties. That's where we get a lot of people from diverse security disciplines across our not just our team but internal security and various other uh groups that we work with at data dog. And we get them all together and give them very specific focus areas of like, hey, you need to go find abuses of our platform or look into LLM abuse uh at scale in various cloud platforms. And we try and uh give them these specific focus areas and have them work with people they may not work with uh regularly and ultimately see what they what they find. And we've had a lot of really cool uh findings come out of this. A lot of things are beneficial both for us for internal security and beneficial for our our customers as well. So with that, I want to dive into a couple uh case studies here. One of which is the one uh that I thought was pretty interesting. We came across this uh actually while one of the guys on our team was doing some detection tuning. So he was looking at a very noisy rule. It was uh a downloading an image file. So we have a a rule for that because attackers do this, right? But the thing we're doing now is figuring out how we can okay let's chain this with okay they downloaded an image file but then they added an executable bit and ran it you know as a script. Um so kind of modifying things based on that. But long story short uh he was doing some tuning and came across this attacker who actually compromised a workload uh within a customer environment. And um the interesting thing about this one wasn't just like the image download uh component or anything like that uh was the fact that it actually took a long time going from initial compromise to actually the impact component. So uh I imagine a lot of folks in here have seen workloads get compromised and usually it's a smash and grab. They will break in drop crypto miners and leave or or maybe do some other things. This one they actually waited about 9 days until they actually deployed the crypto miner. So uh and that was only one facet of their attack. They also established pro proxy jacking so that they could monetize traffic going through this compromised server as well. Um and so uh the interesting thing here I I called out these folks you know shout out to Sequoia uh this research team that originally came across this thread actor uh called MIMO or Mimolet uh has been kind of their their name. Um they did a great write up. I definitely suggest you go take a look at that. But uh what was interesting about this is we saw them employing some different trade craft uh than what's been documented before. Um so before they were targeting craft CMS this time they were actually uh compromising uh Magento and uh they leveraged a PHP exploit against PHP's fast CGI uh in order to compromise this uh this server. We're actually not sure of the exact um plugin that they compromised. We're pretty sure it was like a third party plugin uh that was compromised, but uh that allowed them to get in. But essentially um you know once they got in, they employed some real interesting uh tactics um such as um you know hiding hiding all of their actions. They deployed a rootkit um called a lambdar is their is their rootkit technology that they deployed and this uses uh Linux cis calls in order to hide all of their executables in memory. So you couldn't actually see these things running. It'll hide them from the the uh the drive when you do an ls uh and those types of things. So kind of interesting components there, you know, much more interesting than the usual workload compromises that we see on a daily basis. Um on top of just the timeline uh for for how long it took them to uh to actually setting up multiple different uh persistence mechanisms. Um, I think they had like three different C2 servers that this was checking back in with in case one is killed, uh, and things like that so they could get back in. Um, but ultimately, uh, I don't want to spend too much time here because we do have a blog coming out on this where we'll go into a lot more more detail. Um, but essentially just one of the real interesting kind of findings that we had recently um, involving a compromised uh, workload. With that, I'll pass it back to you Anthony. Yeah. Um, thanks. So, shifting gears a little bit, uh, so, uh, we did, um, a little bit more of a a hypothesis-driven hunt here, uh, with with LLM abuse. Um, so, you know, we had been observing, uh, LLM abuse, uh, probably dating back to early 2024 was when we started seeing the some of the bedrock compromises. um uh other folks in the community uh were talking about in fact Permiso set up a high interaction honeypot um to where we actually got to see the action on objectives and some of the nasty stuff that the threat actors were doing. Um but ultimately what what we wanted to know was um were there other public cloud platforms being targeted namely you know Google Cloud and Azure um and in those platforms uh for those AI services were were any of those being targeted? um which APIs on those a uh AI services were kind of like ripe for abuse. Uh and then what we knew was that uh vast majority of these compromises were occurring through you know exposed access keys and GitHub or or wherever they were being leaked from. Uh so what did we find uh you know uh with you know with the hypothesis driven hunts we're casting a very wide net. We know the permanent access keys um are are what we're uh primarily concerned with at least out of the gate. Uh so we run this on our cloud trail data. we have, you know, a handful of um APIs uh particularly uh here against against Bedrock that we hadn't seen either written up or we hadn't seen um and and some of the compromises uh that we had already investigated. Um but uh yeah, what we found uh not a lot going on in Azure and Google. Um uh there there might be some Azure or Google people here that can speak contrary to this, but uh you know, our our theory being uh maybe the market share is a little bit smaller there and not as a ripe of a target. Um but when we did cast that sort of wide net, you know, we expected to bring a whole lot of data back that we'd have to sift through. Uh but the reality was we actually only had two hits across thousands uh thousands of different uh environments and and both of those just happened to be malicious. So um now we have a handful of indicators and we can start pivoting. So I want to talk a little bit about um some of our like pivoting tradecraft um and then uh you know how we go from one environment to the next. So um of all those APIs uh the list models kind of a reconnaissance um based uh you know API uh nothing novel uh but it was an API that we hadn't observed being abused before. Um so in this environment you know we have a singular access key um but with that access key now we have you know a variety of different indicators of compromise. We have uh you know VPN providers uh the IPs that were used uh related to those VPN providers um that we can then now start pivoting from. Um as an example here in this particular case this didn't happen but uh we've seen many instances where uh you know you uh single access keys exposed and many different thread actors kind of smash and grab that thing. So another instance uh you know being uh uh the the attacker will uh you know spin up a bunch of uh ECS clusters and deploy uh you know those malicious containers to run their crypto mining operations. But um more and more indicators of compromise that we have um creating persistence with you know unique IM usernames um even malicious accounts uh those may be a little bit short-lived because AWS will go after them. Um but the interesting thing here is uh you know once we do find one of these uh one of these indicators from from one of these compromises then we can sort of recast that net across the entire uh you know uh environment base. Um and then you know from there we actually see another compromise. Um and then we sort of go down this rabbit hole and find additional compromises which is uh pretty fascinating. Well, so with that, we want to show you guys kind of like a highle view of just some of our findings and and general lessons learned that uh you know, we've observed over the past year. So before I uh get into this, anyone guess uh you know what's the top avenue of uh of a breach into the cloud management plane? Feel free to just throw it out there. Credentials. Yeah, access keys, credentials. I think we had like multiple people hit on it. So yes of course you are 100% correct. So uh the interesting thing at our vantage point is we often see these actors uh after they've already say compromised an endpoint. We don't uh monitor endpoints directly ourselves. We often miss that component but we will see the adversary once they're gaining access to the cloud management plane. Uh by far and away you know the number uh number one thing we see across all of the intrusions we investigate is these compromised long-term access keys. Uh behind that we see a ton of business email compromise. Uh one of the big indicators there is they'll go in and start making rules to hide email from security and things like that. Um you know make a rule that's like a a single period or a bunch of spaces or something like that. Um so so that is the number two thing we see just all the time. And then after that a bunch of uh web application activity. uh we see a ton of credential stuffing across both uh web applications and then also just identity providers themselves like octa and things like that where people don't have a multiffactor configured uh and then endpoints we do have some of these intrusions we're able to trace back to a compromised endpoint a lot of these ones we we capture because we're able to communicate with the impacted customer and find out like how did they get in like oh they got in popped this system had an info stealer or they uh you know scraped the AWS credentials file or something like that. So that's often where it's like very anecdotal evidence at that point. Uh but then after that tons of web application injection attacks see a lot of SQL injection cross-ite scripting and of course the cloud focus focused SSRF attacks. Um you know tons of that uh activity as well. Um so you know the the benefits of being able to do all this thread hunting is we do get to identify all these sort of new uh new and unique uh techniques that we get to you know share out. Here's a you know just a handful of of things we've discovered just this year. Um you know the one thing I want to highlight in bold is those different inputs into thread hunting right whether that be critical signal analysis or thread hunting. Um there's a lot of different ways that that we we do end up finding some of these things. Um uh next slide. All right. Uh just real quick uh some of the pitfalls, some of the challenges that we've had um over focusing on uh you know maybe just hypothesis driven hunting too much uh we create a lot of bias. So um trying to mix it up um looking at different attack surface uh you know while we see credential access uh or exposed access keys being a challenge uh we we try to force ourselves to look at hey let's go look at some of the runtime activity let's look at the uh the identity providers um to to to continue to iterate. Um so uh you know don't look at the same thing over and over again expecting results. Uh there's many instances where we we run the same hunt and we never find anything. it's time to retire that or if it's uh you know a critical signal that you know we look at over and over again and it's the same attacker doing the same thing maybe it's time to um use that capacity a little bit differently. Um and then last but not least uh don't underestimate the value of those atomic indicators um especially malicious infrastructure. Uh you know one thing we've noticed um as opposed to you know a long time ago with crimeware where infrastructure was constantly being disrupted uh a lot of this infrastructure has not changed. So we're continuing to see the same uh threat groups using the same infrastructure, the same uh same TTPs as opposed to uh the you know the what the pyramid of pain has historically told us like uh most importantly I think is what what we can glean from this. So uh so I wanted to highlight here just uh you know some of the improvements we've made based on threat hunting findings. So as you can see kind of like left to right here where we kind of started from where we were initially uh doing a lot more focused threat hunting within our team. Uh you can see that first major quarter where we really overindexed on this. Uh tons of detection gaps noticed. So it was a big thing where we're realizing like okay how are attackers getting in? Where are we missing them? Uh and you know why why is this something we didn't catch? So you can see uh that uh that part of the bar chart getting smaller quarter over quarter which you know we're we're very happy about. But at the same time there's a lot of other issues uh you know that that can uh really impact the ability to uh detect these threats that stem past the detections themselves. So we track a lot of things like is this an issue with the product? Is this something where we need additional visibility from the agent or are we missing a log source? Uh and those types of things. So, so we try and capture all of this additional data as well so that we can further improve our programs uh going forward um you know kind of using this data to to really drive uh the the detection side of the house and so of course I had to talk about AI of course um but with this one of the cool things here I think um like like honestly it's one one of the really neat things for for us to work on because it's very new uh to to me personally it's something I haven't had the chance to do before But we're augmenting our threat hunting process now with AI to basically help us figure out where we should focus, curate large data sets and and kind of distill things down for us, but also feeding back into our AI AI models so that we can do better detection so that we can actually do agentic security. Um, which our PMs will be very happy that we mentioned in our talk. Um, but essentially, you know, that's where a lot of these companies are going is like how do we automate this? How do we index on uh you know what we're already learning and then not just feed garbage in. We really need to feed quality data in that's curated that's uh confirmed uh because we don't want to have a bunch of a bunch of garbage coming back out. So it's really that threat hunting uh to detections to the AI uh sort of loop um there. And that's a big thing our team's been focused on uh quite a bit lately. But with that um thank you all very much. We didn't go as over as I thought either, so that's good. Um, but we'll definitely like, you know, make these slides available because I know we breeze through like some of the some of the charts and stuff. Um, and then, um, you know, we have some blogs and stuff coming out too that are going to go into more detail and some of the threats we talked about. Um, with that, you know, we'll open it up to, uh, to any questions you all might have. So with regards to um credential theft still being a big big vector, do you do you guys have any sort of trends on are things changing in terms of what people are doing with those credentials? Right. Crypto mining is obviously the been around for a while, but are we seeing more movement into the AI services, those sorts of things? Yeah, good question. Do you want to take it or uh Yeah. Um I yeah I I think since since yeah the the AI services have become a thing. It is definitely um becoming more of a a thing. Uh the the the proxy jacking um has definitely picked up as well. Um I think that probably has to do with the es and flows of the valuation of cryptocurrency. Um, you know, a couple of the things that we didn't talk about was probably uh some of the more targeted attacks that are a little bit more sensitive, but there's there's definitely that stuff going on where um it's a little bit more hands-on keyboard, stealing data, things like that. The um and especially like you mentioned the AI uh kind of focus, we've seen a big uptick in a lot of adversaries targeting um you know, the the AI models that are that are available so that they can you know, use free tokens and things like that. um the bedrock example Anthony talked to that's like one of one of many that we've seen. Uh a cool thing about that too is we were tracking uh some of this activity and then we wrote a blog on it and right after that blog came out the adversary changed some of their behaviors. So we like to say it was because they read our blog but it's probably not true. Um but uh but still interesting. It's something that I think is is evolving very rapidly. It's like if you open up cursor or or any of these uh development apps now, there's areas where you can plug in like a bedrock API key. And so those uh so if you're able to come across those credentials, there's a lot of unique ways you can uh or attackers can leverage those. Now, right? Uh one last round of applause for the speakers.
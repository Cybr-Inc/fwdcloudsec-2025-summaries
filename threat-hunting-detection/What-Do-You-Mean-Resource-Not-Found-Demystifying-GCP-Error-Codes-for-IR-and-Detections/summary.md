# What Do You Mean, ‘Resource Not Found?’ Demystifying GCP Error Codes for IR & Detections

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=8cScKAzuEz0)

- **Author**: Gabriel Freed
- **Talk Type**: Cloud Security / Incident Response

## Summary

This talk reframes GCP error codes, typically seen as noise, as valuable signals for early threat detection and incident response. Gabriel Freed explains that by understanding the structure of these errors, which are based on the gRPC protocol, security teams can move beyond simple "success" or "fail" monitoring. The session provides a detailed breakdown of GCP error objects and offers practical strategies for building high-fidelity detections for activities like reconnaissance, permission probing, and resource abuse.

## Key Points

- **Errors as Signals**: Seemingly benign errors can be early indicators of malicious activity. For example, multiple "resource not found" errors can indicate enumeration, while repeated permission failures from a service account could signal a stolen key being used for probing.
- **gRPC is the Foundation**: GCP's internal services communicate using gRPC, and its error reporting is standardized by the API Improvement Program (AIP 193). Understanding this foundation is key to interpreting the logs correctly.
- **The Structure of an Error**: A GCP error has three main parts: `code` (a canonical number), `message` (a human-readable string), and `details` (a rich object containing the most valuable context).
- **The Value is in the Details**: The `details` section contains the `errorInfo` object, which specifies the `domain` (e.g., `compute`, `storage`) and `reason` for the error. This allows for the creation of highly specific and context-aware detections.
- **Mapping to HTTP**: gRPC error codes can be mapped directly to familiar HTTP status codes (e.g., gRPC code 7 `PERMISSION_DENIED` is like HTTP 403 `Forbidden`), making them easier to integrate into existing playbooks.
- **Tracking Long Operations**: For asynchronous tasks, the `operation ID` is a critical field that can be used to trace an error back to the initiating user, even when the user's identity is not present in the final error log.

## Technical Details

- **Protocols and Standards**:
    - **gRPC**: An open-source RPC framework developed by Google, used for high-performance communication between backend microservices. It uses **Protobuff** to serialize data into a compact binary format and runs over **HTTP/2**.
    - **AIP 193**: The Google API Improvement Program proposal that standardizes the structure and codes for errors across all GCP services, ensuring consistency.
    - **ALTS (Application Layer Transport Security)**: Google's internal mutual authentication and transport encryption protocol used between the Google Front End (GFE) and backend services.

- **GCP Error Object Structure**:
    - **`code`**: A canonical error number from 0-16.
        - `3`: ALREADY_EXISTS (e.g., trying to create a firewall rule with a name that's already in use).
        - `5`: NOT_FOUND (e.g., querying for a non-existent resource).
        - `6`: ALREADY_EXISTS (another example, resource in use).
        - `7`: PERMISSION_DENIED (user not authorized).
        - `8`: RESOURCE_EXHAUSTED (e.g., a zone's resource pool is depleted, often seen in cryptojacking).
    - **`message`**: A developer-friendly string describing the error.
    - **`details`**: An object containing more granular information, most importantly the `errorInfo`.
        - **`errorInfo` Structure**:
            - **`type`**: Usually `google.rpc.ErrorInfo`.
            - **`domain`**: The service domain where the error occurred (e.g., `compute.googleapis.com`, `storage.googleapis.com`, `serviceusage.googleapis.com`).
            - **`reason`**: A specific, machine-readable string for the cause (e.g., `RESOURCE_AVAILABILITY`, `ZONE_RESOURCE_POOL_EXHAUSTED_WITH_DETAILS`, `CONSUMER_QUOTA_OVERRIDE_TOO_HIGH`).

- **gRPC to HTTP Status Code Mapping**:
    - `0 OK` -> `200 OK`
    - `5 NOT_FOUND` -> `404 Not Found`
    - `7 PERMISSION_DENIED` -> `403 Forbidden`
    - `8 RESOURCE_EXHAUSTED` -> `429 Too Many Requests`
    - `13 INTERNAL` -> `500 Internal Server Error`
    - `16 UNAUTHENTICATED` -> `401 Unauthorized`

- **Handling Long Operations**:
    - For operations that take time (e.g., creating a VM), the process is tracked with an `operation ID`.
    - An error log for a long operation might have a `null` principal email.
    - To find the initiator, query for the `operation ID` and find the log entry where `operation.first` is `true`. This initial log will contain the principal who started the operation.

- **Detection Logic Examples (PySpark Pseudo-code)**:
    - **Reconnaissance**:
        - *Naive*: Alert on >15 errors of any type from one IP in 30 mins. (High False Positives)
        - *Better*: Alert only on `status.code == 5` (NOT_FOUND). Trigger when a user/IP generates a high distinct count of not-found errors against different asset names, indicating a dictionary-style enumeration scan.
    - **Permission Probing (Service Account Abuse)**:
        - *Naive*: Alert on >30 `PERMISSION_DENIED` errors for one user. (Noisy, lacks context).
        - *Better*: Alert when a single user principal generates `PERMISSION_DENIED` errors across a high number of distinct API methods or services in a short time. This indicates the user is systematically checking what they have access to.
    - **Misconfiguration vs. Malicious Activity**:
        - *Better*: Differentiate between a single caller failing on a disabled API (likely a misconfiguration) and *multiple callers* failing on the *same* disabled API in a short window (could indicate attackers probing different stolen credentials against a target service).

- **Implementation Steps**:
    1.  **Map Codes**: In your SIEM or log analysis tool, parse the error object and map `code`, `message`, `domain`, and `reason` into their own queryable fields.
    2.  **Baseline Errors**: Establish a baseline of normal error rates for key principals (users and service accounts) to detect significant spikes or deviations.
    3.  **Stitch Operations**: Use the `operation ID` to link related logs together to ensure you can always attribute a failed long-running operation to its original caller.

## Full Transcript

Hey, good morning everyone. Welcome to the second talk of the day here at Forward CloudStech. Uh for this talk, we have a remote speaker to present for you. But first, we'd like to again thank all of our sponsors who make this event possible. In particular, we'd like to thank our bronze sponsor, Chaser System. So, go say hi to them out uh in the booth. All right. So, our next talk is what do you mean resource not found? Demystifying GCP error codes for IR and detections by Gabriel Freed. So please give a round of applause. Hello everyone and welcome to what do you mean resource not found where today we'll talk about GCPR codes understanding them and how we could use them in incident response and detections. So, some of us, most of us or part of us might have been in this situation, right? You cleared out your XDRs, your SIMs, or whatever system you have, and you remove the critical, the high, and the medium alerts, right? You're left just with the infos. But what if there's a firestorm of errors happening behind the scenes, attackers mapping their way through these failures, right? We usually focus on positive actions to detect and investigate. But what if we could have stopped or detect this attack before it happened via error codes? Or if we're building a timeline for an incident response, find out when things actually started happening. To do this, it's critical for us to understand GCP error codes. So today I'd like to talk about errors and how in a different context they could be an early warning for something that might happen. We'll talk about protocols and error objects. What is an error object? What is gRPC and why is it relevant? We'll talk for a second about long operations and missed identities. And finally, we'll look at some practical examples of how we could utilize error codes for detections and incident response. My name is Gabriel Frerieded. I'm a principal security researcher at MIGA. I have over 10 years in cyber security. This is my second time co talking at cloudsc and I really appreciate the remote option and super excited and I'm a whiskey dude now also rum rookie. So good luck for me with that. Okay, let's start talking about errors, right? We have them in our environments all the time. You mistyped a resource name and it's not found. you're missing IM permissions and you can't access anything or someone overexhausted a service, right? But what if we take our assumptions of what these errors are and look at them differently? What if instead of a user mistake for a path, we see multiple resource not found on different random bucket names in a short amount of time? Could this be an enumeration script using a dictionary to map out our resource resources? What about IM roles? Right? Let's say we had a service account that never failed, right? It was running automatically. Things were good and impersonated, but now we see this specific service account is failing on multiple different services. Could this indicate maybe someone stole the service account key and is probing their permissions? or if we can't create a VM anymore because some zone or service exhausted, right? Would this maybe indicate someone using it for cryptojacking and they completely filled out our zone with high expensive machines? I don't know. But if we think about it in this way, we might see different context to errors. So now let's learn about these protocols and the error objects. To start, we'll need to talk about something called gRPC and AIP 193. Before 2015, Google needed a specific protocol that will be efficient, reliable, and fast for their backend and microservices. So they came up with something called Stubby, which is based on something called a Protobuff and HTTP2. A Protobuff serializes data into like uh binary format, making things super small, chunky, and fast. And HTTP2 allows multiple channels on like one connection, making this reliable and fast. But around 2015, Google takes their internal stubby and make an open- source version called gRPC. In 2016, it's GA for cloud pubsub. And in 2018, we have something called AIP 193. AP stands for API improvement program and 193 is the one talking about standardizing error codes. Imagine we have multiple different services and each one reports a different error in a different way. How could you make these systems work together? You have to have one concise standard. And in 2020, there's enhancements to something called error info, which is a section within the details of an error code. And today, we're talking about gRPC and the implementation of AAP 193 and GCP audit logs. But we might think, Gabrielle, why are we even mentioning gRPC? So when I was looking at error codes in GCP audit logs, I noticed they were very similar to the gRPC error codes. So I was trying to understand where does gRPC come in in a general request of in a general request to GCP. So my understanding is we have our client our client sends the API request to something called the Google front end, the GFE. This is where we have Google's reverse proxy DOS protection and um the load balancing right then the GFE encrypts this information using ALTS which is like a mutual o TLS implementation by Google and then the internal services talk using gRPC or stubby and here we understand that if they're using gRPC or stubby for their communication their errors are loged loged based on this AIP 193 standard and that is where it ties in. So now that we understand gRPC's connection, let's see how we define an error code in GCP. An error code in GCP has three sections. We have the code, the message and the details. The code is a canonical number, right? Usually 0 to 16 and it will tell us like zero okay, five not found, seven permission denied, etc. We also have the message which is a human readable text more developer friendly for their debugging reasons but I've noticed some services don't force you to have this message also in general different services I'm assuming were written by different times and different people so the structure is still there but not all of them are populated and the detail section the detail section is our treasure chest this will contain something called the error info and extra information now let's look at some examples of these types of errors. We could see here, for example, we have a code three that says that this network endpoint group resource is already being used by a different backend service. Pretty understandable, but we could see what was affected on what. If we look at this code number six, it says that this specific firewall is already exists. Now I've noticed that some offensive tools when they try to abuse certain resources once they have credentials they try to create resources with the same name sometime unless someone changes them. So a different thought we could have on this for example could be if I see multiple already exists could this be an abusive tool trying to abuse something here? It's something to think about, right? Code five, we see a specific resource wasn't found. And code seven, a user wasn't authorized to perform this. But as I said, different services have different ways to explain things. Look at this code 7. We see the exact permission. Now, assume you're needing this for an investigation and you find out exactly what an attacker needed. and maybe you're now going to look if they gave themsel or assigned themselves these permissions or if an attacker sees this, they know what they're missing. We could see how looking for the code and the message could give us more context. So now let's move on to the detail section that gives us a bit more information. The detail is built out of type, domain, and reason. The type will usually write protobuff or air info. I'm assuming these are based on before the enhancement or after. The domain will give us the section in GCP which is relevant. For example, we could have compute, storage, service usage, etc. And the reason will give us a text of why something happened. The big difference is that if we look from above, the error code tells us the what. The error info will give us usually a how, what, and when, like a bit more information, less of the when. So now let's look at an example here. Here we see a code six where the message says an IP was used by another resource, but the details will tell us which IP. Now, this could be good for an investigation if we're trying to see if someone is abusing a specific IP that later on we saw and more. Error code five, we see resource not found, but the details tells us the resource name, the type, and the scope. A lot of useful information here, right? And error code 8. Now here's a classic case of the error info. We could see we have the domain is compute. The meta data gives us a lot of information and we have a re reason here which is resource availability. Now think if you're writing an detection or an IR playbook, you could say I'm only interested in looking at specific domains for errors or specific reasons. Right? If we could parse this right in the right way, we could have better incident response and better detections. Let's look at this re uh error code 8. It says that our zone resource pool was exhausted what we mentioned earlier, right? And the details gives us a lot of information and the message also gives us the context we mentioned before right we can k create AVM on this specific zone cuz uh this zone is exhausted right like we said if we think about it differently cryptojacking I don't know and lastly let's look at this code 7 if we're just going to see okay message says multiple permissions so that we're trying to get all these things but we could see this error info tells us that it's service usage and the O permission denied. So connecting this with the message, we could actually see the attacker was trying to enable multiple services and not trying to do a different action. What are they trying to enable? What are they trying to abuse? But now we could parse based on this error. Now a question to all of you before I move on. Aren't these errors kind of similar to I don't know HTTP status codes? Yes, they are. We could map most of them to HTTP status codes. And why am I saying this? Let's say you might have a specific playbook or incident uh play uh response technique or detection based on HTTP error codes. We could map them out to gRPC error codes. For example, the 0ero to the 200, the five to the 404, 7 to 403, and even more here, right? We have the internal the 13 to the 500, and 16 to 401. Now, if you could use this mapping to a teach yourself more about gRPC error codes and improve your detections, this is great, and we could do it. So, what did we learn in this section so far? We learned that gRPC runs behind the scenes in GCP services. We learned that we have a code with three sections. We learned about the details that has three main sections which interest us. And we learned that we could map these out to HTTP status codes. So now I'd like to talk about a special case with long operations. What is a long operation? Right? long operations or operations that could last minutes to hours. Think about builds, big exports, right? You're creating big uh like VMs, networks, all these different things. Think similar to Azure when you're building when you're creating a VM or you have something and you have the started, running, and end that's kind of like a similar long operation. Now these operations probe their status through a specific tracking ID which is called the operation ID. Now we might think why am I even mentioning this? So I've noticed when looking at the GCP error codes that cases where it wasn't anonymous GCE or GKE internal IPs which kind of relates to last year's talk. We have cases where our principal email is null. Basically, we're looking for the user that initiated the operation and failed here and it's not in the logs. What I noticed is that in long operations, we could tie this based on the operation ID. In long operations, we have a section called the operation, right? This will contain an operation ID, basically what we were probing the status for. And this is a unique tying point. the producer which is the service a first and a last mainly is this the first operation starting and triggering this long operation or the last naturally if we have an error this will be the last so the way I notice that we could find this principal email is tied based on the operation ID where the first is true this way if we're investigating an error that comes from a long operation we could trace back the initiating user and that is our little section on long operations with error codes. But now let's get a bit more practical, right? I would like to talk about three examples, right? We'll take them in pispark pseudo code. We'll come up with an idea, see how we could create create naive detections and then a bit more specific detections using context and our error codes. Let's see if we could find reconnaissance. Right? We're looking for the multiple uh someone's looking for multiple resources. Right? But if we don't classify which error code, typos could also be part of these errors and we'll need a bit more context. Let's see what I mean. If we have like a naive detection, super naive here, right? We're saying the status code isn't zero. basically any error. We are looking for a specific IP with over 15 errors in 30 minutes. This could cause multiple high um false positives and it has zero incident response context. Why are these errors here? Are they automatic? Are there an issue? This is too naive. But what if we say, you know what, we're only looking for resource not found. And then we're going to look for a minimum amount of specific assets that weren't found based on the user, the email, and the project. And here we're also adding some, you know, score based on how broad the scan is. This is much more specific to the context of what resource wasn't found. We're looking for a specific attack case here of probing. Let's look at some of the stats we have here that we could clearly see this good detection has yields way less results. Right? Um and we reduced all these false positives and got to a point of 2.04%. Now you might say that was way too broad because we looked for any errors and I agree with you which is why we'll move on to the next example. In our next example, I'd like to talk about misconfigurations versus malicious operations. Let's assume an attacker found multiple service account keys and they want to target a specific service and they're trying to probe which service account has access to this service. Now if we just look for permission denied and we'll have disabled APIs which also when we have a disabled API or service unavailable it will be sometimes under permission denied we could lose the actual attack. So how do we differentiate right a naive approach would be any permission denied for a specific account let's say here over 20 and 15 minutes right none of these are production ready but these are just like thoughts of how we could approach this this could be too noisy and again we don't have any context for incident response or what they failed to get but what if we say okay if it was a service disabled only for one account this might be a misconfiguration, right? Someone wrote something wrong. But what if we say we have multiple callers for the same API and same reason we're extracting from the error info in a short amount of time? Could this indicate this case of probing different accounts to attack a specific service? This one gives us much more context and much more uh being much more specific about this subject. Here we see we only found six alert on one of the environments and the rest couldn't find anything. Now again, this isn't production ready, but we could see how playing around with specific context, right, of what we're looking for using the error info if we can and different usages of our error codes could yield much better results. For our final example, I'd like to talk about the a case we mentioned. A user found a service account key and they're trying to probe their permissions. Again, if we just look for permission denied, we could have multiple cases of false positives and no context. A naive approach here would be pretty similar 30 denials for a specific user from an IP. Again, this won't help incident response and could cause high false positive rates. But what if we say we're looking for specific permissions based on the errors or the API, the method name called, and then we're using that in context with the user in a time and saying we're only looking for a specific threshold. Here we could barely see any results, right? We see multiple bad results. The reason for this is because we actually had 0.13% of this good detection and we also found a scanning of permissions actually with this idea. Now again these are just ideas to spark your mind. How could you use error codes for specific detections or incident response playbooks? So now let's talk about how you could implement this in your environment. Right? Start by mapping your codes into your into your systems. you're using them as fields. Don't keep them as that, you know, uh, structure on the side. Try baselining callers and their errors. Does this entity cause a lot of errors? Is there a spike? Is there no spike? You know, use baselining and all the relevant techniques and try to stitch the operation ID so you won't miss any entities. So, our key takeaways from this small section is that we could use these for telemetry. The structure of AIP 193 gives us a better way for parsing these errors and detections and we could reduce false positives. As a summary, we noticed that if we look at error codes differently, we could have a bit more value. We learned gRPC is part of GCP. We learned the flow of of a request. uh the structures of an error code, HTTP status codes, a bit about the long operations, and how we could implement this for detections and investigations. But what now? Tomorrow, start looking at your environment, tag those error codes. In a week from now, conduct an incident response playbook using these error codes, and in a month, write some error-based detections for your GCP environment. But if there's one thing I could have you all take away from today is if you catch the clouds no we could prevent the attackers go. Thank you very much. I was Gabriel Freed. Right. Uh thank you very much Gabriel. Um again we have a thread in our Slack channel. So anybody remotely or here who has any questions please feel free to leave leave them. if you're here and want to ask him live, uh, just go ahead and raise your hand. Um, I'll get started by asking a question of how would you want Google to, for example, change their error codes, error messages, structure them differently to make it easier to build these kinds of detections and how do you think about like applying that to our own firstparty applications that our in-house teams build? Yeah, so thanks and it's a great question. Um, I think the way Google currently uh builds their error codes is inherently way better than I've seen in well at least in other places and you could use them easily to parse based on different structures like for example when using pispark but AIP 193 that we've seen has been constantly going through changes like the last updates been on 2024. So, Google seems to be right on it, especially because gRPC as a protocol is open source and very driven by the community. They had like a whole uh I think it was um a 10-year event a while back and they had a whole conference for it. So, it keeps on going. And how could we use them in our systems is something I mentioned. Try to take them out of that box of the error code, right? We have this in many different vendors also like in Azure, you have the code and the message, but it depends on where it was logged. Try taking them taking them out as different fields when you're parsing stuff to make them a more fast and b more critical and visible for your eye because they could sometimes get lost through a bunch of data. Right. Thank you. Um, any other questions from the rumor Slack? All right. With that, uh, one more round of applause for Gabriel, please.
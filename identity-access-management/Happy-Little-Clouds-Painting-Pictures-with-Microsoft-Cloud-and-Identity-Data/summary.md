# Happy Little Clouds: Painting Pictures with Microsoft Cloud and Identity Data

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=nwYzVTL8Y4Y)

- **Author**: Matt Greyber (Red Canary)
- **Talk Type**: Security

## Summary

This presentation establishes a universal methodology for assessing Microsoft cloud and identity data quality from a security perspective. Using a Bob Ross-inspired approach to "painting pictures" with data, the speaker demonstrates how threat researchers can serve as storytellers by applying minimum viable storytelling principles to transform complex log data into accessible narratives for defenders and responders.

## Key Points

- Threat researchers must act as storytellers to make security data accessible to defenders
- Detection requires ability to discern between benign and malicious events, not just observation
- Minimum viable storytelling methodology: who, what, when, where, whence, and how
- Microsoft log correlation requires understanding unique token identifiers and session IDs
- Practical correlation examples: suspicious inbox rules and Azure role assignments
- Bottom-up detection approach focuses on specific attack technique requirements
- Quality security data sources must provide sufficient context for confident response

## Technical Details

**Minimum Viable Storytelling Framework:**
- **Who**: Identity that performed the action
- **What**: Operation performed without additional detail
- **When**: Date and time of action occurrence
- **Where**: Location where affected resource resides
- **Whence**: Origin location of the actor
- **How**: Means by which actor affected the resource

**Microsoft Identity Correlation:**
- **Unique Token Identifier (UTI)**: URL-encoded base64 representation of access token ID
- **Session ID**: Group of related UTIs spanning up to 90 days
- **Direct correlation**: UTI to specific sign-in events
- **Cross-correlation**: Session ID for holistic timeline analysis

**Data Source Correlation Patterns:**

*UAL to Sign-in Events:*
- Office 365 UAL events require UTI decoding for correlation
- Session ID provides direct correlation capability
- Device property session ID consistent across sources

*Azure Activity Events:*
- Claims.uti field correlates to unique token identifier
- One-to-one correlation with decoded ID field

*Cross-Log Analytics Correlation:*
- Sign-in logs, non-interactive logs, service principal logs
- Correlation ID works across sign-in event types (rare Microsoft success)
- Graph activity logs provide additional correlation opportunities

**Case Study 1: Suspicious Inbox Rule**
- UAL new inbox rule event analysis
- Correlation through UTI to non-interactive sign-in
- Further correlation via correlation ID to interactive sign-in
- Timeline building for comprehensive incident response

**Case Study 2: Azure Role Assignment**
- Role assignment write activity event analysis
- Multiple Azure activity log sources available
- Choice between different log sources based on data quality needs
- Principal resolution and scope identification requirements

**Detection Engineering Approaches:**

*Top-Down (Traditional):*
- Apply ML/AI models to existing data sources
- Find needles in existing haystacks
- Extract maximum value from current log sources

*Bottom-Up (Recommended):*
- Identify specific attack technique to detect
- Reproduce technique in controlled environment
- Identify necessary data sources for observation
- Develop detection logic based on technique attributes

**Data Quality Assessment Criteria:**
- Ability to populate minimum viable storytelling template
- Presence of correlation opportunities with other data sources
- Sufficient context for confident security decision-making
- Distinction between event ingestion time vs occurrence time
- Enrichment possibilities for missing data elements

**Microsoft-Specific Challenges:**
- Inconsistent correlation ID implementation across products
- Organizational silos reflected in data source incompatibilities
- Missing correlation opportunities in Entra ID audit logs
- No direct correlation from audit events to sign-in events

**Practical Implementation:**
- Automated correlation workflows for incident response
- Timeline construction across multiple Microsoft data sources
- Evidence quality validation for security operations
- Responder accessibility without subject matter expertise requirements

## Full Transcript

Good morning everyone. Thank you all so much for being here. Hope you're looking forward to an exciting two days. There's going to be a lot of great talks, a lot of folks to meet, and a lot of fun to be had. Um before we get started with all that though, I'd like to thank our sponsors who make all of this possible. Uh in particular, Hannibite, uh who you can find out more information about from their booth in the hall outside. Um if you have any questions during the talk, feel free to save them and at the end I'll come around with the microphone. you can ask and we can uh save them for the video. Uh joining us to the stage right now is Matt Greyber, a threat researcher at Red Canary and he's going to be presenting Happy Little Clouds: Painting Pictures with Microsoft Cloud and Identity Data. Please give a warm welcome to Matt. Thank you, Nick. All right. So, what I'm going to talk about a lot today is uh let's see, can we get the slide deck here? Is the role of threat research as storyteller and specifically in this case applying that to Microsoft cloud and identity logging. But really what I'd like to establish here is a universal methodology for assessing the quality of data sources and using those to tell a compelling story that responders can use to respond to threats confidently. So who am I or who are we? As Nick said, I'm a threat researcher at Red Canary. I've been in the industry for quite a while now. Um, perhaps a little known fact about me way back when, as I was kind of starting my infosc career, I gave a talk at I think it was the third Derby Con with a co-orker uh Chris Campbell where we coined the term living off the land. And since then, I've always had a fascination in leveraging what a vendor gives you and making the most use out of that. So whether it was way back when uh working on on the offensive side and figuring out ways to leverage novel built-in utilities in Windows and abuse them. Um now we're going to be kind of talking about living off the logs, right? Um making the most use out of the vendor logs that were that were given. In this case, Microsoft, which uh both for better and for worse. Um, I'm a huge fan of Bob Ross. Um, I only wish I could have a head of hair like that. Um, so, uh, I'm channeling Bob Ross here to, uh, as as he might say, uh, give me a head of almighty hair. Um, so Bob Ross, uh, has has anyone here like watched his show? Like you can watch it for free on, uh, like included on Amazon Prime. Um, it's my favorite show to watch with my daughter. She's really into art and uh so yeah just thought it would be a good opportunity to to channel him and uh get some inspiration from him as we go along here. So what would I like to achieve here? Again, I mentioned um threat research, security research, like that can be a very ambiguous role, right? And there are many different applications of us as researchers, whether it's investigating uh discovering novel offensive techniques, finding v vulnerabilities, um understanding what threats are actually doing in the wild. Um all very practical use cases. We're going to be scoping this down in particular to threat researchers role as storytellers for defenders. And I wanted to lay some very basic groundwork here. Um I think it's helpful uh that we all speak a common language together and make sure that we're um you know not confusing terms. And so as we talk about detection engineering, I just want to establish some very basic principles that at least I think about a lot when I'm tackling a new problem like trying to detect a new attack technique. And as I mentioned, we're going to discuss uh how I go about assessing the quality of data sources. And I hope you'll be able to benefit from that methodology as well. and then how to correlate data sources across the Microsoft suite of products. Again, for better and for worse, it can be challenging. And then, uh, I typically struggle with any talk or blog post that just remains too abstract. While there are lots of great ideas out there, um, I have a hard time if I can't actually be hands-on and apply those methodologies that are shared. So, we're going to do exactly that and have two practical case studies. One where we'll investigate a suspicious uh inbox rule creation and then a privileged Azure role assignment. But as I mentioned, I want this to be universally applicable. So, don't think that this will only apply to these two arbitrary use cases. This is globally applicable to any attack technique. All right. So the best data in the world honestly is meaningless if it can't be correlated and it doesn't tell a story. Right? So uh as you all know like we're swimming in data all the time but if we can't identify the value from it like find those needles in that haststack and present it in a compelling way that is actually accessible to defenders and responders that we're not really doing our jobs too well. And so as that role of threat researcher, as that storyteller, uh we are helping out detection engineers and responders, right? So as a detection engineer, I like to think of them as their primary role is developing those trip wires and we can help inform that by establishing what those requirements are for defining what those uh trip wires are. And then response requires a story, right? It has to be presented in a way that is accessible to defenders because not all of them and they certainly should not be expected to be subject matter experts in every esoteric attack technique. Right? Right. So as threat researchers who are those subject matter experts in the individual attack techniques, how do we present it in a way that's actually accessible to defenders who have to defend against a myriad of attack techniques and not just in the Microsoft cloud and identity space but you know across cloud providers and endpoint and whatnot. All right. So uh I often feel in my role like the guy on the left right. So trying to make sense of all the data sources, how they're connected and trying to present and often failing to articulate what is actually happening. Um, more often than not when someone explain when someone asks me to explain to them like what's happening in in a data source like I'll go through like each individual field and uh just start rattling off all these um things that I'm seeing making assumptions that who I'm speaking to knows the data sources as well as I do. That usually doesn't work out too well. But our goal as threat researchers that you too can paint almighty pictures. This is just a good reminder to us that everything that we should be doing should be in the interest of making our work more accessible to others. Right? It's a lot of fun to go deep into those rabbit holes, but we need to come to the surface every once in a while and present our findings in a way that's accessible. And everyone can do this. If I paint something, I don't want to have to explain what it is. So, I'm thinking of that threat that was published to a customer, to a defender, right? If you have to explain in excessive detail what it is, then you're not going to make for a timely response, right? It should explain only the necessary information for the responder to uh respond quickly and confidently. And then if you present them with too much information, it's going to lose its effectiveness. So there uh two potential dangers, right? Presenting too much information which is going to overwhelm the defender or presenting not enough or uh sufficient ambiguous data. I think of all the ML and AI generated alerts that are more or less useless without proper correlation. Um and so we want to tell a really good story here. What I also like about Bob Ross is he's convicted, right? He knows what makes for bad form. And so as threat researchers, we should understand what data is necessary and learn to ignore everything else and move on, not obsess about perfection and that science and discipline inform his art. And what I'm going to be covering here does involve a good amount of subjectivity, right? But science and discipline can inform uh those subjective decisions that we make. All right. So let's lay some more groundwork here. So first of all, you cannot detect what you don't see. I've read more than enough blog posts that cover, you know, really cool, interesting, offensive techniques. And then at the very bottom there's a little bit of lip service given to detection where it generally would say something along the lines of if you want to detect this technique collect this log source and that's it. Right? So that's not detection. Detection involves the ability to discern between benign events and malicious events. Right? But we certainly need to establish what we need to observe in order to do that detection. So this sounds obvious when I say it, but this reminder, I certainly need it myself and I think the industry um uh could use this reminder as well of that there's a very important distinction between detection and observation. And we can't see what we don't have, right? So we're typically at the mercy of the vendors uh for better and for worse. And another role as threat researcher since we are diving so deep into the data we we know it inside and out. Um we should have relationships with the vendors and we should know uh what what we need from the data and uh and offer that feedback accordingly. Now I see there are like two uh two primary modes of doing detection engineering. So there's what I call a top- down approach. It's taking the existing data sources that you have and the analogy that I use is finding needles out of those hay stacks and so applying ML models, AI, um static rules like threat hunting through the data sources that you have. And this is extremely valuable, right? is just extracting as much value out of what you already have. Whereas a bottom up approach, which is going to be the focus of this talk, is identifying an attack technique that you want to detect, reproducing that and identifying the data sources that are necessary first to observe that specific technique that you replicated and then what you would need thereafter to uh to actually detect it. So the analogy I would use here is uh with a bottomup approach, we are learning to discern specifically the attributes of the needles that we expect to find in the haststack. All right. So how do we go about assessing data quality? Um what I have found through my experience is that it just feels like the majority of log sources are just meant for troubleshooting or for compliance, right? And so as security practitioners, we need to make the best use out of uh what we can get. Um it just seems like there are very few data sources that are designed with security detection and response in mind. So it is what it is. Um, but what I'm going to present here is a methodology for assessing the quality from a security perspective. And so I refer to this process as minimum viable storytelling. Right? We don't want to tell we're not talking about telling the whole story right now. Like that's going to require correlation and enrichment, but what is the bare minimum like necessity to determine is this a quality security data source or not? And if it's not, what do we need to obtain to extract more quality out of it? So, um we go back to the who, what, when, where, and how. Uh with one extra, uh thing added in there. So, um the who and uh again like when I state these things, it sounds obvious, but this just helps me to have a consistent uh process and definition and language that I can speak to so that I can follow this process. consistently, right? So, who is the who or what is the who? So, that is the I I would consider this to be the identity that performed an action. The what would be the operation that was performed without any additional detail. And then, so we'll get into some like grammatical concepts here. Maybe a while since uh you've had to discuss these, but direct objects and indirect objects in a sentence, right? So I a direct object would be the resource that was affected and the indirect object would be what was actually done to that resource. So when should be pretty obvious. So this is the date uh and the time that the action occurred. Now it'll be important as we go through log data like so are we looking at date timestamps from when the event occurred or when the event itself was ingested. So that's an important distinction to make when we're building on a timeline. And then um unless you're uh you know regularly reading older literature uh you may not encounter whence too often. But I just wanted to use this as a way to distinguish uh a different form of where. So whence means you know from from where did something happen. Um I couldn't think of an instance to use wither in here. So we won't we won't consider that. Uh but whence would be you know where did the actor or the who originate from. So where would be the location in which the affected resource resides and how would be the means by which the actor affected the resource for example an entra app ID or a user agent string something like that and again so this is what I'm defining hopefully you would agree with this and as I apply it uh you might agree with my process but this is also very subjective okay so what we'd like to do is take you know this barf of data here and make it actually intelligible right so you don't actually need to read this and try to understand this the point I want to make is um once we've identified candidate data sources to observe a technique that we want to apply that minimum viable storytelling process to see if in this case these two events related to the modification of authentication details um are good from a security perspective. All right, so here's what that template looks like. All right, so I try to apply this to every data source because it helps me understand like do I actually understand what's happening in this one atomic event or not? So it follows like this generally this template. So on, when, within, where, who did what, from, whence, by, how. So specifically in the case of this log data translating that after identifying the sources that would populate the who, what, when, where, how, and whence, you get a summary like the following. you know on on 53 2025 with an Entra ID tenant ID uh that good the user Matt G added an SMS phone number of that phone number as an authentication method to their own account from that IP address. Now one thing that is absent in this case is the how. So would I deem this to be a good uh security data source? Personally, I would uh I don't always necessarily need the how, but just because we don't have it doesn't mean that we'll never be able to get it. That just means we need to investigate further and see if there are correlation and enrichment opportunities where we might be able to get that how for example the app ID or the user agent string. Okay. So once again we've gone from this to a very basic summary of what's happened and we have a consistent process by which we can understand these data sources and really check our own knowledge but assess whether or not any given data source is quality or not. All right. Now let's bring this to Microsoft events specifically. So we need to establish some basic terms here. So there's a in Microsoft data sources there's a unique token identifier. This corresponds to the identifier for a specific access token. Right? So um and this the way it's captured is it's a URL encoded B 64 encoded representation of the ID field specifically within signin events and we'll see some examples of what that looks like. So the pro of doing correlation off of unique token identifier is that you you're able to do direct correlation back to a single specific sign-in event versus with session ID. Right? So this is a group of related unique token identifiers where Microsoft defines a session ID as all tokens associated with an originating uh signin. So this would constitute the original signin authentication and all subsequent refresh tokens that were that were issued. So the pro for this is that uh for responders it gives us a holistic timeline of events uh for any given identity. But on the downside this can span across 90 days. So if you're trying to do like very tactical like detection engineering based on correlation of session ID um it's probably not going to work out so well. Um, and just so we know like a decoded unique token identifier is not the same as session ID. We need to keep those as two very distinct principles because they're represented differently in in data. All right. So, and again, a session is comprised of one or more UTI, unique token identifiers. All right. So, here's what one of those uh URL encoded B 64 encoded uh UTI will look like in log data. So once we uh URL decode that right we recover the original B 64 and then decoding that into a GID this would be the ID field in the corresponding uh Microsoft signin events. So good reference right there. All right. So um some further reference here. So I wanted to offer this. So if raise your hand like are you in Microsoft log data all the time? Okay. So, I hope this will help because this was very challenging for me initially and I needed this kind of reference. Um, so we're going to be talking about UL event sources. So, the unified audit log, but also log analytics based data sources and uh those signin events and correlating between the two of them. So if we're just talking about correlating uh 0365 UL events in this case like the specific example is UL exchange events to signin events that come from UAL logs then the correlation opportunity we have is the following. So that unique token ID right in that format if we want to correlate it to a sign-in event we actually have to decode that because in the UAL signin events it doesn't pre uh present the B 64 encoded UTI. So you have to do that decoding in order to do the correlation back to that uh GID on the bottom. Okay. And then to correlate to a session ID, you have the device property session ID field across both of those data sources. Now, this is the only log source where you have to actually do that decoding in order to do that correlation. But when you're correlating from UAL to a log analytics data source, so any of the following sign-in events, so signin logs, these are the interactive sign-in logs, non-interactive service principal manage identity logs. Um there's more or less a one toone correlation just using different names. Okay, moving on to Azure activity event correlation. So from any given Azure activity event field, we would look to the claims.uti field and to correlate that to a specific signin, you have the unique token identifier on the right side. And if you wanted to decode that, you know, that would correspond to the ID field as well. So um if it wasn't obvious, unique token identifier and ID, they're synonymous. One is just decoded. All right. signin events. This looks like it's pretty straightforward. Yes. So, if we want to correlate a non-interactive sign-in log to a sign-in log, we have that opportunity. It took me way too long to realize that this correlation was even possible because I've had such mixed results with correlation ID in Microsoft data. So, uh little bit of a rant here. For those of you who work in Microsoft data all the time, you might understand just how inconsistent and how uncorrelatable correlation ID is, it totally depends on the product that you're dealing with. And you can really get a sense of just how siloed the organizations within Microsoft are when you're dealing with these different data sources where sometimes correlation ID doesn't correlate to anything and it's just a glorified identifier for the event itself. But then um other times correlation ID will correlate uh within a Microsoft technology but not without. So like not across to different um log sources. This is one of those few cases where correlation ID uh actually correlates across entities in this case from uh a non-interactive signin log to a sign-in log. Uh and this is important to know because let's say we've done a detection off well we're going to use a use case uh shortly of the uh suspicious inbox creation. If we want to go from the inbox creation to the corresponding interactive sign-in event, we need the non-interactive sign-in event in order to make that link. And I'll show you what that looks like shortly. We also have graph activity log correlation which is pretty cool. So just presenting that here as a reference. All right. And then uh so I've already mentioned uh some of the issues with correlation ID. Uh one glaring uh omission from Microsoft which is just a constant point of frustration for me is the fact that you can't do this correlation in the audit logs table. So any Entra ID audit events at least I'm not aware of. If you know a way to do direct correlation back to a sign-in event, you let me know. But there is no unique token identifier represented in those events. All right. So let's dig into some practical case studies. All right. So let's consider this one unified audit log event where uh an inbox rule was created. So I've distilled this down. This is probably like onethird of the original event. I had to fit it into a slide, but I also just wanted to capture uh the most important fields that we'll use to populate that minimum viable storytelling template. All right. So, what are we going to focus on here? All right. So, I've highlighted what will be used to constitute that minimum viable storytelling template. Right. So, we had the when. So, this is the creation time. And again, we'll want to investigate in documentation if it's even there. Is this when the event was ingested or is this when the event actually occurred? We'll want to understand that. And then the what. So what actually occurred? Okay, we have a new inbox rule. Where did it occur? So in this case, this will be the tenant ID that the event took place in the who. Who is the actor that performed the action? This is Matt at Ktoso Corp. From where did the who originate the action from? That's the IP address. And then what resource was affected? So that's that direct object right there. In this case, it's the Matt Graber inbox. And how did that action occur? It happened through this particular app ID. Now, this doesn't give us the pretty name of what that is. So, that would require either more correlation or enrichment, which we'll get into shortly. And then what actually happened here. So, uh we need to know uh what what was the name of the rule and what was the logic of the inbox rule. So, we get that below. And then finally, we also get the object ID of the identity. So, this is the GID corresponding to Matt at Ktoso Corp. All right. So, this is great. We've been able to populate that minimum viable storytelling template so that we can translate it into something human readable. All right. Now, I have done one extra step here where I did the enrichment to resolve that app ID into one outlook web. So that could be based on like a static list of uh built-in uh Entra ID applications. Uh there are other ways to do this enrichment. But there are also um correlation opportunities which we're going to get into where we don't where we won't actually have to do that enrichment. So this makes sense to me. So going from this to this, I think anyone who works in Microsoft cloud and identity data would be able to understand these two sentences. Okay, so I mentioned that enrichment that I did for the app ID and also we didn't have a user agent string. I always like to have that if I can get it. Um because that is just one of many different ways that we can assess whether or not this activity is normal for this given identity. So let's go back to that unique token identifier and see what we can correlate. All right. So K Am let's remember that. And here's an example log anal analytic analytics query taking that unique token identifier and seeing if there are any corresponding sign-in events for that UTI and what we get is a single non-interactive sign-in event. So this is to be expected that onetoone correlation from the event that occurred to a single non-interactive sign-in event. Okay. Um and this is important to understand because this event that occurred, it wasn't Matt at Ktoso Corp that did it. it was the application that did the actual action right on behalf of Matt at Ktoso Corp. And so that's why we have the non-interactive sign-in event to represent that it was the one Outlook or whatever it's called uh app that per the one Outlook web app that performed the action on behalf of Matt Graber. And you can see below we do get the user agent string. So we have a direct correlation opportunity to further fill out that minimum viable storytelling template. Now we could go that one step further and correlate to the corresponding interactive sign-in event. And with that we'll use correlation ID. Right? So here's that corresponding single interactive sign-in event. And this is helping us build out that timeline as well. So starting with what is highlight highlighted the third one. So pretend this was like the detection logic that we had right. So something triggered that suspicious uh inbox rule that was created and now we need to automate the correlation to start building out this timeline. And so this process is what we've just done again starting with that one UAL event working backwards with the correlation points that we had. So now we can take that and start to build out that timeline even further. So we can ask the questions, okay, besides the suspicious inbox rule creation, what else did that actor do? Did they delete any emails? Did they read any emails? Did they forward any emails? We need this to help build out that timeline, not only to help with more confident detection, but to also facilitate a response. But then what was the leadup to that suspicious inbox rule creation? With all this correlation, we're able to start filling in this more holistic timeline. So, for example, were there any failed sign-in attempts prior to the successful one leading up to the inbox rule creation? All right, so just to recap, this is how that correlation was performed. We started with the UAL new inbox rule event went to the corresponding single non-interactive sign-in log and then to the corresponding single interactive sign-in log. And again, I mentioned before that going directly from the newbox rule event to the interactive sign-in log is not possible. I mean, you can make some inferences based on session ID, but you're not going to have 100% confidence, but rather like why why make those inferences when you don't have to when you have these direct correlation opportunities? If only Microsoft would document them better. But anyway, here here we are. So hopefully this will serve as a reference um for uh facilitating this sort of thing. All right, let's jump into our next case study. All right, so an Azure role assignment. So I want to direct your eyes to the commands at the top specifically the second one. All right. So here the owner role is being applied to a subscription that A5 F4 subscription and owner is being applied to the FBB7 um uh principal ID. All right. So start thinking about what you would like to capture in event data that would capture all the context needed to fill out that template. All right. So I'm already thinking the object ID, the role in this case owner, and then the scope, the subscription that the role assignment was applied to. Now as a result, what happens? So we have a spec specific role assignment name. So this would be relevant to response right? If we need to remove this role assignment this gives us the ability to directly remove that without any question that uh we may or may not be removing the the wrong role assignment. Um we also have the object ID that FBB7 is resolved here below as test sub owner at Ktoso Corp. um that's going to be helpful for any responder to to know the actual user principle name instead of just working with gooids all the time. And then it's also helpful to have the the object ID again of um of the actor who performed the role assignment. All right, so let's start applying this process and think about how we would fill out that template. So the who in this case for a role assignment would be the identity that performed the role assignment. So that would be Matt at cantoso core performed the role assignment and then what so that would be that a role assignment was performed in the first place. We need to know which specific log source actually captures role assignments. The direct object would be the target scope of the role assignment. The indirect object would be the object and the role definition. Right. So the test sub user at Ktoso Corb that's going to be the object and the role definition would be owner. Right? When it occurred from where did the actor originate the IP address and then the where so where is the resource located in this case this would be the hierarchy. So the tenant and the subscription of the target resource which is a scope in this case. and then ideally the application and the user agent that performed the action. So going into our log data investigating this we discovered that it's the um role assignment write activity that would give us this context and start filling out that template. Now which one do we choose? So we had two to choose from. Uh is one better than the other? Do we need both? We'll have to investigate. So, um, if you've worked with Azure activity logging a lot, uh, you've run into that same question. Is start or success? Is that the one I key off of or do I need both? We'll investigate. So, let's start filling out that template. So, who is the who? So, again, it was Matt at Contoso Corp that performed the RO assignment and we're dealing with Azure activity logs. And so, we get that user from the caller. And if we want the object ID, which we really should as well because we we would need that for response, the object ID that GOID, you get that in um the in the principal ID field, which would have to be normalized or ideally normalized to a GID if it wasn't obvious to you that that was a GID. All right. Next. Um so the what so what actually occurred? So a RO assignment was written. We get that from operation name value. And then what was the specific role assignment ID? So if we need to uh remediate this be good to have that good there which is found in resource ID and then what was this uh what was the target resource that was affected. So in this case this is a scope which we get from ro assignment scope and again that is that direct action there. Okay. So continuing on uh we have the what um so we need to know who the role assignment was applied to. So that was owner. All right. And then the role definition ID we get that there as well. And so with all that populated if we wanted to remediate that we have all the information we need to perform that remediation either manually or in an automated fashion. Next we have our timestamps and we need to choose uh which one is which. Oh, and let me go back. So the principal ID, so who the role assignment was applied to, you only get that in this in the start event, not the success event. So the request body, those will always be in the start events. So that context is very important. So I generally lean towards preferring those start events over the success events. Okay. went pretty straightforward and then the how app ID. Okay. All right. So, Azure activity role assignment write start events is what's necessary to populate the majority of the minimal viable storytelling template. All right. The event does have sufficient context based on the guidelines that we established to perform remediation and any missing context can be acquired with correlation or enrichment. Right? So for example the uh role assignment principal name that can be enriched and then through correlation we can also get the app name that performed the assignment um beyond just the app ID and then the correlation as well can be correlated to the sign-in event. All right. So my subjective data quality assessment is that um that Azure activity start event uh is medium to high on its own without correlation because it fills out that template sufficiently um with a few exceptions. All right. So this is what it would be translated to. And then if we had correlation uh and further enrichment, we could get some additional details that you see highlighted there. So some closing thoughts. It's important as researchers as we're going through this data and trying to make sense of it and articulating it to customers and defenders that we have a repeatable universal process to determine what is necessary to observe, detect, and respond to threats. The unique token correlation offers that highest signal to noise ratio for initial detection. It's not going to tell the whole story, but it helps build up that necessary story so that we're not making too many inferences about how to make those connections between events and signins. And now we have a process for assessing the minimum data quality standards. and we have an ability to to correlate and but without that we will fail as defenders because we can't tell that story effectively. Now Microsoft is getting better at correlation. So there are more data sources over the years that now support unique token identifier. Hopefully we'll see that with audit um events eventually. Um and there's one reference out there. Um, so Microsoft gives a little bit of uh reference documentation on how to do some of this correlation. Um, so I just wanted to provide that. But otherwise, that's all I have and I'll open it up to questions. If you have a question, go ahead and put your hand up and I'll come run over to you. Thanks for the talk. Uh I was curious um have you done any uh research on um with adversary in the middle with all the um session tokens versus refresh tokens and that it's quite annoying in logging. So I was curious to see if you had any uh good stuff on that. Yeah. So the question was have I done any research on adversary in the middle? Yeah, I've done that. And there in lies s part of the challenge because like adversary in the middle is really abusing a lot of this process, right? So like when we're trying to establish that that timeline and tracking unique token identifiers, right? So uh when a token is stolen, right, that unique token identifier is going to be the same, but we have some things to to key off of. For example, like the the w the IP address, right? So when those subsequent events are logged say um from the legitimate use to when uh like after it was stolen subsequent uh events would be generated with uh with with a unique IP address. So you know uh as you may know like there are some things that we can key off of but um yeah abusing that uh adversary in the middle is yeah it is a perpetual kind of thorn in in our side. Yeah. Thank you. Hey. Um, I had a question. Um, you know, you mentioned oftentimes vendors are not designing logs for detection and response. And I'm curious like who who are they designing it for? Is it negligence? Is it some kind of compliance framework? Like I can only speculate, but I would assume it's for compliance auditability, right? Um, troubleshooting. Um, right. So I mean that there are certainly more usea use use cases beyond security for these log sources you know like uptime and um availability that that sort of thing. So these logs have to serve many different use cases not just our infosc purposes. Um so I don't know it it would be nice if um there was more of a internal focus on making these more accessible for security-minded folks. Um but you know we we have some uh methodologies at our disposable disposal to uh extract as much as possible. So it's a good question. I think that's all the time we have for questions. If you have any more definitely check out the uh Slack thread. You can chat with Matt directly. Uh can we get a round of applause for Matt?
# IAM Roles Anywhere â€“ now for everyone with Let's Encrypt
[**Video Link:** Watch on YouTube](https://www.youtube.com/watch?v=M1hXUcBMf1Q)
- **Author:** Dhruv
- **Talk type:** Security Implementation / Cloud Infrastructure

## Summary
This talk demonstrates how to use AWS IAM Roles Anywhere with Let's Encrypt certificates to enable secure, temporary AWS credentials for workloads running outside of AWS infrastructure. The speaker presents a cost-effective solution using free Let's Encrypt certificates instead of expensive AWS Private CA, while addressing key security challenges around certificate distribution and private key protection.

## Key Points
- **Cost-effective PKI**: Use Let's Encrypt staging intermediate certificates instead of AWS Private CA (~$2/year vs higher costs)
- **Security workaround**: Public CAs can't be used directly, but intermediate certificates from Let's Encrypt work as trust anchors
- **Automated renewal**: ACME protocol with Lego client enables automated certificate lifecycle management
- **Detection over protection**: Since private keys must be on disk, implement environmental sensing and TOTP validation for compromise detection
- **Rapid revocation**: Use IAM policy deny statements for immediate certificate revocation rather than waiting for CRL updates

## Technical Details
### Architecture Components
- **Trust Anchors**: Let's Encrypt staging intermediate certificates (E5, E6)
- **Client Tool**: Lego (Let's Encrypt Go client) instead of certbot
- **AWS Tools**: AWS signing helper for credential generation
- **Authentication**: PKCS#11 interface support for hardware security modules

### Implementation Steps
1. **Domain Setup**: Purchase cheap domain (~$2/year), host in Route 53 with dedicated account
2. **Certificate Generation**: Use DNS validation with restrictive IAM policy for Route 53 access
3. **Trust Configuration**: Configure IAM roles with certificate subject common name restrictions
4. **Workload Integration**: Replace static credentials with credential process using signing helper

### Security Enhancements
- **Environmental Sensing**: Generate TOTP codes from environment fingerprints (hardware IDs, static assets)
- **Detection System**: Validate CloudTrail events against expected TOTP values
- **Blast Radius Limitation**: Use dedicated AWS account with minimal permissions for DNS management

## Full Transcript
Welcome back everyone. This is Dhruv kicking us off for the afternoon session. I am RollsAnywhere now for everyone with Let's Encrypt. Talk sponsor is Sweet Security. Hi everyone. Sorry for cutting your lunch break short. I'll be discussing how to use I'm RollsAnywhere with the free PKI. We all love Let's Encrypt. Show off hands if you've used I'm RollsAnywhere. And Let's Encrypt. Oh brilliant. This will be easy. So roughly the four sections the presentation is divided in. The reason why you want to assume an I'm Roll outside of AWS could be some of these like on-prem. In our case we were trying out a few GPU providers. Legacy CI/CD, Jenkins and stuff. You do not like static credentials. There's no OIDC opportunity like between GitHub and AWS. And you don't have hardware security modules or TPMs etc. available. Solution is I'm RollsAnywhere of course but it comes with challenges such as you need to have a PKI but you don't. Spiffy infrastructure doesn't exist. AWS private CA cost is somewhat of a concern for smaller organizations not so much for larger ones. You may not have CA experience. Certificate distribution is also a problem that remains unsolved even if you have a PKI. And how do you protect the private key once it's on the disk? So these are some of the challenges I will be addressing in the following slides. I'm going to skip over how asymmetric cryptography works but basically private and public key are linked together with some magical maths. You can come and talk to me about it afterwards. This is what a certificate looks like when dumped via open SSL. I've highlighted in red the bits that are important for this talk. It has a serial number, it has an issuer which is a string. It's only a string what really matters is the large numbers which you can see at the bottom of the screen. Certificates come with an expiry date which is useful, makes them stateless. The subject common name equals a domain name is very important today because let's encrypt will only issue certificates against domain names. So that's a bit we will leverage. The format of certificates we use these days are X509V3. They can have features turned on and off. The digital signature key usage bit needs to be turned on. Let's encrypt issues certificates which have this turned on. This is required for TLS to work at all. So that's problem is solved. CA false should be true on end point certificates but true if you want to use this as a CA inside I'm roles anywhere. We will see how to obtain one of those certificates from let's encrypt. Very quickly how a web browser works is that they come preloaded with CA certificates. You visit a website, the web browser does the maths to see if the certificate presented is actually issued by CA certificate present in its store, checks whether the certificate is valid, matches the name, etc. And the browser tells you that yeah, this certificate is okay. But in the case of I'm roles anywhere, you have to load the CA certificates into something called trust anchors in roles anywhere. Then you create a relationship, trust relationship between an I'm role which lives outside of roles anywhere but still just like a regular role. And make it trust the trust anchors. You then create in roles anywhere profile and add roles to it. This is just like a menu card of options that anybody is using a role via roles anywhere can choose from. The next two are how a workload will presumably generate a private key and never have it leave its own perimeter or the system where the key is generated. But it then has to get that key a certificate signed from a CA. We will go into more detail on that shortly. AWS have published an open source tool called the signing helper which takes these five arguments which if you pass to the roles anywhere service gives you back a session token. So it's the public private key pair, the ARNs of the trust anchor, the profile and the role itself. I am roles is now acting as the browser. So it tests everything whether you've presented the right certificate, what's it allowed to do, has it expired or not, et cetera. And if all checks out, you're given a session token and from that point on it's just like an assume role. So how do we fit let's encrypt in I'm roles anywhere? AWS documentation says that public CA's cannot be used as trust anchors and I've tried that. They indeed cannot be. Let's encrypt uses hierarchical set of CA's. So they subordinate CA's or intermediate CA's, they go by two different names. And the end user certificates are actually issued by intermediates. This is common for all CA's. This is not particular to let's encrypt. What you can do is you can use intermediate certificates and put them in roles anywhere. And that works. But another recommendation I would make at this point is to use the staging intermediate certificates. Let's encrypt has a staging endpoint as well. They are good enough for us because we don't need browsers all over the internet to trust the certificates that we are going to be using. So the staging endpoint is perfectly sufficient for our use case. So that's what I've chosen for my own deployment. I have figured out what intermediate certificates were in use. And I went to the staging website they have, they have a GitHub repo where you can download the CA certificates which are in use at the subordinate level. Get the current ones, E5 and E6, concatenate them and add them as a external certificate bundle into the trust anchor for roles anywhere profile. And that works. Next problem is how do you distribute certificates or issue certificates to begin with. And that's where the ACME protocol which lets encrypting invented a few years ago comes in handy. They have four different methods for validation that you do indeed have control over a given domain name. Three of them require open ports. So that's not the route we are taking. We'll just use the DNS validation. Step one is to buy a really cheap domain name which has got nothing to do with you or your company. So there is a cost for the solution which is about $2 per year. You host it in route 53, give it its own account, create an IM user with a very tight policy which we will discuss in the next slide, obtain static credentials for it. I know static credentials are a problem but just please bear with me until the next slide. The tool we are going to use instead of cert bot is Lego. Let's encrypt client written in go. And this policy is advised by them. You see it's very restrictive. It can only get a change, propagation, list all zones in the current account. That is why I'm recommending you put this in its own account. List records of a specific zone and only be able to create new text records. So even if these credentials leak, the blast radius is very limited. Next, you run these two Lego commands. The first one is to get your initial certificate and the second one you can run weekly to keep renewing your certificate. When this talk goes online on YouTube in a few days, I will add the full system D service files and the timer files on my blog post to go with this talk. So you can look at all the code I have. This is also true for all the JSON policy documents that you might see on the screen. Provided this is running in an environment with your static credentials, these two commands and the purple bit is by the way the staging API for let's encrypt. These two commands will result in your workload having an identity now. So you've got a certificate here and a private key. Some might even call it an NGI. Next step would be to configure a role and establish the trust relationship with the trust anchors. Most of this policy is taken as it is from AWS documentation except the bit in red, which I've added. The common name in the certificate subject is restricted to the domain name you bought or the subdomains of it rather. And this is what protects you from anybody having a let's encrypt certificate at all to be able to authenticate with your roles anywhere. This will constrain it down to just the domain name that you own. And because let's encrypt has done the validation that you did indeed have control for making text records on it, we can fairly be sure that this certificate will be presented by a workload that we own. All the roles that you have, you can add them to in roles anywhere profiles or they become available to any workloads that want to assume them. It's just a menu card, I believe. The policy condition of the subject common name being available is because of certificate attribute mappings in the profile. This, what you're looking at on the screen is the default. It breaks down the subject into all the components. You may as well delete the rest of them, but I don't think there's a harm in leaving them in. To use this on the workload, you modify your credentials file not with static credentials, but with a credential process and point it to a script. The script calls the AWS signing helper binary, which is the bit we discussed earlier, it's available on GitHub, and pass it the five parameters that we discussed earlier. Path to the private key, the certificate, and the ARNs you want to assume, and you'll see that the role has been assumed. By this stage, you can be sure that the let's encrypt staging subordinate workaround is working. Now, one of the concerns people have is how do you protect your private key, which is lying readable by any malicious actor on your system. So let's discuss a way to build a detection around it. We can't really protect the key unless it's in a hardware security module. If you have a hardware security module of any sort, for example, a UB key, the way to do that is to generate a certificate signing request using the utilities provided by your HSM vendor, and use the certificate signing request in a legal command, rather than passing it in the domain name. And then the AWS signing helper can also use UB keys via the PKCS 11 interface. But I'm not going to go further into use of hardware security modules, but it is a possibility. So I've taken inspiration from something called environmental sensing. Security engineering, Professor Anderson sadly passed away last year, but his pact with the publisher was that upon his death, they would make his book available for free download, and it's available on the University of Cambridge's website. This is from the nuclear command and control chapter, where the weapon would not arm if the right aspect of the environment was not sensed. In another case, the atomic demolition munitions requirements were replaced by use of a special container. So these are some fail safes built into military hardware, and we're going to use a similar sort of approach to see whether we are using the private key in the right environment or not. So pick an invariant from the environment. Static assets shipped with your application. Platform hints from -- so /sys has hardware identifiers, /proc has kernel identifiers. If you're familiar with Spiffy, Spiffy has good documentation on what things they look at for attestation. If you have an NFT lying around, it's literally a checksum, use that. From the fingerprint of the environment, create a sha1 sum, and that sha1 sum will be used to generate a time-based one-time passwords, taught be the six digits that keep changing every 30 seconds. And we will pass that in to roles anywhere. To pass it in -- sorry, one step here is that once you've taken the fingerprints of the environments, the sha1 sums of it, build a database out of it. It's very important that you capture the fingerprints of the environment, rather than give it to them. This is just a JSON file in which you can do this. There's no need for a proper database. Go back to the profile and turn this option on, which is accept custom role session name. And then in the trust policy that we established earlier between the role and roles anywhere, we add an additional clause which says that the role session name should be set to six letters. And that way, if the role session name has not been passed and it's not exactly six letters, the request will be denied immediately anyway. And you add a sixth argument, which is role session name, and try passing the tautp to it. To calculate the tautp, we will use auth tool. This is not a woth. This is completely different. It's installable and available on all sorts of Linux distributions. And you just pass in the sha1 sum and it gives you the code for the current time. Or you can even pass in a timestamp and it will tell you what the code would be at the time. It could even be in the past. And we modify our script to calculate the tautp on the fly. So in the first highlighted line, you will see that I've calculated the checksum of my NFT, which is something in my environment. It could be anything. And passed it to auth tool to give me the tautp code for the current time. And then I pass it in and in the end, you can see in the second highlighted one. The cloud trail event that comes out of it now will have a role session name set to six digits. We already know in the cloud trail event what the certificate subject was. And we know the event time. Using these three, we can retrospectively validate whether the right tautp was passed or not. This is a sample script which can do this. It's written in bash for just the purpose of this talk. You can run this in a lambda if you like. And this script also calculates the tautp 30 seconds prior to the event, just to account for network delays and latency. So it looks at two different tautp's and sees if it's one of them. If it's not, it raises an alert. So if somebody were to compromise your private key and you were continuously running these detection scripts, the attacker would be caught in just one misstep. They just have to use it once badly and they would be caught. What happens when they are caught? How do you revoke the certificate? So let's encrypt generates CRLs only once a week. So we don't have that much patience. So in the trust policy, we can add another clause of type deny and start denying all the subject common names that you feel have been compromised. And this has immediate effect. So how did we solve the problem? We used let's encrypt for our PKI. Spiffy, for those who are familiar, we replaced that with ACME. Answer to everything else is basically let's encrypt. The private key protection, we could not protect it but we've built a 2FA sort of detection around it. Some things to consider are that I'm roles anywhere are regional. You can lower the session duration to 15 minutes. Let's encrypt is going to start issuing 6-day valid certificates which I think will make this even more effective an approach. You should definitely monitor the certificate transparency log for that domain name that you bought just in case somebody has indeed started issuing genuine certificates from it. There are alternatives to let's encrypt which use ACME, ZERO SSL bypass. I have not used any of them. They have a free tier or you could host your own. Thank you. Any questions? Questions? Anyone? So one question maybe is you mentioned in your last slide there future TBA shortly profile. What would that actually look like beyond the 6-day servility? So the current certificates are 90-day and 6-days means that you will force the workload to renew more early. So if something's been compromised that would mean that there's less chance of it being out there in a vulnerable state. Do you see a need for anything less than 6-days? Would that be helpful in any way or is it just generally a limitation of less encrypt? Yeah, no I don't think. I mean actually even comfortable with long-lived certificates but that's just me. Other questions? All right, I think we'll wrap up here then. Anyone give it up? Thank you.

# Putting Workload Identity to Work: Taking SPIFFE past day 0

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=oHlPGzpFT_c)

- **Author**: Dave Sudia
- **Talk Type**: Security

## Summary

This presentation provides a practical guide to implementing SPIFFE (Secure Production Identity Framework For Everyone) at enterprise scale, moving beyond initial proof-of-concept deployments to production workload identity systems. The speaker shares real-world strategies for deploying cryptographic workload identity across hybrid and multi-cloud environments, focusing on implementation challenges and organizational change management.

## Key Points

- SPIFFE provides cryptographic workload identity based on provable metadata rather than static secrets
- X.509 certificates with SPIFFE extensions enable strong machine-to-machine authentication
- Implementation requires careful goal setting and starting with simple, scoped use cases
- Hybrid and multi-cloud environments benefit significantly from unified identity standards
- Organizational change management as critical as technical implementation
- Short-lived certificates eliminate credential rotation pain points
- Major cloud providers (AWS, GCP, Azure) have implemented SPIFFE support

## Technical Details

**SPIFFE Architecture:**
- X.509 certificates as primary identity documents for workloads
- SPIFFE ID field extension containing trust domain and workload identifier
- Metadata-based identity verification (OIDC tokens, cloud metadata, TPMs, Kubernetes JWKS)
- MTLS connections with certificate verification on both ends
- Federation capabilities between different SPIFFE issuers

**Identity Verification Sources:**
- **Cloud Metadata**: EC2 instance roles, GCP service accounts, Azure managed identities
- **Kubernetes**: JWKS tokens from cluster service accounts
- **On-Premises**: TPM attestation, systemd unit properties
- **Code Signing**: Container image signatures and verification
- **OIDC Tokens**: GitHub Actions, CI/CD pipelines

**Implementation Components:**
- **Central Issuing System**: Servers providing certificate authority functionality
- **Edge Agents**: DaemonSets in Kubernetes, agents on VMs/instances
- **SDKs and Proxies**: Application integration or sidecar proxy solutions
- **Administrative Framework**: Team structure and policy management

**Use Cases:**

*Workload-to-Workload Communication:*
- Replace API keys with continuously rotated certificates
- Metadata-based authorization decisions (region, environment, service type)
- Eliminate long-lived static secrets in inter-service communication

*CI/CD Pipeline Security:*
- Short-lived credentials for deployment pipelines
- Eliminate static AWS/GCP/Azure credentials in repositories
- Automatic credential expiration based on pipeline execution time

*Hybrid/Multi-Cloud Identity:*
- Unified identity across AWS, GCP, Azure, and on-premises
- Single credential type instead of platform-specific service accounts
- Cross-cloud authorization and federation capabilities

**Implementation Strategy:**

*Goal Selection:*
- **Developer Efficiency**: Reduce credential maintenance overhead
- **Security Risk Reduction**: Eliminate long-lived credential exposure
- **New Capabilities**: Enable previously difficult cross-cloud scenarios

*Scope Management:*
- Start with CI-to-cloud provider authentication
- Small edge server groups with bounded scope
- Single enthusiastic team for initial implementation
- Isolated data center for workload-to-workload communication

*SPIFFE ID Structure Design:*
- Keep structure simple initially (easier to add complexity than remove)
- Consider future differentiation needs in URI design
- Balance granularity with operational complexity

**Enterprise Scaling Considerations:**

*Organizational Requirements:*
- Initial implementing team with technical and compliance expertise
- Administrative team for ongoing issuer management
- Internal best practices development group
- Integration with existing PKI infrastructure

*Technical Integration:*
- PKI team coordination for certificate authority integration
- Code changes for application integration or proxy deployment
- Metadata verification system configuration
- Revocation strategy (primarily through short TTLs)

*Change Management:*
- Cross-team coordination for workload-to-workload expansion
- Stakeholder education on benefits and implementation requirements
- Iterative deployment to prove value before organization-wide rollout

**Case Study Examples:**
- Large financial organization implementing GitLab CI authentication
- Multi-tenant data center with service-to-service authentication requirements
- Cloud team standardizing new project credential provisioning
- Cross-cloud deployment with unified identity requirements

**Current Limitations:**
- Limited native support in legacy database systems (SQL Server, Oracle)
- Authorization framework still largely bespoke implementations
- Proxy solutions needed for systems without native SPIFFE support
- Service catalog integration not yet standardized

**Future Considerations:**
- AI/ML agent identity requirements and implementation patterns
- Integration with service mesh and cloud-native security platforms
- Standardization of authorization policies and service catalog integration
- Tool ecosystem expansion for operational management

## Full Transcript

Uh, we're going to start putting workload identity to work. Taking Spiffy past day zero with Dave Sudia. Uh, quick announcements. Cell phones off. Q&A. Raise your hand or put them on Slack. Uh, and last, I have to thank our sponsors, Tam Noon. Uh, gold sponsor Tam Noon this year. Uh, it is the sponsors that help us put this thing on. So, thank you very much. Please welcome Dave. Howdy. Am I on? Oh, there. There. Now I can hear myself. Okay. Uh so yeah this is a lightning talk on this process which needs more than a lightning talk to get really deep. So this is going to be a bit of an overview but um you know uh Patrick very kindly offered me a whole room later if someone wants to sit and talk with me for four hours about this. uh um and how how much of a how lightning I need to be is going to depend kind of on the answer to this question which is you know spiffy is a relatively brand new technology not many people are implementing but so how many people here feel that they're confident about what spiffy even is and what the value is raise your hand okay cool so I am going to fly through what spiffy even is and what the value is and then hopefully and then still hopefully have time to go over the the implementation part at scale part uh but hopefully even if you get value out of the first part that will be good. So, so we are definitely going to do section one based on the answer to that question uh which if it had been different we probably would have skipped over and that is just like what are we even talking about here right and then section two is use cases that is going to be very brief just to kind of give you a sense of what what could you be implementing this for and then section three is some of the strategies for putting it to work and we're just gonna we'll get we'll get through what we can so why trust me I'm a former platform engineer so not from the security side But although at the place some of the places I was at as a platform engineer, I was also the security team and just had to do as much of that as I could effectively. And you know submitting this talk was really about the places that are doing spiffy right now and doing it at scale can't talk about it. uh there they are not places that go out and publicize but we work with several of them and so I've kind of seen it happen and and can kind of indirectly you know scrub the names off the board here and talk a bit about how we've helped people do that so what is Fippy so to just start with the whole value proposition here right the the problem that you have is that applications need secrets you know CI systems need secrets and you might be keeping those secrets in a vault but to get your secrets from in the vault, you have to authenticate yourself with the vault. And to do that, you need a secret. And it's just secrets all the way down, right? And so the idea here is that instead we should be ident like finding the identity of the thing, right? And issuing it a document based on provable metadata about that thing rather than asking for an API key. So we'll talk about what that looks like in a moment. This is a same you have the same problem with humans, right? Uh, for those of you not on history or United States history, this is the door to a speak easy, which in the 1920s in the United States, alcohol was outlawed and so lots of sort of underground bars sprung up and you'd go and you needed to get a p had to have a password to get in, right? There's a clear problem here, which is that cops could have the password. And so what we want is, you know, and now that has uh uh migrated to if you want to get into a club, you got to be on the list, right? and they're going to ask to for your identification to make sure you're on the list. And that identification comes in the form of a document. You have at least one, right? And some core things about this document are it's issued by a trusted authority. It's confirmed by metadata about you, either other documents or your biometrics or some other thing. And it can be used for both authentication and authorization, right? It's not just proving who you are, but it also governs what you're allowed to do. You're allowed to drive, you're allowed to cross this border, whatever. So for machines and for workloads, we have a great standard for this. It's X509 certificates. Uh there are a couple other things you can use for this uh that I'll talk about in a little bit, but the primary one is X509 certificates. They're cryptographic. They're shortlived. I'm not going to go through the whole list because I want to get to the the other core part of this talk. But the point is they're really good for this. And the Spiffy standard is a graduated CNCF project that sets out some extensions on X509, some new fields that allow you to do this more effectively for a specific workload. How do we issue those? What's that metadata for the machine? Well, for machines, we can do things like OIDC tokens. Yes, this is that GitHub pipeline, uh, GitHub actions pipeline, right? Cloud metadata. We can guarantee something is coming off of a specific EC2 instance via its assumed role. If it's on prem, you have TPMs. Kubernetes, you have the JWKS from inside the cluster. For workloads, we have lots of metadata, right? We can say, oh, it's this set of labels on a pod or this systemdunit property or I signed it with this the code with this signature and I know that the container is running with that signed code, right? And so again, Spiffy has set out a standard for doing all of this, not only for the uh the issuance, right? But sort of how you can have common ways of communicating to receive these. You can reach out over an API uh over a socket that has has been laid out in the standard other sort of ways of communicating around this how spiffy issuer uh issuers can federate with each other and there's pretty broad adoption now that I'll talk about in a little bit in a minute. And the core part like the main part of that extension on the X59 certificate is there is a field for the spiffy ID. It's an encoded thing that has a trust domain here and then your whatever other field you want and you get this spiffy verified identity document that is that certificate. It can also be a JWT. Uh the workloads uh when they are communicating with each other create an MTLS connection verify that on either end the certificate is something they trust from one of their trusted authorities and then they can authenticate and use that to make authorization decisions. So how do you use it? Uh you run an issuing system that issues these fids. You run agents that issue them out at the the edge and verify the metadata about the workloads to get it into your actual applications. There are SDKs that you can add in. Um I can show code examples after for anyone wants to see this, but you know you swap out the the TLS server implementation in your Go app for one that supports this. And now you know it's a couple lines but it is some code changes. There are also because it's an ecosystem there are proxies that you can run that adopt this where if there's something you can't make a change there's not an SDK you can use that. So some results out of this why you would want to do it now everything has a trusted cryptographic identity you have real zero trust in your system because when you verify right you can actually fully trust the thing that you are talking to you remove long live static secrets out of your system whether that is in apptoapp communication or inci pipelines or whatever you're saving time you're more secure okay that's all right I got 13 minutes cool so workload so some use cases for this right are workload to workload you have a thing that's talking to another thing. What we're doing now is we're removing say an API key, right? Or a certificate that you have to set that you have to get out and maintain and rotate. And we're now replacing it with a certificate that is being constantly rotated and updated uh on demand. And with that now certificate, it's not just a certificate that could have been distributed anywhere, right? It is a certificate that was distributed to that application based on its metadata, right? And so now we have a guaranteed identity for that thing rather than just a credential. By doing that you're removing those long live static secrets. You're removing the maintenance pain of having those things and having to go through proper practices for them if you do. uh use case there is I've worked with a very large financial organization that I will not name and it made me a little nervous about my the financial system because they came their auditor said you have to rotate these database credentials at least every two years and they were like well if we did that uh that is they had so many credentials that if they rotated the credentials every two years that is all that team would ever do and they would have no time to do anything else and so if you can remove that pain and have things automatically issued that's good uh you have authorization based on identity right now, right? So you can say if you structure your um your ids correctly, you can say oh that is the EU version of that service versus the US one and we have data sovereignty issues and so I should reject it because it's not from the correct region or something like that. It's also really useful for hybrid and multi cloud things. You know, when I'm talking to people about this sometimes it's like, well, we're entirely in AWS. And so, we just use the IM enroll system within there. And it's like, okay, yeah, that's actually pretty good, right? If you're all in one place, if you are trying to cross boundaries, um, the AWS and GCP and Azure have all implemented this. So, I was talking with someone who like very very impressively they were doing real full workload identity across AWS, GCP, Azure, and on-rem. And the way they were doing that was they were issuing an AWS key pair and a GCP service.json and an Azure service credential and they were issu and they were distributing those to all three clouds plus their on-prem systems for each workload. And I was like that's very impressive but I can simplify your life. Uh because now we just have one credential you can issue and it's used uh it can be used across all of those environments right and there's that rapidly growing ecosystem of support. So I would not be surprised if other clouds eventually adopt this. you have those proxies and other things that come in to fill gaps where we have them. Okay, now the meat. Uh, and just to to look at those for a second. So, you know, this is very briefly a a workload to workload example. Um, and the image is not going up increasing size as much as I'd like, but essentially like I have this node app here. The node does not have a great SDK at the moment. So I'm running its traffic through ghost tunnel proxy which is a spiffy compatible proxy. Uh I have a an issuing agent over here that verifies metadata and uh issues those verified identity documents. This front end is set to only uh accept responses and connections with this specific backend that's written in Go could be running in Kubernetes VM wherever and uh and so we have that like very strong guarantee here. I have a reference GitHub actions pipeline. So this uh I use this to configure a demo environment and it this actions pipeline has admin access essentially full access to VPC, RDS, S3, EC2, EKS, whole lot of uh services and systems within AWS. And the only secret I have in this actions repository is a Palumi config fase which is required by Palumi. I don't actually use it. I don't store any secrets in Palumi. But the entire actions pipeline basically again can go into detail on this if anyone wants to later. Essentially I issue out a key andert uh public certificate into the pipeline and then Palumi just picks that up and that certificate has a TTL of like the time it takes to run this pipeline plus 1 minute essentially. So even if someone were to compromise the pipeline, the credential that's there is not valid for for very much longer. Okay, so things you need to actually do this at scale because I think a lot of people have probably gone out, they've deployed a spire server or something and or maybe the three people that raised their hand earlier when I said do you know about spiffy? Uh but there are some things that you need to implement the system. You have to have a SPID issuer. There are lots uh you can go to the spiffy.io docs overview page and the list is like right there in front. Those are comprised of central servers to run the issuer that is uh provides the certificates and then you run agents out where you want to provide those certificates. So might be a Damon set in your Kubernetes cluster. It might be an agent running on your EC2 server or whatever uh lambda gets and other serverless gets a little more complex. And then you probably need to make some code changes or add proxies or whatever to create this uh the interlink. And then the other thing you're going to need is an administrator for that issuer or administrating team. You'll need an initial implementing team and some kind of group focused on setting internal best practices. Now, as I go through the rest of this presentation, you're going to see that you're going to look at this like this is implementing any change in any organization, Dave. Like these are all the best practices. So yes, and what I'm going to do is try to tie it back to some specifics for this technology that you want to focus on. So core pieces are setting your goal. There are lots of benefits you can get from this technology and if you don't pick the one that's most important to you, you're going to have a much harder time getting buyin from folks. You're going to have a harder time presenting a clear outcome for this project and this can be a big project. So having that clear outcome that you can communicate to people is really critical. Some of the main ones are saving dev time, right? reducing that credential maintenance, speeding up creation of new projects by not having to send people out to, you know, provision and and maintain all their secrets in the first place, reducing breach risk, not having those things stored, you know, those static long live credentials, and enabling new capabilities like crosscloud or things that are hard to do right now. The next piece is picking your simplest use case, keeping it tightly scoped, making sure that you can get it done quickly, and then figuring out, okay, how do we scale this out? So on the goals, saving dev time, efficiency is the number one that uh uh you like goal that we're seeing people pick. It really does help to to reduce that startup time and and on the other side for the platform or security teams, it takes a ton of burden long term out of maintaining and rotating everything. Uh reducing breach risk is the next one we're seeing. And then uh enabling new capabilities. A lot of people have the capabilities right now to do this, but they're doing it by having a long live AWS keep pair in their pipeline or something. Um, so a case study on that, right? We were working working with an organization, very large uh financial organization that was working trying to pick like, okay, what do we do first, right? And they decided they had two use cases. They they were going to use this for GitLab CI and they also had a workload to workload use case that I'll talk about in a minute because they did end up moving to that. And they went, "Okay, our first goal is dev efficiency." And then going off of that, they kind of calculated, "All right, GitLab is going to get us there faster on the dev efficiency side, and we're going to put work workload to workload off for later." For your simplest use case, keeping it tightly scoped, making sure it's easy to implement, finding a team that can do the experimentation, right? So, some places you could start, some examples would be CI to a cloud provider. you have a CI system that's outside of a cloud provider, you want to, you know, either reduce the blast radius or make maintenance of that much easier. That's a great place to start. A small group of servers at the edge. We have some small group of things that's pretty tightly bounded to itself that maybe needs to reach into the cloud and it's running outside. If you have that one little edge use case of like we're 90% AWS, but we've got these 10 servers over in Azure, that's a great place to start with this, right? Another is an actual isolated data center. You have actual isolation of the thing. You know exactly where the bounds are and you can enable new capabilities within there. That I'll talk about in a moment on the workload to workload side. Um having a single enthusiastic team keeping it scoped to the team level, right? Uh you have a much higher likelihood of adoption. So case study on this was after that CI implementation. So they we kind of met that goal with that organization and I'm going to keep this all in sort of one story here. The next thing they wanted was they wanted to start on the workload to workload thing. that that is the biggest scariest like final frontier for a lot of people because the moment that you say we're going to completely change how we do authentications now okay that's a 10-year project right but if you can find a single place so for them they had a data center everything that talked in that data center stayed in that data center but they had multi-tenency within it so there was a real problem to solve right they had multi-tenanted control planes speaking to multi-tenented service services and they needed to make sure that only the right control plane could speak to only the right service but they couldn't implement network controls because it's in just you know racks and so this was a perfect place where issuing a strong identity to each of those services was perfect for going ah I'm service B I should only accept from control plane B I'm service A I should only accept requests from control plane A etc it starting out with that single team right uh moving on to reusable tools by having collaboration between that single team and the team that has broader respons responsibility for that thing and then eventually moving it out. So the case study here is going back to that GitLab CI there was that one team they just said okay we're going to do this we're going to try implementing you know Spiffy's certificates within RCI they kind of had these rough scripts that were very like long not very optimized in terms of ergonomics for the team but they got the thing working right the cloud team then noticed oh hey that's great because what the cloud team does is every time a new project spins up they issue an uh they create a new AWS account They create a GitLab pipeline that sits outside of AWS and then they provision an like complete admin AWS key pair into that pipeline. So every one of those pipelines has a blast radius of all of AWS within the account, right? That's a lot of crypto mining servers that someone could spin up. Yeah, I know now who has had that happen. Um so so the cloud team went oh well that's we would we could get you know once they saw this team that that one team came to them and said hey by the way you don't need to rotate our key pairs anymore because we wrote that we literally create them on demand and expire them every single time a pipeline runs. the cloud team went well that's got value for us and they worked with that team to like you know hone in on the better versions of those scripts and then they're slowly you know going to do a staggered roll out across the rest of the organization until it's standard operating procedure now special consideration here for getting started as well calling out beyond those is your spiffy ID structure so keeping it as simple as possible it's easier to add complexity than remove it uh it's tempting to be like oh yeah our structure for those URIs is going to team, app region, pod name, you know, pod id down at the end, right? But if you can keep it smaller, that's better. um some specific example there was that organization decided to do like slashteame and then slapperci because they're like well the point where we will have differentiation in the data needs is there then you if you kind of want to differentiate like what's critical versus like to have cryptographically guaranteed versus not do I need to know the region or could that come across in a JWT claim or just in other metadata sent by the request and where can it fail with 40 seconds okay checking with every team. The number one place we've seen this fail, especially with workload to workload, is that a team is like, "Oh, yeah, this is great. We're going to implement this." And then it turns out that they're using it for apptoapp communication. It expands beyond where they thought. And the team that runs the internal PKI is like, "Wait, you you're issuing certificates with what?" Um, and so, you know, being sure that you can integrate with the broader PKI system really critical. If you have technology where it's not baked in, making sure you have solutions for that ahead of time. And the number one objection I get is, well, what if someone spoofs the metadata? And my quick answer to that is, if someone can get into your system, know exactly what five pod labels it needs to issue an identity, and can create a workload that spoofs those things, this is not your problem. You have a larger problem than that. So, quick takeaways, dev efficiency, decrease risk of breach, new capabilities, and I've got a list of resources for you to look at in the slides when they get posted. And thank you very much. Um, excuse me. What's the maturity of dealing with revocation for the clients that you work with? Dealing with replication. Revocation. Oh, revocation like of events that require, you know, the issuer to be re revoked. the the issuer. Okay. Or individual workloads, either one. Like how does that work? And yeah, totally. So I think the first piece like on the workload end the main thing is keeping TTLs short because the agents are available to provide the certificate kind of on demand for any given workload. you um and this gets into a bit of like node at a stationation versus workload at astation that I didn't quite get into in that spiffy intro piece, but you can keep short TTLs because you're constantly refreshing. So that makes it very easy to just expire them out versus needing to explicitly revoke them. Um I'd say that's the main place to start. And then you know revocation capability is different across the different backend systems that do all of this. Um, we haven't run into a lot of need for revocation simply because of the short TTLs. That's my very like we haven't come in I haven't seen a a I haven't personally experienced a situation yet in which they've needed to revoke because there's been a breach or issue that need required the revocation like not to say that it's not an important thing yet. I one more. So the cloud and Kubernetes thing is really clear. Uh but have you seen successful at scale implementation for legacy database services like Microsoft SQL and Oracle and you know that that zoo? Yeah. Um yes I have. So so this is again like that end piece about the limitations and and where can it fail right is like the technology considerations. Um, I'll say that like we are seeing this and we're working on a thing we kind of have in the mix at our organization is uh creating like a translation proxy that just sits in front of it and is like a pure proxy that just does the spiffy verification and passes the traffic through because that is a thing that our customers are asking for. Um, and that's something we'll hopefully open source. But uh but yeah, I think like that is one of the core limitations right now is that if there's not a proxy for the thing that you need and it doesn't natively support it, then that's that is a a real blocker for a lot of technologies until you can figure it out. Uh I have one please. You uh did not mention AI at all and MCP and ADA are workloads that are starting to need identities. Has there been any discussion of bringing these things together? Yeah. Um, I'm going to let you know after August 1st because my team's my team about a month ago was told figure out what this means for agentic identity. And so I think figuring out the definition of agent is going to take all three days of the hackathon. Um, but tools in a loop according to Simon Lewis. Yeah. But I mean, so to me, right, like if I picture an example use case here, right, bedrock, bedrock is running in lambdas. If you can issue an identity to lambda and you know what lambdas are running for a specific agent then that's my initial brainstorm on we want to provide an identity to this thing and then any and then if you're providing tools to your agent right and the tools are essentially just you know a function that's like call the database here right then you should be able to build using that identity certificate into those calls I'm telling you this completely theoretically because I haven't done it yet uh but from the the early architecting we've done it would look something like that. Yeah. Cool. Any other questions? Uh yeah, so I had a question around how do you tie this into declarative connectivity, right? So you should be able to say connect this entire ecosystem to your service catalog and then be able to say this service or this workload can talk to these three other things. read only, write, update, operator like is there any standard and have you seen that work at scale especially on the direct declarative nature of it like how do we use this uh to bring uh to to tackle a lot of multicloud and even on-prem all kinds of hybrid workloads. Yeah, that is a great question and you'll notice that nowhere in my presentation did I really discuss how authorization works. Um because there's no authorization rules built into this standard, right? It's completely about identity issuance and authentication. And right now I I think you're you're saying is there is there a best practice or a standard and I would say not yet. Um you know authoriza using this for authorization is currently pretty bespoke. Um and I think we'll really you know if you're talking about tying it into a service catalog it's going to require that service catalog supporting reading spiffy ids right and understanding them. So I don't have a great answer for you. I I have an answer for you on that one. It's not great. It's no. Yeah. Thank you very much, Dave. Yeah. Thank you.
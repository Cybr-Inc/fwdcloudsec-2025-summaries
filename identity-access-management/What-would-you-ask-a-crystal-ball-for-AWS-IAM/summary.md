# What would you ask a crystal ball for AWS IAM?

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=nkLNlvXZ8CM)

- **Author**: Nick Slow
- **Talk Type**: Cloud Security

## Summary

This talk addresses the immense difficulty of understanding and analyzing complex AWS IAM configurations, particularly in large-scale environments like Netflix. The speaker, Nick Slow, details the shortcomings of existing tools when faced with performance, accuracy, and integration challenges. He introduces Yams, a new open-source tool written in Go, designed to perform high-speed, accurate IAM simulations to answer critical access questions, model hypothetical policy changes, and ultimately enable quantitative security metrics like "exposure" and "efficiency."

## Key Points

- Standard tools for IAM analysis often struggle with the scale, complexity, and advanced features used in large cloud environments.
- The complexity of IAM, with its many policy layers (identity, resource, SCPs, permission boundaries), makes it behave more like a programming language than a simple access list.
- Answering questions like "who can access this?" or "what happens if I change this SCP?" is critical but difficult to do with confidence.
- The speaker created an open-source tool called **Yams** to close the capability gap between the questions security teams have and the answers existing tools can provide.
- Yams uses AWS Config data to build a model of the environment and runs a high-performance simulation engine to answer queries.
- It can be used for troubleshooting, security posture analysis, boundary analysis (e.g., test vs. prod), and modeling "what-if" scenarios.
- The talk proposes using metrics like **Exposure** (how many principals can access a resource) and **Efficiency** (needed access vs. granted access) to create KPIs for IAM security and translate technical work into business-friendly terms.

## Technical Details

- **Tool Introduced**: Yams
    - **Description**: A library, server, and CLI for high-performance AWS IAM simulation.
    - **Language**: Go
    - **Source**: Open-sourced at `ncio.github.io/yams`
    - **Architecture**: Consists of a server that loads environment data and keeps it "hot" for fast querying via a REST API, which is used by the CLI.
- **Data Source**:
    - Primarily uses **AWS Config** as the source of truth for IAM policies, principals, resources, and account structures. The data is expected to be available in S3.
- **Core Features**:
    - **Inventory Management**: List and query principals, resources, and API actions within the environment.
    - **Simulation**:
        - `yams sim <principal> <action> <resource>`: Simulates a specific, fully-defined scenario.
        - **Query Expansion**: Omitting a parameter (principal, action, or resource) causes Yams to simulate all possibilities for the missing piece. For example, finding all principals that can perform a specific action on a resource.
    - **Explanation and Tracing**:
        - `--explain`: Provides a short, human-readable explanation for an access decision (e.g., "explicit deny in resource policy").
        - `--trace`: Outputs the entire, detailed evaluation logic walk-through.
    - **Hypothetical Scenarios ("What-if")**:
        - **Overlays**: Users can provide a JSON file that redefines parts of the environment (e.g., a modified SCP, different resource tags, a new policy) to simulate the impact of a change before applying it.
- **Performance**:
    - Designed to handle very large environments (e.g., Netflix).
    - Can load a large environment from S3 in ~10 seconds.
    - Can run billions of simulations in 15-30 minutes on a multi-core (e.g., 128-core) EC2 instance.
- **Key Metrics Proposed**:
    - **Exposure**: The number of principals that can access a given resource.
    - **Efficiency**: The ratio of principals that *need* access to the principals that *have* access. Calculated as `(principals needing access) / (exposure)`.
- **Prior Art Mentioned**:
    - AWS Policy Simulator
    - AWS Access Analyzer
    - pmapper
    - I'm spy (likely a reference to Cloudsplaining)

## Full Transcript

Hello everybody. Welcome to our last long session of the conference. Uh this sess talk is what would you ask a crystal a crystal ball for AWS AM by Nick slow and our sponsor for this session is trust on cloud. So thank them for making this possible. Uh and as always we'll be back for Q&A at the end. All right. Thank you. Let's see this good. Cool. Uh yeah. So welcome to talk. What would you ask a crystal ball for as I im so hope everyone can muster some energy. Um and I hope you like IM stuff. Um so a little bit about me. Uh I'm currently on the cloud security team at Netflix. Previously I was at Apple doing pretty much the same thing. Uh my GitHub is there. Uh my LinkedIn as well. I don't know why I put it there. don't really use it but if you want to reach me it's theoretically possible. Um and favorite AWS services which is my new favorite icebreaker for conferences like this. Um obviously I am um and big fan of S3 and SQS2. Um a bit about this talk. So this began as a 20% project at work. Um the name of the talk is actually the name of a survey that I sent out to a bunch of people I'd worked with previously and some other folks in the industry just generally asking you know what problems you have with IM like what would you like to know more about and if you have opinions and want to contribute to my market research um there's a QR code there um but this fed into some internal tooling and projects I talked to some other people and generally what I heard back was hard same like everyone kind of like runs into this no one has quite the understanding they would like to Um, I'll caveat this by saying it's it's definitely still early days. Like this is I guess technically started in like 2023. Uh, but it's taking a long time to start like figuring things out, building out the capabilities. And so I still think we're we're just like tip of the iceberg here. Um, I also am aware of the irony of presenting a talk about Netflix stuff and saying like, "Hey, uh, you all have this problem too." Maybe at the same conference where Ramy said, "You are not Netflix." Um I I I know but bear with me. I think there's at least some some overlap here. Uh cool. So let's talk about the problem or how this this kind of started. Um so a little bit background on Netflix very large cloud environment. Um very early to the cloud. Uh made some decisions that you would probably consider mistakes today for sure. Uh and we just have a whole spectrum of infrastructure. I say like old as dirt to bleeding edge and that's very true. like there are resources that are older than the career of many people here and there's also stuff that like just came out that we're trying to make use of and we have every conceivable access pattern. So because of the age there's often sort of a disconnect between the original intent of how access was meant to be stood up um and what it actually looks like today. you imagine doing um deprecating a bunch of role types, doing migrations, take advantage of all the IM features that come out and just doing that, you know, over a decade or longer and you start to get some very interesting combinations of those things and things that just don't play very well together. And so these these kind of like archaeological layers of going through things. Um so the the start to this actually has nothing to do with cloud security uh altogether. It was it was kind of a little bit of a more boring start. there was a desire to make like an enterprise security metrics program. Um, and kind of what that meant was start building out some metrics for security, gauge how we're doing, um, and make them very standardized across domains and and obviously quantitative. So, it was a lot of like requiring simple answers to simple questions. Um, and we quickly found out that we didn't have those simple answers. Um, so this is kind of how I would frame it. um the domains that we were uh working with had a much more straightforward access model. So if you get asked, you know, who can access this Google doc, you go look at access control list and you have the perfect list of people who can access this Google doc. Um and then trying to do the same thing in the IM world, as I'm sure many of you know, it it looks very different. So as those capabilities for describing authorization go up, it makes it harder and harder to model and harder and harder to kind of get right. Uh so as a a fun little exercise um I'm sure a lot of you have had this sort of uh thought exercise before um but like can can you access or delete objects in a bucket across account you start to think about all the things that go into play here all the things you need to to answer and it's pretty intimidating you know you get inline and attach policies the resource policies permission boundaries RCPs and SCPs um default root trust which is service specific And then my favorite edge case is if you have a principal and an account and the principal has no permissions whatsoever, but there's a resource policy that trusts that principal explicitly, it has access. So you might look at a role or a user and say, "Oh, it cannot do anything." It can actually potentially do a lot. It's very very hard to know all these things um keep them up to date and then build a broad understanding of things. Um these are very good capabilities though like I I think all of us would be hardressed to to do our jobs if we didn't have this. There weren't many many ways to describe the the authorization story in AWS. Um but they also do make our jobs a little uh a little harder at times. Um this is an old tweet that I made back when I used Twitter. Um I didn't own the cloud chip posts handle but uh saying my favorite programming language is I am not principal condition statements and while was meant to be kind of a joke uh it does kind of ring true in that uh the more and more you get into the advanced features of IM the less and less it looks like something you can very quickly answer. It's not like a sort of static access list. It's a lot more like a programming language. Um and so if you want to answer questions about it you have to be able to to work with that. So um regardless of how complicated it was uh we did need to still participate in this metrics program. We still needed to to answer these questions. Uh luckily I am not the first person to start work in this space. There's a lot of work that came before um and had every intention of using this. So policy simulator OG I'm sure a lot of you have tried it. Surprisingly, even though it's first party not the most accurate on simulating IM policies uh and pretty clunky to use uh access analyzer uh has sort of been growing over the years higher level insights most recently sort of like resource level insights too if you are able to pay for them. um very expensive can explain in a bit why we could not uh pmapper uh is a tool I took a lot of inspiration from that was one of the original tools that can answer some of those like who can type questions you know build an access graph inside of your environment and and be able to query it for things um and I'm spy very cool too and know people here have probably written and or contributed to these um and the uh I do want to stress like I went out and built some other tooling and like Netflix started going in a different direction. But if you have workflows that use these, I highly recommend sticking with them. Um because uh we we have some unique constraints that very much so might not apply to you. Hat Ramy. Um so the game plan here was wire up some tooling, make some data, move it around, make some graphs done. So somewhere in the Jira I have a ticket in the height of hubris that's like this will probably take like a week. Uh and that was in 2023. So obviously this did not take a week. Uh ran into some some some interesting challenges. Uh so the biggest problem was that the the prior art kind of fell over and no shade at all against these tools it just didn't really work in our environment. So the first problem was accuracy. uh we were using a lot of IM features that just there wasn't support for uh performance was probably one of the biggest um just loading the environment was very difficult uh if you take all of our access policies and put them into JSON file it's many many many gigabytes um and then high volume simulations so everything worked really well doing like thousands of simulations um start to fall over when you get into the millions and definitely into the billions um and the last one being integration so we wanted to connect this to some other tools that we had and just the the way that the tools were designed, both the languages they were written in and also just their their model of operating didn't work well for that. Um, so uh it all kind of culminated in uh I was really really intent on using pmapper. I tried to get it to load the environment. I waited for like eight hours for it to try to build this access graph. It just kind of ran out of memory. I control seed out of it. I was like, "Okay, back to the drawing board." Um, and I kind of came to this conclusion uh that there is a capability gap between the questions we want to ask of IM and the answers our tooling can give us. Like we needed these very simple questions. Um, and there it didn't seem like there was anything out there that could answer them very well for the environment. So part two of this is about closing the gap. Uh, so this is really how it felt at first. um like just about every approach was okay like I need to know more and more about IM and you end up just getting to the point that you're re-implementing it or rebuilding it. Um and I went to start with kind of no shortcuts and I ended up taking a bunch of shortcuts. So we'll talk about like the failures of this journey along the way. Um so the very first version of this was a tool called IM metrics next. Um, it was a Python script that focused on local IM simulation. Again, very heavily inspired by Pmapper. Really just tried to answer one question so we could get these metrics kind of out of the way, which is how many principles have access to this resource. So have access definitely a little bit more art than science, right? Like there's many different forms of access, but we were looking especially at like data plane operations, anything that will let you read, write, mutate, delete data. um and started with uh support for tag based conditions because we were on this Aback journey and we were building security controls that used Aback heavily. So it was very very important that we could answer questions for policies that revolved around tags. Um but there were a lot of drawbacks. Um so one is we're missing the who and why. So we could give you a resource and give you a number of people who could access it or a number of applications that could access it. Uh, and it turns out that's actually not super useful because your next question is, uh, well, who who can do that and and why can they do it? And it's like, okay, well, I can't answer that. Just it's it's it's a number. Um, limited API call support was really focused on just a few services. Uh, same with the resource type support like all of it was kind of hardcoded how you pull out the policy and just doing some very very basic access metrics took 19 hours per run. uh it was not really a very agile tool and people come to me with questions and be like I can change something and tell you tomorrow maybe uh and it wasn't like a great uh iteration uh so as far as like why it took 19 hours talk a lot about performance in this uh but it's just kind of a numbers game where the numbers are stacked against you so if you want to calculate access across a very very large environment you have your number of principles times your number of actions times your number of resources and that's the number of simulations you have do. So for our environment, we have well over 100,000 roles, um a handful of actions if we were just talking about basic simulation and then many many resources. For example, if you just look at like SQS, we also have well over 100,000 SQSQs. So you do the math on that and just to answer the question for SQS, you're looking at like somewhere between 50 and 200 billion simulations. And it doesn't really give you a lot. You want to do that across a lot of different services. You want to be able to ask different questions. um ended up being performance limited pretty quickly. Uh so there was a second one I just I I cloned the script. I called itrics 2. Uh and just tried to to butt my head against the problem a little bit more. So this was a lot of like parallel parallelization and optimization. Uh simulation is mostly like an embarrassingly parallel uh problem. So you can just kind of add cores to it, scale it out. It kind of works. um added support for org based conditions which was another thing that we were starting to do a little bit more of and so needed to to model and then take the results put them in the data platform. Um ran into a lot of the same issues was a little bit better but found that that who and why is very very important. Who is really needed for the investigation and the why was actually a lot of debugging. So needed to know why a certain access decision was being spit out. um because it is hard to have confidence in a tool that just gives you an answer and can't really tell you why, especially when you're developing it. But we did get managed to get it down to two hours for generating these basic access metrics, which is better, but still not, you know, it's not synchronous. You're still putting the data somewhere and then trying to answer these questions after the fact. Um so it felt a lot like this. So it was just kind of whack-a-ole. uh I would do some feature work. Someone would come back saying hey like the obvious next question be like yes okay still a 20% project so I will try to find time to work on it. I would get back like a month later. It was not it was not fun. Um and it was a lot of the value of this was just gated behind feature work. So you couldn't really explore the data or manipulate it. You we would constantly have to go back do a little bit more development support something else and then try to reanswer it. uh and a lot of that is because the the domain of IM understanding has breadth and depth that is is much more significant than most other access domains. So in terms of breadth uh you want to pull out metrics and you want to pull out insights like you want a very very broad understanding. You want to be able to step back look at your organization and have something point at the weird stuff or the things you should be paying attention to. U boundaries and perimeters are another good example of this. they'll talk about a little bit uh later. So if you want to understand like how effective my is my security boundary, well that's a very very broad question, right? Like you're not really looking at pointto-oint access at that point. You're trying to model something much larger. Uh so some examples of questions that very actively came up uh at work were who can perform this API call and what has access to this resource across everything? Uh how exposed uh are my critical data stores? Talk a lot about exposure a little bit later. Uh, and then modeling changes like what happens if I make this SCP change? Very, very scary. Um, we've burned ourselves a couple times with with rogue SCP changes. So, it'd be nice to be able to make a change, see the effects of it, and then proceed with a little bit more confidence. Uh, there's also a lot of depth to understanding IM. So, the first is like troubleshooting. And this comes up a lot for our team. We manage most of the IM changes at Netflix. it is one team bottlenecking many of those and so our on call is kind of constantly dealing with questions like why am I getting an access denied here or why am I not getting access denied here uh and understanding exactly why is is pretty errorprone so you one you have to just have a good understanding of like well what are all the policy layers what do we typically do why might this come up but ends up being a lot of just contextual and historical knowledge um And it is very very hard to get that to a user fast enough and pretty much impossible for a user to be expected to understand that by themselves. So especially where we do try to hide many aspects of the sharper edges of AWS and IM from our users. Um nobody who's building application should really be expected to know ah yes like this SCP at this layer of the OU would be resulting in this that's where I need to go ask for a change. So all falls in a very small number of people. And so that's where this concept of a a crystal ball really started to come up. So I wanted to be able to answer not just this question but the next one and the next one the next one have a stronger foundation for building out some of these use cases and these workflows. Um if you remember the graph about like capabilities go up, ability to model goes down. Really want to like have our cake and eat it too. We wanted to use all the features of IM and not have that infringe on our ability to understand things. Um, unfortunately that was too much work to do in the 20% project at work. So I decided to kind of strike out on my own. Um, which is good news for you all because uh I created a project called Yams which is open sourced as of today um and is really meant to address Oh, thank you. Yeah. Yeah. Everybody likes code they can run. Yeah. Um, so this is primarily meant to address some of the problems at Netflix. But I don't think that those problems are 100% unique, espec especially other large customers. I think you'll run into a lot of the same things. And this was kind of the foundation upon which want to go build out and do more of the this type of work. So what is Yams? Also name. A couple people ask me where the name came from. It just if you say I am a lot really fast, it just starts to sound like yam. Uh, and here we are. Uh so Yams is a library and a server and a CLI. It was originally just going to be a library. I found that that wasn't super useful. Um and then it was going to be a CLI as well and I found that that didn't scale well and they add a server. So it is the definition of scope creep. Um it's written in Go. Uh we end up using Go for more and more of our cloud tooling. So this seemed like a good choice. Also again really needed the performance. So anything I could eek out a little bit faster seemed well worth the effort. And the goal is pretty simple. will just answer these questions via simulation, be able to do it pretty quickly, uh, and be able to to build more and more workflows on top of it. Uh, so this is a very rough diagram of what this kind of looks like. You have data, which at the moment really just means AWS config. If you're not a config customer, I'm sorry. Uh, but data flows into yams about your environment, your principles, resources, policies, uh, accounts or etc. And then you can ask questions of it and I'm fully expecting the questions to be pretty diverse. So here you might have like CLI users who are doing the troubleshooting aspect of it. Like you're just on call, you're trying to answer a small question. Uh internal app users, we've built a lot of cloud tooling over the years and don't really want to expose yet another UI to people. Really, we don't just want to build another UI is the big issue. Um, and so it's much nicer if it will integrate with stuff that we have today and be able to expand enrich some of those experiences. Um, and also just workloads. So one in a model where you don't have to worry about access to the data. You don't have to worry about loading it. You have to worry uh less at least about the scale of what you're doing and you can just kind of hit an API and get some of the answers. So if we revisit the challenges that brought us here, these accuracy, performance, integration piece, uh, Yams was designed to move the needle significantly on each front. It does not move the needle all the way. Like there is still plenty of work to be done, but we we did want to actually be able to change the feasibility of answering these questions. So as far as accuracy, it has full support for IM features, all different policy types, kisha keys, etc. Uh, the performance. So again that server model came out of mostly needing a little bit of a different way of invoking it. Uh and especially found that is very very hard to answer these questions unless the data is hot. So you have all this gigabytes of JSON file policy information. You don't really want to load that every single time you're asking a question. Um also really wanted to double down on the multipprocessor parallel simulation side of things. uh integration wise uh it's really built around that kind of like REST API as well as the Go library. So uh expecting like some people to be able to integrate with it natively. Sometimes you might have to use uh the the REST API or the server instead. But there's a way to take a tool that you have and get it integrated into this this tool. Um so yeah, I'd love to show you what it kind of looks like in practice. And hopefully this is pretty visible all the way at the back. I know the the font size has been a bit of a problem. Um, so this is based on a very real world uh small test environment. The account IDs, that sort of thing are all very real. They are my personal account. Please don't go poke at them. Um, all these examples are going to be for the CLI, but the CLI is a very naive wrapper over the REST API, so it all translates about the same. Uh, so first is you would start up uh a server and you'd give it some sources. And so uh sources are periodically refreshed. Uh they can be local files or in S3. We keep a dump of all our config data in S3. So it's kind of built around that use case. Um in theory you could expand this to just be about any source, anything you could read from uh especially file-based. Uh so whereas before things were taking a very very long time to load our environment and be able to even be ready to answer questions, um this took about 10 seconds to load. So it was much more usable from like I need to start asking things right away. Um you can run a stat command to see the size of sort of your universe like when the last time the sources were updated and also how many things that you have. You see like 1445 for entities. Most of that is the list of AWS managed policies that we have to pull in to be able to simulate. Um so the first capabilities are around inventory. The idea here is put all of your policy information at your fingertips. So uh you can list all of your principles. Um we'll also be truncating all the JSON just so it kind of fits here. Um you can query against your principles. Uh here we're s doing a case insensitive full text search. So re green roll red roll. If you want to see the details uh on any of these you can use the dash key flag and it'll show you just a summary of the principle. So this data is as fresh as you can get it into S3. Um we do about 15 to 30 minutes for adabs config. Um here is a similar query for role that has a permission boundary. So you can see it at the very bottom there. Um but you'll also probably see that uh it is a reference rather than the full policy. And that's because things like the manage policies permission boundaries only get resolved at simulation time. Um it's sort of like Shinger's cat, like Candace's cat, soon like we have to pull in the very current definition to know. So everything kind of floats up once we simulate it. Um we'll pull in all the full policy information and give you an answer back. Um but if you want to see that full piece, let's say you just want a deeper understanding of like what does this what are all the um affected permissions on this IM role, you can throw on the dash freeze flag and you'll see this permission boundary expanded out. And you'll also see things like accounts and SCPs uh get resolved at this time. So it's a way to say for principle, for resource, show me everything that relates to it. I want to be able to see like the the full universe of of permissions. Uh same is true for resources. It's really only interesting for things with resource policies, but any other resources that you define would be in there as well. Uh and you can also inspect the individual API calls. So I think a lot of people have worked with like the service authorization reference or those IM actions conditions web pages. There's a programmatic version. This pulls it in and exposes it. So you can see things like what are the condition keys that would apply to this. What are the resources that this would apply to? That sort of thing. Just a little bit of an easier way to answer those questions. Um but what do we really build this for? It's for simulation. So this is an example of what this looks like. Um the sim command takes a principal action resource and gives you back uh a result. So was allowed or denied echoes some of that information. So this is like a single individual well-formed scenario just you know I want to test this one piece out. Um we talked a good bit before about like breadth and the types of questions we'd have to answer there. Um and so this is really most useful if we can answer some of those higher level questions. So if you leave out any one of the principal action resource tupil pieces uh Yams will attempt to fill it in. So in this case we left out the principle. So Yams is going to tell you every principle that can perform this action against this resource. Um if we look at a resource definition with a deny base bucket policy and aback condition, this is something we do very heavily Netflix. Um we can see okay so this um the red ro that we saw earlier did have full S3 permissions against some of these uh buckets and so when we see it here in the which principles can do this it's a little bit weird to see it absent um and that's because of this deny policy so if we remember um like why you know all the different policies that apply to this uh we see that Okay, yes, there was a policy back there that I should recall had a allow permission. We see a deny here. You can kind of start to piece it together. But if you don't want to piece it together yourself and that's kind of the situation we find ourselves in constantly. Um, and so there's an explain flag and I will try to give you the shortest possible human explanation as for why this is happening. So Red Roll does have access to this bucket or at least it should except for that deny piece. So the explain flag will show you okay here's an explicit deny in the resource policy. There's also a trace option which will just walk through the entire evaluation and tell you how it uh came to the conclusion that it did. So you can see it's going to start with the resource policies. It'll do SCPs. It'll move through everything. The output ends up being so large as to almost not be useful. But if you're very very uh interested in why a particular like condition is evaluating the way it is, this will tell you why. Um, similarly to omitting the principle, if you omit the resource and give a principle and action, Yam will tell you, okay, this is what you can access with that principle and action. And then if you omit the action, you will get the a similar answer. So these are all the actions that this principle can perform against this resource. So it's an easy way to kind of like pivot around the different principles, actions, resources you have, and be able to answer these questions quickly. Uh so being able to simulate your your org or your the the universe of things that you know is useful. Um but why limit yourself to things that actually exist? Uh so one thing that uh my team has asked AWS for a lot in the past is I want to understand what happens if I make this change. It's a policy change or a tag change or an account move. What happens? Uh and so uh one of the things that Yam supports is the uh concept of overlays where you can basically redefine your environment the way you want pass along those redefinitions uh and then get an answer back saying okay well if you had made those changes this is what would have happened. So in this example uh it's much more minor than that but here is the uh pulling the in policy definition for red roll. we will make some edits. In this case, we are throwing on S3 put object ACL permission which it did not previously have and then we can simulate it and we can add in that overlay. So, whatever you can define in that JSON file. Um maybe you want to change something about the account or the tags in the resource or a separate policy definition or really anything you would want to edit. Uh and then you get the results back. And in this case, we have this put object ACL that wasn't there before. And so this is a way to be able to uh work towards some of those hypotheticals that we had talked about before um as far as like you know what would happen if I do this. Cool. So now we have a place to ask questions. We have the data required to answer those questions and a simulator that can take the data and the logic and kind of put them together and produce something useful. It's not yet a true crystal ball. like there's there's still plenty of things that come up, but it is something that feels a lot more foundationally appropriate for the questions we're trying to ask. Um, and here are some examples of those limitations. So, I mentioned like a reliance on a config that's very real and we're not really sure how to solve that. Uh, we definitely don't have the ability to assume roll into every account and start describing resources without hitting some very serious uh service limits and just the time of that would be wild. Um but AIS config is missing authorization data for a handful of things. Uh and so that does limit us pretty pretty strongly. Um inconsistencies in IM are still very much so there like there are uh plenty of examples. There were talks at this conference about examples of that. Um and it's just all too common to find out that like oh there's like the one way that condition keys can evaluate for this particular action or like uh the multi-resource API calls like EC2 run instances. um they just have very very different characteristics and so it can start to get kind of clunky. um it works best for uh direct like principle to resource interactions. Um but we do have this uh and so if we take that and we know it's not perfect, we can at least get to the fun part and see what we can point it at to uh make our lives a little bit easier in a few different areas. Uh so the first is troubleshooting. So one of the things that we we constantly need is that policy info at your fingertips and where these access decisions are coming from. Um and so we have some self-service tooling for users console me. We've talked about that at some previous uh conferences and the goal is to do a little bit more integration there so that the troubleshooting aspect of before you would go make a permission request you can understand is the change that you're making even going to fix it. We see pretty frequently that people will like try to give themselves permissions for something that it's not actually that they don't have permissions. They're running into a control elsewhere. They're running into a permission boundary. They're running into an SCP, something like that. Um, and especially for our own call, the ability to just move around between all these different policy types um expecting it to be pretty useful. Uh, posture checks is another place that this has come up before. So um we we have the kind of standard misconfigurations that many people have um things like public resources but there's a lot more that we want to ask. We have you know environments that we don't want to expose elsewhere. We want certain uh I have a co-orker who really wants me to use the term security invariance and I refuse to use it. But like things like that uh you always supposed to be true always want to check for them. Um and this is a way to build out some of those checks. uh the two categories I don't really think this is quite the right naming for it but you have like algorithmic checks like very very like should this be possible should uh should this principle be able to access this resource should you be able to hit this resource from outside of this account those types of things you also have heristics so you know how many things can access this resource um am I seeing like a large jump in it is there an anomaly there uh some of those more higher level questions that you may want to ask of your environment U boundary analysis is one that a use case that came to me externally for somebody who is very very interested in data parameters specifically. Um for for us it's a a test versus prod like we technically have a test and prod environment. They're not as well separated as they could be. um and on the networking side they had the ability to analyze like how much traffic goes between them. And we we got very jealous of that. And I want to be able to answer the same things on the IM side like how much access is there from test to prod. If you can simulate enough of the principles and resources is actually a question you can answer. Uh standard and regulatory is another one like I think we and Netflix gets off pretty clean on the regulatory side. We mostly just deal with credit card processing but other people with a more um intense requirement to have strong separation and segregation between their environments. uh can use to see how many things cross over. Um but most importantly, let's talk about how we got into the whole situation which was this enterprise metrics program. Uh so if you're a manager, get excited. This is mostly for you at this point. Um we've long had this difficulty of translating the value of IM security to a larger especially executive audience. So I am frequently in the position to uh summarize my team's work upwards and say like okay we we did a lot of really really good work that eliminate access but you have to read these like three paragraphs to understand exactly why and how far and it doesn't really do the work justice. Uh and so what we really have wanted for a long time is this like KPI for cloud access like can we just look at a couple of metrics and tell you how the program is doing or describe these projects um and like the effects that they are making. So there's two concepts we're going to talk about briefly which are exposure and efficiency. So exposure is pretty simple. How many principles can access this resource? So a measure of uh how much access coming into a particular resource is there. And this number is very contextual. Um in some cases it should probably be zero. In some cases it might be like 5,000 and that's expected. That's great. Only you really know what the exposure of various resources should be. is probably uh a lot of ways you would want to slice that data. Efficiency is one that's also very interesting. So if you take exposure and put as the denominator uh and the numerator is how many principles need to or currently are accessing a resource, you start to build this idea around like how close to right-sized access am I. Uh so if five principles need access to it and five principles have access your efficiency is one or 100%. You're doing great cloud security. Um if one principle needs to access it and uh or let's say 10 principles need to access it and a thousand can your efficiency is 0.01 or 1%. And you're very very very far from right- sized access. Uh, and so these metrics are useful because they're intuitive and they're pretty easy to understand. Like you you would want exposure. You hear that word, you want it to go down. Efficiency is always a good thing. You want it to go up. So you can intuit it about where you would want these different um pieces of your IM story to go. So how do you get these numbers? Uh exposure em was kind of meant to solve one of those big pieces. Um but also any other IM analysis tools. Um how many principles are or need to access this resource is tough. Um if you can use cloud trail and data events do it. That is that is by far the best way. Um we absolutely cannot afford data events. Uh so we gave a talk at reinvent 2022 about instrumenting SDKs as our way of uh data events for for cheap but with gaps. Um but maybe you also have org context on this and you can start to say like oh yeah these tagged resources are supposed to be locked down. uh this is what access is supposed to look like in this environment like I'm comfortable with you know certain thresholds um and those translate very very nicely into OKRs which was also a big motivator for doing this um so here are some examples of the types of OKRs we've wanted to write for a very long time uh one of them is like average access exposure in environment it's always a number you'd want to drive down right you can understand in aggregate and you can report on the measure of success of you doing IM am on that number trending downwards or maybe it's more targeted like your sensitive data stores or like a particular bucket where you want very very high efficiency very very low exposure overall things shouldn't have access to your your critical data stores unless it's actually needed and you can actually quantify that and say yes we're either very close to or far away from the goal um maybe you want to look at the prod environments exposure to test like I mentioned before or maybe you want to look at the IM roles that are available to many many people and say like we want those to be more efficient. If if we're giving out roles to a bunch of different uh people who work at the company, they shouldn't have a lot of permissions that they don't need and you can start to to measure it and move closer to something. Um I have some colleagues uh this conference's own Patrick Sanders uh and Joseph as well uh who gave a talk at reinvent last year uh talking about some of these things. So this was like using the concept of exposure to chart the progress and the impact of account migrations. Whereas normally you'd say like yes we're moving things out of big count it's a good thing trust us. You can also really start to quantify that and say like there's one key metric we are looking to drive down and it's this concept of exposure. Um so what is next for uh this work? Uh the first thing is integrating more deeply with Netflix tooling. We're still mostly on that like metrics v2. I would love this to be pulled almost like reverse open source back into Netflix to solve these problems. Um performance is still a thing. Uh it's not chasing performance for performance sake, but want to be able to do more things. Uh be able to calculate data and throw it away. If we don't need it, compute should be pretty cheap. um we want to be able to answer more things synchronously instead of asynchronously. and uh have on demand answers. So I've talked a lot with teammates about this, but like if you go view a resource or you're looking at a resource uh or an IM policy change, being able to see the effect of it or you're seeing uh the uh someone's requesting access to a resource and that resource is very very minimally exposed or extremely locked down, you should be able to see, you know, a green number or red number to help you understand contextually what's happening inside of that request. Um the real goal overall is that AWS provides this like there have been not one uh not two but three talks at this conference about making like understanding IM more um and it doesn't really make sense for people to keep building these tools when like ads has this can they can answer 100% accurately um and probably do it better than any of us uh but it's we're still yeah u but if interested in this, I highly recommend uh David Kerber's talk from yesterday and uh Jason Cow's talk from today about just like uh you probably are making mistakes and I am need to understand it better. Here are tools to try to help with that. Uh we do need more firstparty support. Um yeah, so that is it. Uh things are available at ncio.github.ios. I want to leave a little bit of time for questions, ideas, light roasts we have, but thank you for staying and hope this was as fun to hear about as it was to build. Thanks, Nick. We do have a couple of questions in the room. Uh the first one I think you did cover, does yams require a server or can you do the CLI standalone? Uh you can kind of do the CLI standalone. You'd have to run a server locally. So, uh, you do need some server running. Got it. And then Eli asked, does the overlay let you evaluate across your entire policy universe to the scenario of what would happen if I did this? Being able to evaluate the effect of SCP changes across all principles would be super cool. Uh, yes, that's actually the exact use case it was built for, which is you would redefine the SCP, the policy, write it at whatever you want to be, and then start asking questions based on that. And there's one more question. Probably easier for you to look in there. All right. Do we have questions in the Oh, no. I have a personal question. Why didn't you invent this when I was still doing cloud security assessments? Hi, thank you for the talk. It's very interesting. I I won't. So, hypothetically, if you had an environment the size of Netflix, uh how long do these simulations take to run? Yeah. So a lot of that really depends on the the scope of it. So it's that like number of principles, actions and resources. Um and so for the very very basic things that we were doing before, which is very small number of actions only like three or four resource types, it takes about 15 to 30 minutes just kind of depending uh that is the deployment strategy for this Netflix is go put it on a 128 core uh EC2 and just let it rip because like that's that's all we really know how to do. Uh, I'm Chris Coleman. Um, uh, did you go into why you're limited by AWS config data or like why you're limited to AWS config data? Did I miss that part? Um, I I covered it very briefly, but the the short answer is just uh too many resources to describe very effectively. Um, and then contention for those describe APIs. uh because we we have some other talks that talk about this but like we're concentrating a very small number of accounts and so like service quizzes for those describes are very limited and we found that you try to describe a bunch by the time you finish one your data stale so you just kind of have to go back to the beginning but it's mostly just paging through hundreds and thousands to low millions of resources and you don't have to answer this but uh if that only took you one week what are you do up to next week yeah did not take took a here. Any other questions? Oh, in the back of the room, of course. Um, it did you end up having to essentially rewrite IM or is there any you reusing of the simulator or anything else like that? No, it's just rewriting IM and go pretty much. I think that's the approach that I think a few a few projects have had to take. Um, so, uh, AWS, if you could release a library to do it locally, uh, that would be extremely useful for so many of us. Are you, uh, taking orders on what other parts of AWS to rewrite? Yeah. Okay, we had some more questions. Uh, oh, nope. One was already asked verbally. And, uh, this is amazing. Have you thought about rooting this in real human identity eg or I'm sorry I octa principle identity center etc. Yeah I think that's been the most common question and uh ideally yes but don't really know how um so I think there have to be some abstraction over what does human access to an a a principle look like and genericized enough to be able to do this with whatever your identity provider is. Um, I understand like for many people it's just group membership, LDAP, AD, Google groups, that type of thing. But I I think there's something there. I'm just not really sure how to how to crack it yet. Um, great job on this. So you you talk a lot about the principle to resource and what that kind of simulation looks like looks like. I'm curious on whether you have considered like some of the resource to resource interactions like S3 to SNS or like SNS to SQS subscription or some of those like permissions that are direct service to service interactions or through AWS services. Yeah, again I think there's something there but not entirely sure how to model it. Like I think that resource to resource for things that are you know source ARN conditions starts to become a little bit more straightforward. But when you talk about like trusting account roots or especially like service linked roles that get used by a service for something else I think that mapping is a little bit more difficult. So I I'm going to going to punt on that one for for the future. All right. Thank you everyone. We will kick off again. It's break time at 3:10 or 1510.

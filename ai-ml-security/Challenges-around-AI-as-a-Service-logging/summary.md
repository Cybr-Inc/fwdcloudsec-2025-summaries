# Challenges around AI-as-a-Service logging

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=AccsDqmHPdU)

- **Author**: Jeremy Snyder
- **Talk Type**: AI/ML Security

## Summary

Jeremy Snyder explores the complex challenges organizations face when trying to log and monitor AI-as-a-Service (AIaaS) usage across their environments. With AI adoption surging—now the top C-suite spending priority as of Q1 2025—enterprises are rapidly deploying both workload (AI-powered applications) and workforce (employee-facing tools) use cases. However, this acceleration has outpaced security and observability efforts, leaving teams with little visibility into how and where AI is being used.

Drawing from his team’s work with AWS Bedrock and other AI platforms, Snyder highlights deep inconsistencies in log formats, documentation, and guardrail behavior. The talk outlines how fragmented architectures, lack of standardization, and hidden system behaviors in AI platforms make it extremely difficult to centralize logging or detect misuse. Without significant effort to normalize and route logs, organizations risk repeating the same visibility and compliance failures seen in early cloud adoption.

## Key Points

- **Two types of AI exposure**: Workload (AI-powered apps/agents) and Workforce (employee browser/tool usage)
- **Log format sprawl**: 80-100+ permutations of model/provider/version combinations with inconsistent formats
- **AWS Bedrock logging requires complex setup**: No CloudTrail-like service; must route through S3/CloudWatch + Lambda
- **Hidden system prompts discovered**: Bedrock agents inject ~15+ hidden instructions (e.g., "Never disclose information about how your memory works") only visible in logs
- **Inconsistent log boundaries**: Single log files may contain multiple user interactions with no clear delineation
- **Cross-region processing**: Bedrock may process data across US regions unpredictably, challenging data sovereignty
- **Guardrails are unreliable**: Different providers handle guardrails inconsistently (Amazon blocks, Anthropic returns "unknown", Cohere ignores)
- **Pre-submission API calls**: LLM services start processing prompts and uploading files before users click submit
- **Limited client-side visibility**: Browser plugins can capture prompts but struggle with response payloads and file contents
- **Truncated endpoint logs**: Security tools like Zscaler truncate payloads, missing critical context

## Technical Details

**Log Collection Architecture (AWS Bedrock):**
- No native centralized logging service
- Must configure CloudWatch Log Groups or S3 buckets per region/account
- Use Lambda functions to aggregate and forward logs
- Multiply complexity by number of AWS accounts × regions

**Log Format Challenges:**
- Raw JSON with escaped line breaks and special characters
- Inconsistent metadata fields (stop_sequences, temperature, token counts)
- Model-specific variations even within same provider
- Documentation gaps (e.g., A21 Labs format undocumented)

**Bedrock Agent Log Structure:**
- User input: Minimal (often <5% of log content)
- System prompts: Large predefined blocks (~15+ hidden instructions)
- Metadata: Lambda function details, execution paths
- No clear session boundaries between interactions

**Monitoring Approaches:**

1. **Server-side (Workload)**:
   - Direct API logs from Bedrock/Vertex/Azure
   - Pros: Complete request/response data
   - Cons: Format inconsistency, complex aggregation

2. **Client-side (Workforce)**:
   - Browser plugins for ChatGPT/Claude usage
   - Pros: Easy deployment, captures user intent
   - Cons: Limited response visibility, DOM security blocks

**Key Guardrail Behaviors:**
- Amazon Bedrock: Returns "content_filtered" message
- Anthropic: Returns "unknown" status
- Cohere: Silently processes blocked content (e.g., base64 payloads)

**Data Processing Concerns:**
- Cross-region processing within US (no control over specific region)
- Pre-emptive file uploads before user submission
- Multiple API calls generated from single user action
- No consistent request/response correlation

**Current Limitations:**
- No security-focused audit roles from AI providers
- Rapid API/format changes (daily/weekly)
- Limited documentation focused on adoption over security
- No standardization efforts across providers
- Endpoint security tools truncate AI payloads

## Full Transcript

Hey everybody, welcome back to track two. So the talk that's actually in here is a swap with track one. Um so if you're looking for the original session, it has moved to track one. Uh please join me in welcoming to the stage Jeremy Snyder who's going to uh give his lightning talk challenges around AI as a service logging. This talk is sponsored by Sweet Security. Give Jeremy a round of applause. All right. Um, thank you all for being here. Thanks for showing up. Um, I have a lot to get through. I will apologize in advance that I'm going to go kind of quickly through things. Uh, you're more than welcome to take pictures of anything that is going on. There's also a QR code at the end to get all the slides for yourself. Um, so I'm going to talk today a little bit about like why we started looking at this problem. What was kind of the background and the context for looking at logging AI as a service offerings currently? Some of the approaches that we looked at. um some of the different types of logs that are out there and why those present challenges on their own and then a few lessons learned. I will say just at the beginning I don't have all the answers by any means and in fact this is a super rapidly changing space which is part of the problem. Uh we'll talk about that a little bit. Uh a couple other random disclaimers here where we looked at kind of like the AI path offerings from the cloud providers. We tended to be very focused on bedrock because that's what we know best. We're mostly an AWS shop. Um, so I apologize I don't have a ton around Azure or Google stuff. Um, some of the problems are also related to the models themselves, not the logs. I'll call those out in a couple of shots uh spots. There are some screenshots taken from Firetale instead of directly from the raw JSON files just to make it easy for me to put this together. And then obviously like we embarked on trying to solve this and look at this because of our own use cases and you might have slightly different use cases. I'll explain a little bit about why we looked at things the way we did look at them and your mileage may vary. Um, that's a little bit about me. Um, why we started looking at this problem. Um, really, you know, the the thing is that like AI is finally kind of quote unquote real. Um, I like this really the chart on the right there. You look at that yellow line. That's actually the spend priority across seuite executives over the last several quarters. And it's actually only now in Q1 of 2025 that it is now the number one spend priority across pretty much all of the seauite. And so that means it's going to become a problem and a challenge for like everybody in this room for your own organizations. And so that's really kind of like why we started looking at this use cases are finally like making it out to production. This uh line graph here takes some data from the um what is it called? The responsible AI collaborative group which is a nonprofit that's been tracking AI related threats going all the way back to the 1980s by the way. Um really interesting. So the 80s and the 90s we kind of consolidated into decades and they don't really register as a blip on this line graph. Um but unsurprisingly right around that 2021 release of chat GPT time frame you really see the inflection point turn up and this is all kind of AI related security incidents whether that is like a data breach or an abuse. It also counts stuff that is kind of silly in terms of like defects and frauds and whatnot but unsurprisingly like chat GPT comes out the threat level and the incident and the risk level goes way the heck up right. Um, and I really think interestingly this Capgeemini survey that's 2,000 enterprise customers surveyed about like, hey, have you had risks or problems related to your AI adoption? And 97% of them self-reported. Yeah, as a matter of fact, we have. We've had data exposures or breaches or things that could have gone very very poorly. So, this is kind of the threat landscape that we live in. Um, on top of this, a lot of organizations are now asking themselves the question of like, are we okay with our AI usage? and they don't really have a lot of tools or frameworks that they can fall back on. Right now, really kind of the number one thing to look at is the OASP LLM top 10, which as I always tell people is not a compliance standard or a compliance framework. It is a threat model. So remember that as you're looking at it, it it is also subject to interpretation. And the interesting thing from our perspective is when we looked at it, we kind of mapped it onto a sort of logical architecture here. And this is obviously our mapping and yours might slightly vary as you think about it for your own adoption. Um, but most of what you see is that like where you see the threats, you don't have tool coverage currently for most things. Some of it is obviously blackbox from the LLM providers, but a lot of it is really looking at things that you may not have looked at in the past. And so a lot of organizations are kind of struggling to figure out like crap, are we okay with AI usage and do we have threats around that? Um, we're also starting to see the first real world examples. Um, shout out to the Promeo folks. I don't know if any of them are in the room, but they actually have a really good blog. I like a lot of their research when they find threats. Um, if swing by their table, give them kudos. They do a great job on this. I won't go through all the details on this, but this was a case where like, you know, moving fast uh led to breaking things and it led to some exposed keys. Very, very similar to mistakes that we saw 10 years ago with cloud adoption where, you know, like we're moving very fast and we're making a lot of the same mistakes again. And so uh exposed keys led to compromise of an AWS um bedrock environment which led to provisioning of a jailbreakable model which le led to jailbreaking of that model which led to the compromised AWS account um paying for the privilege of hosting a jailbroken LLM service that powered a chatbot um a sex chatbot on a third party thread actor website that was then monetized. Um so this is roughly what happened in this case. And then another incident that we um I just picked another random one here. Uh this is chatgpt's indexing service uh for web crawling of different websites for training purposes had an unauthenticated API endpoint where you could send it a URL and say please go index that URL and actually you could send it the same URL thousands of times and it would just go index that website thousands of times and it would parallelize the request resulting in an effective DOS of that website. Um so when we look at that we we we know that we have this kind of like threat model we have some incidents um happening and the thing that struck a chord with us for those who don't know us we started life three and a half years ago as an API security company is we see like oh actually we're actually making a lot of the same API related mistakes and in fact we we have this saying there is no AI without APIs and if you look at kind of like most LLM related architectures if you're building an LLM powered app or an agent or whatever you're heavily relying on APIs and in that threat model they're one of the few things that you actually have good full control over not only your own firstparty APIs but your consumption of those third party APIs and how you're structuring your apps around all of that. So this is a little bit the background on why we started looking at the the this whole space and the challenge that we were hearing from organizations is um in many organizations the horse has already left the barn so to speak like AI adoption is already happening kind of organically and so a lot of organizations are trying to figure out well what is going on and if you're trying to figure out what is going on you need to understand like where is AI already being used and largely with what data because that is primarily for a lot of organizations like what is the concern are we okay with the data that we're using with these LLMs and and different AI models. And so, um, we broke this down into kind of two flavors of AI consumption that are happening. And I can't take credit for this workload versus workforce. Uh, that that's a phrase that was coined by Fernando Montenegro, an analyst over at Omdia. Um, kudos to Fernando for coming up with this. I think it's really helpful in framing it. So, you've got kind of like two flavors of AI usage happening right now. So, one is the workload side of things, and that's all your LLM powered applications. I've got an app. It makes one call out to an LLM. Awesome. I'm building an agent. I'm building an agentic system of systems. Uh system of agents kind of system. Like that's all workload, right? So anytime that you are building code that is incorporating LLM or LLM functionality, call that under the bucket of workload exposure. And then your workforce, meaning your users and what they're doing, call that under the bucket of workforce AI exposure. And I'll talk about the challenges around both of those. So on the workload side, just to start off with, you've got about like five or six different flavors of apps that you might be building right now. Everything from like an app that has a single LLM interaction point to an agent or an agentic system or an MCP or what have you. And then you have all the different engines that you could be running this against. And that could be like natively through calling out to an anthropic uh cloud.AI AI or to an open AI chat GPT or that could be using the kind of AI you know platform as a service offerings coming from any of the cloud providers. Um and so what this leads to is you have now this like log format sprawl problem. Um none of these vendors unsurprisingly are collaborating on like hey what should go into an AI log and what should be an AI log and I'm going to talk about some of the challenges that we've observed as we've looked at this uh to kind of illustrate the the problem that I mentioned. This is Bedrock and that's um the Google uh Vertex. Yeah, Vert.ex marketplace of different AI providers. And what you see is you've got like okay, you've got providers and and Amazon Bedrock kind of like consolidates it by provider, but underneath either one of them, you've got in the order of like 80 to 100 permutations of model, provider, version. Couple that with regions that you could be running in and like you you now start to have not only like a format sprawl problem, but a destination sprawl problem of where these log files might be going. Um, on the workforce side, the number one use case is really through the browser. You know, it's really like I am chatting with chat GPT or I'm uploading an office document and asking it to summarize it for me or whatnot. But you do have other use cases like co-pilots for either office or um IDE applications. you're increasingly seeing native apps uh being built. So there's now like a Mac desktop application for chat GPT where you can interact directly and then you have some edge cases around things like um CLI uh usage or let's say like I'm in Postman and I'm interacting with the API of anthropic uh or what have you. Um and these are primarily hitting not so much the paz offerings from the cloud providers although you can do that and I have talked to many organizations that have deployed let's say like a private SageMaker instance of a metal llama augmented with their own data and then tell all their users no no don't chat with chat GPT chat with our internal LLM for whatever purposes so that is a thing but primarily like the majority of use cases are really like talking outside to third party LLMs right so that's a little bit the backdrop of like all the different permutations of AI usage that we're seeing across those two categories. Hopefully that's making sense. When we looked at this, we're like, we've kind of seen this problem before. Um, again, going back to our history as an API security company, we're like, oh, you remember when we built that system to normalize logs across the different types of API access and API architectures, we were like, okay, do we see a parallel with these AI logs? And it turned out, in fact, we did. And I'll go back a little bit to like what are we trying to solve for? What we're really trying to solve for is again like understanding the AI usage that's happening. And ideally, I think most organizations if you said like if you could wave a magic wand, what would you want? And what you would probably want is you'd want like a unified log stream where all your AI usage is in one location. You can see what data is being used with what LLMs and where and for what purposes and so on. And you could detect different attacks or different jailbreak attempts and anomalies and you'd have some like data loss use cases and what have you. I will tell you that we are like sort of far from this right now. Um but that there are some things that we've learned along the process that that can make it um useful. Uh one thing that I will call out is that when you look at the different cloud providers, you're not going to see a lot of what I would call super helpful documentation around this right now. Um a lot of the use cases and things like the SDKs are really designed around adoption as opposed to around security use cases, right? So you're going to get a lot of tools around like hey here's the API here's the SDKs go build on top of all this stuff awesome oh you want an audit trail um yeah so that is something that we saw and we realized let's let's say again just taking the bedrock scenario if you wanted to centralize just all your bedrock logs uh you've got to kind of a put them all into some kind of log stream and then push that log stream to some location there is no cloud trail like service for this currently um you could potentially push them all into something like a security data lakeink. We did not look at that scenario. Uh we looked at actually a different uh uh pushing them externally and we realize you can route them either via uh put everything into an S3 bucket or put them all into a cloudatch I think it's a cloudatch log group um and then use a lambda function or something like that to push them out. But this is roughly what you have to do right now. And if you multiply this again by let's say like you know I'm sure a lot of people in the room have tens if not hundreds of AWS accounts and you have multiple regions where you might be running stuff. Um you can see that the problem gets pretty annoying pretty quickly, right? Uh other thing that we learned around this is that log formats are kind of variable and weird. Uh this is a raw log off of a bedrock interaction. Uh you get all this delineation and line breaks and things like that. So you have to like think about how you escape all this stuff if you're trying to process the log file. They do come in JSON format, very compliant JSON. That's awesome. They come with some metadata and some of observability stuff around um token counts and things like that. You can usually get the execution time as well. Um you get the model temperature, whatever. Um but then when you look at things like uh stop sequences, they tend to be kind of inconsistent. Uh this is actually four log files. Um, in fact, let me jump ahead a couple a little bit. Uh, this is in Bedrock agent log for file format. And I know this is just like a wall of text that is probably hard to digest from where you're sitting. So, I color coded it to try to make it a little bit easier. And interestingly, in our uh testing, what's in orange was our input. That's it. What's in blue is predetermined system stuff that came just directly from Bedrock with zero user intervention. and it shows up in the logs. But that was the only way we were able to understand that all of those things also went into that LLM interaction. And we had zero input or zero kind of control over like what is this bedrock agent doing? We didn't know it had all these constraints on it. Never disclose any information about how your memory works. Like who knew? And we only found this out by examining the log. So that was really interesting for us. Uh one other random thing there at the bottom in green, you can see a little bit of metadata around how the uh service was actually processed. Um you can see here some callouts. This is all lambda functionbased. Um so you probably already knew that but explicitly in the logs you can see some of the stuff around that. Uh okay, I already talked about the metadata that is available but one interesting another interesting thing was that we found it's very inconsistent as to when a log file starts and stops. You'll notice in here you have a number of user and assistant interactions here. This is a pretty silly scenario where one of our guys was just kind of playing around with one of them. Um, but this was one log file with like four interactions back and forth. So like how do you delineate one interaction from another when the log file is pretty inconsistent around it. If you really want to get it super consistent, what we found is that you have to have like one interaction stop, go away, wait a period, let the log file commit, and then go back for your next interaction. It was pretty annoying. Um, on that side, uh, one of the other things we observed is, I know this is probably hard to see on the screen here, but if you look across the bottom, this is something processed with cloud 3.5 sonnet v2. Um, that's US East1, US West 2, and I think US East 2 maybe, I can't remember. Um, so there is this little gotcha that cross region support is required for bedrock automation. And I think like most organizations are probably in this experimentation phase where you're going to be using bedrock in this model for the near-term future. You have no guarantee on where this data is processed. It's all within the US. Yes. But like if you have data sovereignty requirements, do you have good assurance that the data is only where you want it to go? Hard to say. Uh other things. So we took a bunch of different formats and this is a little bit of our source code on parsing and normalizing. We tried to call out like what is the least common denominator around it and what you actually get is really just stuff around like model temperature, P90, things like that. Even that was sort of inconsistent. And then I love this comment that I'm sure is really hard to see. Um it this is from one of our developers who was trying to solve the A21 log. Seems like super spotty documentation. We have to just guess like basically like the documentation for what the format of the A21 LLM log should look like just isn't there. Um so this is one of the challenges. Uh, guardrails are also inconsistent. So, here's an Amazon guardrail. Content filtered. Here's an anthropic guardrail. Unknown. Uh, here's a coher guardrail. Actually, it didn't fire. The coher model just went ahead and processed this B 64 thing that should have been caught by a guardrail. Um, but wasn't. So, the guardrail was in place and coher just kind of ignored it and went on. So, this is I mentioned sometimes the problems are the models, not the actual log file format. So, that was kind of an interesting observation for us. Coming back to the workforce side of things, one of the interesting things is like how are you going to log all of your users AI usage? Um, we looked at a couple of different formats. We found that actually the browser plugin is probably the easiest and lowest hanging fruit for most organizations right now. Um, drop something in a browser that captures all of the users interactions with LLMs. Um, and but then you get client side logs as opposed to everything that we just looked at which is server side logs, right? And so you're fundamentally getting a different set of data around it. Um we also looked at what if you wanted to do something like an endpoint agent logging. Um and so like it's not bad. It's pretty good. I I don't mean to pick on Zcaler. Um they're one of our investors. We I just like uh we we partner with them and so I sampled some of their log formats. But you'll notice that a lot of endpoint logs are truncated intentionally to kind of reduce the data volumes that can go in them, right? And so you might not get things like the actual payloads. you're going to get like URIs of what was hit, but maybe not all of the request parameters or if there is something like a document upload, you're probably not going to get that. Maybe a file name. Um, so that was pretty challenging. We also found that trying to get response payloads is really, really hard from a client log perspective. The browser security DOM kind of blocks that in most cases. Um, even getting the contents of a file upload is kind of challenging in a lot of scenarios. So at best you get kind of visibility onto let's say like the user prompt plus the file name um at least so far and we've been working on some stuff around that as well but uh uh anyway you might have that problem. Uh one of the other interesting observations I I don't know if you've ever observed this for yourself but when you go into a lot of LLM chat services um they start submitting stuff before you've actually clicked submit. Um so here you can see our little monitor window there. That's our little browser plugin thing. Um, I haven't submitted anything here and yet it's already fired off multiple API calls to the back end. So, we've noticed that it pre-processes not only the user prompts, but it'll actually pre-process your file uploads. So, if you drop a file in there, in the interest of trying to serve your chat request as quickly as possible, they will actually start streaming up the file contents to process that in advance of your question. Um, so that was like a we were like, wait. So we had to confirm that and we went through that a couple of times just to make sure we were seeing what we thought we were seeing and like clear cache, clear everything, try it again. Yep, sure enough. Um, so that was pretty interesting. Okay, so observations. Uh, fundamentally you have like two types of logs, client logs and server logs. And you have a lot of differentiation around it with not a lot of documentation. And one of the other things I should have mentioned, all this stuff changed like really rapidly like sometimes daytoday, week to week as we were working on this. Um, ideally you'd kind of work backwards from your goals like what are you trying to solve for and like I said most of the use cases that we see organizations trying to solve for is like understanding what's going on and then like monitoring observing what's going on inside their organizations. Uh, you can usually identify the user which is great. You can usually identify the request you may not get the responses in most cases. Um, I think things are likely to get worse before they get better. Like we're in this massive AI arms race, right? and everybody is kind of moving fast and again like the priorities are typically enablement and adoption security secondary a lot of the third-party kind of SAS AI providers I don't think they've really kind of woken up to the idea that an organization would want like a security audit role for the organization on their service or platform. Um so that's it for my presentation. If you want the slides, there's a QR code. Uh looks like we've got a few minutes for questions. All right. Uh, pans for questions. One right here. I was curious. I was um just that clarifying question for the big log that you had up that you color coded. Was that blue section like a static thing that every log is returning? Yeah. Um so for every agent that we had there um was coming back with that and I don't so like caveat we built a few agents and we saw consistency across that we didn't build like tens or hundreds of agents and we also didn't try to experiment with let's say like once we knew that those things were in place we didn't try to experiment with like oh let me try to override that set of instructions with something of our own. We were just like, wait, what is going on here with all this stuff that we did not control? Um, yeah, that was really surprising for us though. Yeah. Awesome. With the log formats that are coming back from bedrock, do you and there was an inconsistency with that format and how it was given and all those uh line returns. Um, do you think that they were using AI to take the data that was given and then giving it back and so it was just filling out as fast as it could? Um, I don't I think these really are just like the Amazon interpretation of the log formats coming from the each of the different providers. I don't think that anybody's gone to the depth of like, oh, let me LLM augment this logging format. It's like, I just slapped in a 100 different models on top of this like Lambda platform. Just fire. Oh, you get me back something that is roughly JSON. I'm going to make it JSON complete and just give it back to the user. I don't actually care if it's like one interaction or five. Just like get me something that is there. Um, yeah, great questions. By the way, patches for both of you, but like I've got one more, by the way. So, like one more question and that's my last one. Got one back there. Great talk. Thank you so much. I'm just in it for the patch really here. Uh I just wanted to add an extra note that I have been building some of this stuff with the APIs and the guardrails when you turn on tracing the input and output are both evaluated and the logging format is totally different. Like one is pluralized and one is not pluralized. Yeah. And it's just awful. And I I don't really have a question. I just want to say thank you so much for like publishing all of this. Um it's terrible. We appreciate it. It's totally terrible. And in fact, like you, Kyler, like you highlighted something that I forgot to mention is that like some of this stuff, even like the little inconsistencies around like casing and formatting and pluralization and whatnot is there and we've seen it in some of these cases where it's just like, oh, stupid. Okay, I have to account for API and APIs plural or I have to account for API lowercase and API camel case and crap like that. And it's just like annoying. Yeah. So, all right. Well, thanks for the great great talk.
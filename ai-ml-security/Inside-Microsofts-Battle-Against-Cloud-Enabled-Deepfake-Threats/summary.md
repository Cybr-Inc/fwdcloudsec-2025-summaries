# Inside Microsoft's Battle Against Cloud-Enabled Deepfake Threats

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=EDQfmtwa49A)

- **Author**: Bruce and Stefano
- **Talk Type**: Security

## Summary

This presentation examines how attackers exploit cloud AI APIs to create deepfakes through a technique called "LLM Jacking." The speakers detail Microsoft's December 2024 lawsuit against threat actors who built infrastructure enabling large-scale deepfake generation, including the OAI reverse proxy and D3U tool. They explore the shared responsibility model for AI security and provide detection strategies for organizations using cloud AI services.

## Key Points

- LLM Jacking involves stealing cloud credentials to access AI APIs for unauthorized deepfake generation
- Major incidents include a British engineering company losing $25 million to deepfake video calls and election interference in Slovakia
- OAI reverse proxy serves as infrastructure allowing attackers to sell access to stolen cloud AI credentials through Cloudflare domains
- D3U tool provides a web interface for generating images using stolen OpenAI credentials with jailbreaking capabilities
- The tool strips C2PA metadata that identifies AI-generated content, removing crucial provenance information
- Microsoft's lawsuit in December 2024 targeted the creators of OAI reverse proxy and related tools
- Attackers sell access to compromised credentials and tools through platforms like Patreon
- Attack chain: stolen credentials → OAI proxy → D3U tool → Cloudflare masking → OpenAI API calls

## Technical Details

**Attack Infrastructure:**
- OAI reverse proxy: Sells access to stolen cloud API credentials with integrated support for AWS, Azure, GCP
- D3U tool: Python web interface for DALL-E image generation with jailbreaking features
- Cloudflare domains: Used to mask real IP addresses and infrastructure
- Credential theft: Vulnerable applications and repository scraping for cloud API keys

**Microsoft's AI Shared Responsibility Model:**
- **AI Usage (Customer)**: Staff training, access management, data governance
- **AI Application (Shared)**: Safe design, custom guardrails, plugin interactions
- **AI Platform (Microsoft)**: Compute infrastructure, base model security

**Microsoft's Security Controls:**
- Content filtering on inputs and outputs using classification models
- Abuse detection based on usage patterns and heuristics
- Prompt transformation to make harmful content safer
- Prompt shields against adversarial/jailbreaking attempts
- 30-day prompt retention with human review for flagged content
- C2PA metadata embedding for AI-generated content provenance

**Detection and Monitoring:**
- Azure API Management required for logging OpenAI API usage
- Monitor image generation API calls (primary vector for D3U attacks)
- Event streaming to SIEM/detection platforms via Event Hub
- KQL queries for prompt analysis and response monitoring
- Detection rules based on API usage patterns and prompt fingerprinting

**Defensive Measures:**
- Role-based access control instead of API keys
- Private network restrictions and approved subnet access
- Advanced guardrails for image content analysis
- Runtime detection through API management logging
- Regular credential rotation and exposure monitoring

---

## Full Transcript

Uh welcome back. Last talk before lunch here. Track one forward clouds sec day one. Uh this is inside Microsoft's battle against cloud enabled deep fake threats. Uh so uh quick announcements before we get started. Cell phones off questions after the talk on Slack or raise your hand and I'll bring the mic to you. And we have swag for good questions today. It's exciting. Um lunch is going to be immediately after this talk outside. Feel free to pick up lunch and bring it back in. Bring it to a birds of a feather room. And lastly, if you ordered a t-shirt, please go to the registration desk and pick it up. There's no reason not for us to there's no reason for us to bring them home. Uh with that, uh please welcome uh Bruce and Stefano. Cheers. Thank you. So, hi everyone. Um I'm Stephano. Here is Bruce. And today we're going to talk about uh deep fake and how attackers are using um cloud API for for deep fake. H in this case we're also going through uh what some of uh the work done by uh Microsoft on their lawsuit that they did in December but we'll talk about that a bit later. So let's start introducing uh the main content. So let's start with what deep fakes are is pretty famous already but just you know for for the sake of argument uh deep fakes are synthetic media uh which usually are videos, images or or audio uh created by AI um using the fake learning techniques to manipulate or generate new content uh in order to pretend to be real. So that's the main part like the pretending to be real. Um in the slide we just uh added three main uh cases that happened like very uh very recently uh and very different uh from each others. So one is related to Mr. D fake website which was a website that got closed uh which were pretty interesting because you could um you could ask for on demand content and and on demand um defa content on whatever person you you wanted. Uh so very convenient, very easy to get and of course you will get the uh the defa content um as soon as as soon as we pay and uh that's it. So very easy. Of course it got closed uh with a pretty good pretty big fine for uh for the person who created that. Um the other example uh that we posted here is uh a British um engineering uh company EUP which got tricked into sending to attackers $25 million. Um how it worked um was mainly um DA videos was used in the call uh in order to simulate like um senior management uh approves for um to sending money to to the attackers in this case were uh were different payments shipped to them uh like 15 um and then attackers were able to excfiltrate $25 million which is a pretty good amount. Um the last one uh is related to uh to elections which we know being a very hot topic. Um in this case is happening in Slovakia where um a video came out on a top top candidate for for an election and claiming to have a rigged election on for for its advantages of course. So this could be very disruptive of course for his campaign but for for all I mean the overall election I guess. So um those are just three example in order to start just saying that of course the the fake uh issue is real is something that we have seen before and first of all we need to kind of start dealing with them and start to understanding how to deal with them. Um now let me introduce LMJing even in this case is something that have been uh there for a while. Um we spent uh some time uh around 2 years with with LMJing in our research. We were the first like publishing something on LMJing. Um during our research through these two years we saw uh deep fake being one of the main goals for for attackers. It's not the only one. Um we saw uh attackers also playing with um uh uh content for role- playinging website for instance. So defake of course is not uh the only one but is definitely one of the main things that um that attackers and threat actors want to want to do once they have access to uh to cloud to cloud API. Um how LMJing works uh first of all attackers trying to get uh credentials uh usually by um exploiting vulnerable applications or um or just u extracting uh secrets from uh from repositories around that they can find and scrape credentials. uh those credentials uh usually cloud credentials are then used uh in order to um to call APIs on on cloud in this case specifically for uh any AI CRV in cloud which could be uh bedrock or u as open AI or vertex AI of course it depends on the cloud um the other interesting things uh is also uh LMJing is now a thing so it's very kind of well known is already is also added into Maitra to uh resource hijacking uh because of course who who is paying the bills uh eventually is the account owner who's got compromised uh and we also in the past we also created a status um model um related to um AWS bedrock which was one of the um cloud API that um that our taxes were calling Um, one of the interesting part of LMJing is defin reverse proxy which later on will kind of connect with what Microsoft uh found and and the lawsuit that Microsoft did. Um, oax proxy reverse proxy is a proxy that is mainly used for on LMJing is we saw this using very extensively during LMJing attacks. Um how it works is pretty easy. So um the actors set up instances of this oi reverse proxy. Um they sell access to the to the proxy. So end user eventually connect to the reverse to to reverse proxy and starts using whatever uh API they want to use or whatever model AI model they want to use for their purposes. Um it's pretty handy since uh kind of all the cloud are already integrated into that. So the end user don't need to uh care about um credentials. They are already all in it. They can just use it. Uh that's it. That's the kind of on demand service that are selling. uh and as you can see support all the kind of um cloud API that could be the other important stuff the important thing is that um reverse proxy let you choose a domain if you want to choose a domain or uh you can also um I mean automatically set up a cloudflare domain um which we will see an example later uh which of course we know being very very handy for for attacker because is is masquerading the real IP And here is um how uh kind of uh OI reverse proxy kind of fit with what Microsoft did um in in December 2024. Microsoft uh did a lawsuit to uh a group of people uh which wasn't uh discovered back in in in December. Um in in February though we we saw some of those names coming up. Um and here we reported some of them. Um one thing that you can easily see is how many of them are related to uh to reverse proxy and way reverse proxy that we were talking before. Um kind of leading to uh how also jacking is kind of um connected to to all of this. Uh we can find here who created the OI verse proxy uh first of all and uh what Microsoft put together is also people that put together rather or created other instances for for for this reverse proxy or also added some features for uh for this specific proxy related to specific cloud cloud providers and we can see some something related to GCP or uh Asure for for Drago as well. Um the other import the other kind of interesting fact is uh the the last one fees which is kind of related with uh ox proxy but is the developer who created the3u which is a tool that proves is going to talk talk a bit about that uh later but is still connected with with um uh with oi reverse proxy. Um this is um a screen that we posted in our blog in February a little bit before uh Microsoft kind of disclosed the the names the names that we saw before. Um in this case we can see uh this is a screenshot related to um an attack that we recorded on LMJing and um as you can see uh Drago is the threat. Um and we also kind of posted the um uh the location which fits perfectly with what Microsoft also posted in their in their report and in their lawsuit. Uh the the other important important and and also the name and and the email which fit with with what with what Microsoft said. Uh the other important thing is also um the drag in this case the tractor is also selling um their tools to patron which is a a website where people are selling not just content but also scripts and we saw that he's selling the script that he use for uh credential theft and all the um and like extract extracting um credentials and also validate them especially for cloud API. Um, so that's it from my side. I'll let Bruce go a bit deeper into D3 and all the rest. All right. Thank you. Um so as Stephano mentioned one of the main tools that have been used during LLM LMJing attacks um involving deep fakes is D3U which is a Python program exposing a web user interface and um it basically allows people to um generate images using the D3 model which is the main one for generating images of OpenAI um using um this web user interface. Um this proxy can be integrated with the OI reverse proxy. You can authenticate by either providing an access key for openai or an access key for the overse proxy. In this case all the traffic will go through the os proxy. Um the author made this tool publicly available at um wry.org URL which is now has been taken down as well as the GitHub repository of this tool. And um one important feature of this uh tool is that um it exploits one feature of Azure because if you whenever you run a prompt to generate an image, Azure has a system to revise your prompt and modify it by either enriching it or if it contains some word that trigger some content filtering. It uh it modifies it in order to make it safe basically. So this tool has a jailbreaking option which basically inserts the is user input prompt inside a jailbreak prompt which basically tells the LLM to not modify your prompt and send it without any changes to the to the delry model. And another um important feature is that this tool strips completely the metadata. In the case of Azure um one security measure against deep fakes is that Azure puts some metadata regarding the origin and the provenence of the of the image. So for example if the image has been originated from generated by an AI this metadata will tells uh that it has been generated by an AI and everyone can easily verify it on the web. But by stripping out those metadata which are called C2PA content credentials, this tool poses a risk an additional risk to the deep fake scenarios. So now we have all the components of the attack um which starts from stolen keys which can be um stolen keys used directly by the end user with D3U or as we saw um stolen keys of of an OIS proxy and then the user just puts his prompt to D3U to generate a fake image and those um those D3U will be then forward the the request to the OS proxy through the Cloudflare and then um you will call the OpenAI APIs to generate deep fates. So um the point the why this works is because it's very simple for an end user. It just can buy access very easily to an AI proxy instance and use V3U to generate harmful content and and deep fix stuff at large scale. Um when analyzing security incidents in the cloud involving AI, one could ask what are the responsibilities of the cloud provider and what are responsibilities of the customer of course. And to answer this question, Microsoft provided its AI shared responsibility model which defines which groups three main areas of responsibilities starting from the AI usage which defines um how the AI capabilities can be used. starting from training your staff and the user to use those capabilities in a safe way. Then everything that it's related to um access management is of course a customer responsibility. So for example, defining roles and permissions, defining the roles that will be used by users and apps accessing those capabilities. Of course, the data governance is one of the customer responsibilities because the data that goes in and out are controlled by the the customer. Then uh the second area is the AI application which basically defines everything uh concerning how you build an AI application using Azure OpenAI. So if you design it in a safe way um how it interacts with other AI applications, how it connects with AI plugins and most of these things are a customer responsibility. Um also Microsoft by default provides of course some content safety systems but users can even add some custom guard rates. Therefore, this area is a shared responsibility one. And the third one is the AI platform which is um related to anything uh related to the compute infrastructure which of course is an Asia responsibility to make it safe and everything that is related strictly to the model. So so for example model data tuning uh how the data fits into the model which is model dependent in the sense that if you build your own model it's one of your responsibility otherwise it's a Microsoft one. Microsoft implements several technical procedures to make um its AI service safer starting from content filtering uh leveraging like classification models that run both on the user input and on the LLM response of course. Then it um implements some abuse detection which is uh on a heristic which triggers alert based on the usage patterns of the customers. Then as we saw they implement some prompt transformation to make your prompt safer in case it contains some harmful content. Then some prompt shields against um adversarial prompts like jailbreaking stuff. And then um Azure also stores prompts for up to 30 days. Um and in case of only in case of any flagged content on those those prompts, there is the possibility of like human review in the sense that um Microsoft employees can in a very secure way uh control and further analysis this prompts. Um by default Asia OpenAI is um authenticates with API keys but is but this is not the most secure way of course. So um if you really want to use API keys be sure to follow the classical best practices which is rotating those and monitor for exposed credentials but instead Azure suggests to use role based access and Microsoft and for your uh users and apps access accessing open AI. Um also you can it's suggested to uh restrict your AI apps in private networks and allow only access by um approved network subnets. you can um leverage advanced guardrails which apply to images too. So the the content of the images checked is checked to and then you finally surely have to implement some runtime detection. How to do that? Um by default I Azure doesn't provide logging of events that are related to the LLM usage. Therefore to do that you have to configure another service which is Azure Azure API management to wrap the Azure OpenAI and by doing that every um API called by Azure OpenAI will be monitored. Um you can configure this and eventually send all the events to a streaming service like an event hub which then can eventually send those those events to uh third party software like runtime detection software which have detection rules for monitoring and uh and respond to to threats. To do that you can just enable diagnost diagnostic settings on your um API management instance. Then you have to import of course the Azure API service APIs and then you can just uh forward the the events and write for example some detection rules based on the PI. So for example there is the image generations create API which is the one used by D3 to generate an image. So that's one of the API you can monitor and of course you can just you can al also use the QQL queries to um an further analyze prompt um prompt input and LLM response in your in your recent activity and with that I leave Stephano wrap it up. Yeah. So after um as you said though this is kind of a a team effort right. So it's not just what the cloud provider can do. Uh it's not just what we can do in order to secure ourself. We also need some guidelines and some um some regulation set. Um and like you Microsoft in this case uh also published a very expens extensive report on what also policy maker could could do and should do in order to update um laws and update regulation in order to uh match the needs and make sure that we can for example uh make sure that all the content created by AI is labeled in in a way that it can be prove the authenticity for instance right So all the things that can be done by policy makers needs to be done in order to make uh it kind of safe for public to use it and avoid exactly what we saw before in in the example that we saw before. So this is literally a team effort where all all the part needs to do their kind of job all the kind of effort in order to make it secure and usable in uh uh in future. Uh so it's that's it from our side. We are here for question if you have any. So thanks for attending. Okay, questions from the room. All right, I'll start with one. Uh it seems like you've found a couple places where there are these open services that can end up being subverted to create deep fakes using sort of other people's API keys. Has anyone sort of started cataloging those? Is there like a good place the showdown of of open AI uh open AI applications? Yeah. Like what is thread intelligence look like for you in the space at this point regarding deep fakes? Huh? Uh well I guess like the tools to create deep fakes. Um I'm not sure. I mean you you you like you talk about how just I had not heard of like D3u or whatever is the like where where did you find that? like what what helped you sort of find those kinds of tools? So we we observed like um we have been observing attacks in several environments AWS, Azure as well and we try to fingerprint the attacks trying to figure out where they come from which websites will like which website was exploiting those environment because for example roleplay content was used by a lot of websites for fake like girlfriend stuff and as well as deep fakes. We figured out like by fingerprinting um the logs, we managed to find a way to like see what was the what was possible website using those and for example the tool um the tool the D3U was used because in the logs we saw the the prompt the jailbreaking prompt to and this allowed us to fingerprint as it is but it was just like a dynamic fingerprinting uh phase. I mean Any other questions? All right, I guess we'll head to lunch. Thank you so much. Thank you.
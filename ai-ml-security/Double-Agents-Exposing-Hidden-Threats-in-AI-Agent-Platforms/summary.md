# Double Agents: Exposing Hidden Threats in AI Agent Platforms

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=NgrpxhCDdrM)

- **Author**: Michael Kachinsky and Hegy Kessenberg
- **Talk Type**: AI Security

## Summary

This talk exposes critical security vulnerabilities in AI agent-building platforms, demonstrating how attackers can discover and exploit improperly secured agents. The speakers show how static identifiers in web integrations are often leaked in public code repositories, allowing attackers to bypass authentication and domain-based restrictions to access sensitive data and perform other malicious actions like denial-of-service attacks.

## Key Points

- AI agent platforms are proliferating rapidly, often with security as an afterthought.
- Many platforms use simple HTML snippets with static, unauthenticated identifiers for web integration, which act like credentials.
- These identifiers are frequently found in public GitHub repositories, exposing the agents to unauthorized access.
- Attackers can use these exposed identifiers to interact directly with agents, bypassing any website-level authentication (e.g., an internal company portal).
- Domain-based access controls, intended as a security measure, can be bypassed by spoofing the trusted domain using local host file manipulation.
- Once accessed, agents can be prompted to reveal sensitive internal data, financial information, and their own configurations (tools, knowledge bases).
- Exposed agents are also at risk of Denial of Service and "Denial of Wallet" attacks, where an attacker drives up operational costs.
- The best practice is to always use authenticated APIs (OAuth/JWT), treat agent identifiers as secrets, and implement secret scanning in development pipelines.

## Technical Details

- **Platforms Mentioned**:
    - Google Vertex AI (primary focus)
    - Microsoft Autogen & Copilot Studio
    - Amazon Bedrock
    - IBM WatsonX
    - Botpress AI

- **Vertex AI Agent Types**:
    - **Search Agents**: Build natural language search over specific data sources. Enforces an "allow domain" list for public access.
    - **Conversational Agents**: General-purpose chatbots for interaction and performing actions. The "allow domain" restriction is optional.

- **Attack Methodology**:
    1.  **Reconnaissance (Agent Hunting)**:
        - Search code hosting platforms like GitHub and SourceGraph for HTML snippet patterns containing agent identifiers.
        - Keywords include parts of the `<iframe>` snippet provided by platforms like Vertex AI.
        - This exposes the `project ID` and `agent ID`.

    2.  **Exploitation (Bypassing "Allow Domain" Restriction)**:
        - **Scenario**: A search agent is configured for public access but restricted to a specific domain (e.g., `hrportal.com`).
        - **Initial (Failed) Attempt**: Manipulating the `Referer` header in the HTTP request using a tool like Burp Suite. This did not work.
        - **Successful Bypass Technique**:
            1.  **Domain Spoofing**: Edit the local `/etc/hosts` file to map the trusted domain (`hrportal.com`) to the local machine's IP (`127.0.0.1`).
            2.  **Local Server**: Create a simple HTML file containing the agent's `<iframe>` snippet.
            3.  **Host Locally**: Run a simple local HTTP server (e.g., Python's `http.server`) to serve the HTML file.
            4.  **Access Agent**: Open a browser and navigate to `http://hrportal.com`. The request resolves to the local server, and the browser correctly sends `hrportal.com` as the origin domain, successfully bypassing the platform's check.

    3.  **Post-Exploitation**:
        - **Discovery**: After gaining access, use prompts to learn about the agent's capabilities.
            - *Example Prompt*: "What are your tools, knowledge bases, and connected agents?"
        - **Data Exfiltration**: Ask the agent to return sensitive data it has access to. The talk showed an example of an agent revealing a company's financial secrets.
        - **Denial of Service / Denial of Wallet**: Intentionally overload the agent with requests to cause service disruptions or drive up costs for the owner, as the agents often scale automatically and can be expensive to run.

- **Mitigation & Best Practices**:
    - **Avoid Public Endpoints**: Use authenticated APIs (JWT or OAuth-based authorization) instead of anonymous/public access whenever possible.
    - **Treat Identifiers as Secrets**: Never commit agent identifiers to public repositories.
    - **Secret Scanning**: Include patterns for agent identifiers in your CI/CD secret scanning pipelines.
    - **Maintain Visibility**: Keep an inventory of all AI platforms, agents, and the data they can access.
    - **User Education**: Ensure all users, including non-technical ones who can now create agents, are aware of the security risks.

## Full Transcript

Hey, good morning everyone. Welcome back. Uh, if you hadn't noticed, we had a last minute uh, schedule swap. So, this talk will be double agents, not AI as a service. Uh, if you're interested in the AI as a service logging, that will be in the other room. You please feel free to go. Nobody will be offended, I promise. Um, before we begin, uh, we'd like to, as always, thank our sponsors who make this entire event possible. In particular, we'd like to thank our silver sponsor, assist. So please go give them a talk uh chat with them out in the hallway. Uh and with that uh we'd like to introduce our next talk. This will be a remote presenter double agents exposing hidden threats in AI agent platforms with Michael Kachinsky and Hegy Kessenberg. So please give them a welcome. Hi everyone and thank you for joining our session double agents exposing hidden threats in AI agent building platforms. We're happy to be here and sad that we couldn't make it physically to the conference. In this session we'll talk about the risks in agent building platforms and we'll see how attackers can access and exploit agents created using these platforms. So first let me introduce ourselves. I'm Michael and together with me Kagay. We are security researchers from the Microsoft Defender for Cloud Research team. We mainly focus on cloud native and Kubernetes threat protection. And recently we goten into AI security as well. And on our agenda for today, we'll start with an overview about AI conversational agents and search agents to make sure that we are all aligned. Then we'll see what does it take to build an agent via typical bending platform. Next, we'll show you how we were able to access agents we don't own and bypass a restriction mechanism in Vert.x AI platform. And finally, we'll talk about mitigations and present some best practices. There are many types and definitions of AI agents. And in this session, we'll focus on customerf facing AI agents, specifically conversational agents. To see what sets them apart, let's walk through their key attributes. So, interactions with conversational agents happens through a natural language chat. When user sends a query or a request, these agents cover a wide variet wide variety of roles from customer support to personal voice assistant. Creators can tune agents tone and persona to match the brand. And they can execute multi-step goal oriented workflows that begin with an open-ended request. They preserve context between interactions. And a good example could be an assistant on a clothing website that helps you find items matching your style and search them in stock. And by contrast, non-con conversational agents can relay on a closed set of commands. They are built to perform a single task and they activate only when triggered by a schedule, a button press or a system event. And a good example would be a job that is triggered automatically but involves an LLM in the process. So essentially conversational agents are chatbot but much better. And there are many platforms for creating these agents and as expected all major cloud providers have launched their own offerings. For example, Google offers Vertx AI agent builder. Microsoft has autogen and copilot studio. Amazon provides bedrock and IBM offers Watson X. And in addition, many independent companies have developed their own platform as well. And like everything related to AI, the pace of development of these platforms is amazing. More than 50 have already launched and most of them just in the past year. With such a rapid rollout, we can assume that security was not necessarily the top of mind. And in addition to that, most of the platforms are no code or low code designed to be accessible to everyone. That means even non-technical employees can deploy AI agents bringing all the risks and responsibility that comes with it. And we'll circle back to this later. And in general, the process of creating an agent using this platform includes few basic steps. Users need to choose a base model from the catalog like GPT, Claude or Gemini. Next, they will define the agent purpose and guidelines by writing a system prop that describes what to do. Then they will connect data which is relevant to the agent. For example, it could be your company's guidelines or a database that contains information about available stock. And they will connect tools and plugins like giving it the ability to read and send message on your Slack or Teams channel. And finally, they will need to publish the agent which is the part that we'll be focusing on today. And of course, every platform has its own unique features. This is just a highle summary of the common foundations shared across these platforms. And now that we've explored the leading platforms and understand the process of creating an agent, some of you are probably wondering is this event secure and we'll focus on how these agent are published and integrated with other systems. Each platforms handle the details a bit differently, but the underlying concept is pretty much the same. And what really caught our attention was the platform's website integrations. Agents can be embedded with a single HTML snippet supplied by the platform itself. The snippet usually contains only a unique identifier. No tokens or authentication flow are required, just a static ID. The simplicity is alarming and that's why we launch our research. And now we will switch to Hag. Thank you, Mel. Now let's see this in action and dive into our research findings. Although our findings applicable for many platforms, we will demonstrate them on GCP Vert.x AI which is one of the leading platforms in the industry. So let's start. So we are now with the Vert.xi platform creating our first agent and we can see that as first step we need to choose our agent type. Now Vert.Ex offers several agent types for different purposes but in this session we will focus on two of them. The first type is search agents which allow users to build natural language search engines over their data supporting a wide range of data sources. And the second type is conversation agents which are designed to interact with users through natural language by providing answers or performing actions based on the agent instructions and the user input. So let's start with the conversational agent. We are now at the agent setting page. This is where I can define it purpose tools to connect data sources and basically fine-tune how the agent will behave. And once I configured everything, the next step is publishing the agent say to make it available in my company's portal. So here you can see the publish agent button and after clicking it a window appears asking me to choose how the agent should be published with anonymous API or using authenticated API. Now that's an interesting decision point. Let's take a moment to discuss it and we will do it by examine three agent access models and what scenario they serve. So the first access model is agent with anonymous access meaning it doesn't require any authentication and in many cases anonymous access is intentional especially for customerf facing agents that don't handle sensitive data for example a chatbot agent in e-commerce website that designed to assist users with the store catalog. The second one is the hybrid model in which the agent itself is still configured as anonymous but hosted on an internal website or platform with authentication mechanism. For example, an HR assistant agent embedded into a company's internal portal that requires employees to sign in. And the third access model is the O required in which every interaction with the agent require authentication. But for now we are going to focus on the anonymous and hybrid access models in which the agent itself is anonymous or configured with public access. So back to our publish settings we so we can deploy our agent with anonymous API or using authenticated API but in search agents which we introduced before vertex enforces an additional control. Even if you choose to allow public access to the agent you still have to explicitly define the allow domains that would access the agent. And this requirement doesn't apply to the conversational agents, only to the search agents. And I think it makes sense given that search agents are likely to work with more sensitive data or at least with internal data. And once I confirm anonymous API regardless the agent type, Redex provides me an HTML snippet that I can embed directly into my website in order to use the agent. And here is where things gets interesting. Take a look at the snippet. You will notice two identifiers, the project ID and the agent ID. And these are the only unique identifiers needed to interact with the agent. Now, if I will take this HTML snippet that Vert.x provided me and drop it into a basic HTML editor, you can see that my agent is alive and I'm able to interact with it. So, a quick summary. We covered the access models for Vert.x AI agents. For an anonymous user to interact with a search agent, it must be configured with public access and the user's domain must be explicitly allow listed. And for conversational agent, enabling anonymous access is also required. But the allow domain restriction is just an option. Okay. So now that we got familiar with the HTML integration Vertx provides, let's see how attackers can hunt agents and what they can gain from it. And to answer this, let's walk through a practical example of an attacker methodology to achieve that. And as always, let's start with reconnaissance. Well, since Vertex provide us a ready to use HTML snippet, we thought that would be a great place to start. Naturally, we thought about platforms like GitHub where developers store and share their code. So, we began searching on GitHub and Source Graph for websites that embedded Vert.x conversational agents by searching for the HTML snippet patterns and keywords just like the one shown here. And the results came fast on GitHub alone. We found hundreds of public repos with vert.x HTML snippets. Each one exposing its agent ID and project ID. We believe the reason we are seeing so many exposed agent is that many developers assume their anonymous agent is safe only because it's embedded within an internal site or a site with authentication mechanism. But here's the catch. those protections don't apply at the agent level and if attackers find their project ID and the agent ID they can bypass those layers and interact with the agent directly. Let's see how it happens and what attackers can do with all these results. So basically attackers can take the HTML snippets they found on GitHub to plug them into a simple HTML editor and immediately start trying interacting with the agents in order to see if they're anonymous or have any domain restrictions. You would be surprised that using this simple technique, within just a few minutes, we were able to access multiple anonymous agents and retrieve sensitive data like the example you see here where an agent revealed the company's financial secrets all without hitting any access controls. So that's nice. Within few minutes, we were able to access organization internal data using the anonymous agents. But let's raise the bar a bit. Earlier we mentioned that search agents require specifying allow domain that could access the agent even when anonymous API is configured and as a developer I might think okay my agent may be anonymous but it's still protected because only users accessing the agent from my domain which requires them to login can reach it but is it really the case let's take a closer look at what attackers can actually do with search agent identifiers so again a quick search for the HTML snippet patterns on GitHub will return hundreds of search agent identifiers. For example, let's take a look at this agent snippet that caught our eyes. And when we tried to interact with this agent by simply copying the HTML snippet from the repo to our editor, we immediately got an error. And that's message indicated us that our domain wasn't on the allowed list. Now, that makes sense. No one's no one is going to add a random HTML editor as an allowed domain. But as I said, something about this agent caught our attention. It seemed valuable. So we decided to dig a little deeper. And while browsing the repo, we noticed that a specific domain kept popping up. Let's call this domain hrportal.com. So it's reasonable to assume that this domain was included in the agents allow domain list. And now the question becomes how we can impersonate this domain in order to interact with the agent. So we started the impersonation attempt with the basics using burp. We attempted to manipulate the HTTP request we sent to the agent by replacing the original referral in the request headers. Unfortunately, that didn't work and we still got the unallowed domain error. So, we took a different approach. We edited our local etc hosts file which is a system file that map domain names to IP addresses. And basically, when your computer looks up for a domain name, it checks this file first before quering DNS servers. So, it's a great way to override domain resolution locally. So we use this trick to map the suspect allow domain hrportal.com to local host and then we created an HTML file containing the HTML snippet from the repo and spun up a simple HTTP server to host the file locally. Now when we open the browser and navigated to to hrortal.com it routed us back to local host to our local html file and we can see we are effectively spoofing the domain and this time when interacting with the agent we can see it worked. We can see no errors. We were basically able to bypass the agents allow domain feature and freely interact with the agent as if we were coming from the original trusted domain. So that's cool. We have an initial access. And now that we are able to interact with the agent, we are moving to the discovery phase. In this phase, we want to gain knowledge about the agent tools and connected data sources so we could know if it's valuable for us as attackers. But we don't need to work that hard. For example, here you can see a question for the agent. Basically, a prompt that reveals all the agent tools, knowledge bases, and even other agents that connected to it. And this helps me as an attacker to understand if it's valuable for me. And after we gained knowledge about the agent, here's the fun part. All we left to do is kindly ask the agent to share the data it holds. We were able to retrieve numerous sensitive responses from both conversational agent and search agents. In many cases, the agents even returned the content of documents and their direct links even though we weren't able to access the provided links directly. Within minutes, we uncover hundreds of agents identifiers, some fully anonymous, others with the allow domain feature that we were able to bypass. And from there, it's just a matter of evaluating which agents are valuable and extracting the data easily. And this is exactly what we call agent hunting. But that excfiltration isn't the only threat. In addition to that excfiltration, attackers can intentionally overload AI agents, leading to technical disruptions such as denial of service attacks or simply driving up costs since these agents often scale automatically and can be expensive to run. Thank you. And back to you, MKL for Google's disclosure. So we contacted Google to highlight that attackers can bypass the allowed domain restriction, something that customers may rely on to protect their agents. Google replied that they do not treat the allowed domains feature as a security control. And they also stated on the documentation that setting the authorization mode to public access will allow anyone to access the agent. That's the reason they do not view uh the bypass as a vulnerability. And although using a JWT or Obased authorization remains the recommended best practice, the allow domain settings still serves as a fallback layer of defense when an agent sits behind a private or authenticated website and struggen controls are not in place just like example displayed here. And going back to the beginning, we said that different platforms have many concepts and similar and indeed many platforms offers a web integration using HTML snippet with static identifiers. And because of that, they are also at risk of being targeted. For example, here we can see the web integration offered by IBM Watson X and another example from the BordPress AI platform. With a quick search on GitHub for the patterns mentioned here, we found thousands of exposed agent identifiers. And you might be thinking, are you guys going to talk about Microsoft? So the answer is yes. In Microsoft Copilot Studio, the case was a little bit different that what we talked about here. A demo website is created automatically when creating an agent, no matter if and how you choose to publish your agent. A link to this website will always exist. And there is a research published by Doratias that demonstrate how these demo websites of external users could be found and accessed. I'm attaching a link to the research in the presentation and I really recommend you to read it. So let's sum up what we've seen so far. We examined the rise of conversational AI agents, how they are built by dozens of fastm moving no code platforms. A key takeaway is that these agents are often embedded on websites with a tiny HTML snippet containing only a static ID, making deployment trivial but sparkling serious questions about exposed identifiers and anonymous access. So how do we turn those red flags into an action plan? So we need to minimize public exposure and avoid public endpoints whenever possible. We also need to treat every agent identifier as a secret in HTML snippets. These identifiers act like credentials. Never commit them to public repos and include their pattern in your secret scanning pipeline. Next, and this is a difficult one, you need to maintain visibility over what's running. Keep a living inventory of AI platforms, agents, and data they touch. And finally, educate every user, technical or not. when anyone can spin up agents in minutes. Awareness is your first line of defense. Thank you for listening. Thank you very much. As always, we'll have uh questions in the Slack for anybody who's remote or does not want to ask them live. Uh if there's anybody here who wants to ask questions, um please just uh raise your hand. Um so I'll start off. I mean in some of the uh takeaways you talk about monitoring but can you get a little more about like how the different platforms allow guard rails um to prevent anybody from creating these and then how do we as defenders test what they go and implement various types of change control to ensure that we don't expose ourselves to these risks. Yeah. So um can you hear me right? Yes. Okay. Uh so I'll start. Um so first of all we'll need to do a few things. Each each platform offers a different uh type of guardrails. For example, I think that Vert.Ex AI offers some kind of um a quota that they have like uh 1,200 um requests per minute from from a spec specific session session. So they prevent um exhaustion and uh agent overloading. Uh so that's that's for the first question. And then again they also uh there are also many platforms that um I mean the admins of let's say the tenant or the Google project can actually disable au unauthenticated mode for all of the users and then uh it just won't be possible to create uh these uh like agents with these kinds of unauthenticated uh way. Thank you. Other questions in the room? All right, come back here. Uh, have you come across any of these platforms that have guard rails already in the way that you know Google Docs allows you to prevent public sharing of certain documents and I guess 0365 has the same? like are there any of these agent platforms that have more security control in the current time or or are they all kind of at the same level of maturity? Um so we explored um Microsoft, Google's and BordPress and and some other uh they were basically mostly mostly the same. Um we weren't looking a lot into guard rails. I mean we were mainly focused on the authentication method. Um but I haven't seen like something in and specific. I mean I think that the authentication mode is like their way of saying um this is how we prevent uh someone from accessing it. But again every every company was developing their own platform very fast. So I think that things have changed since we we did our research a few few months ago. All right. Um can you talk one of the things you also mentioned um was denial of wallet style activities or denial of service. Can you talk a little bit more about what those risks are, how the users get charged um and people's vulnerabilities to excessive usage of these public agents. Yes. So I think these agents are expensive to run. So I think for organization it's about choosing the platform with usage limitations or built-in usage limitations when you can when you where you can monitor the usage of the agents and limit the access to it and the usage. Uh so this this way this way you can be uh in awareness for the financial issues as well. And I also I also mentioned that uh that Google has their own like resource uh like a quot for each session which could also be um a user can open multiple tabs and and some kind of a bypass it. uh but then they also offer their own product such as I think it's called Google uh armor something which um will be able to will allow you to um to put better quotas to prevent such kind like denial of wallets attacks. All right, thank you. Any more questions in the room? All right, can we have uh one more round of applause? Thank you, Michael and Hagai. Thank you everyone.
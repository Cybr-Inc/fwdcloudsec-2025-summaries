# Taming LLMs to Detect Anomalies in Cloud Audit Logs

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=b-MF3yGk3zQ)

- **Author**: Yuga El Berger
- **Talk Type**: AI/ML in Security

## Summary

This talk presents a practical method for fine-tuning a small Language Model (LLM) to serve as a highly effective anomaly detection engine for cloud audit logs. The core idea is to train the LLM on an environment's specific log data to teach it what "normal" activity looks like. This model is then used to score new logs for irregularity, and these scores are fed as context to a larger, more powerful LLM to accurately pinpoint threats and drastically reduce false positives.

## Key Points

- Traditional rule-based detection engines are difficult to maintain and lack the contextual depth to distinguish between benign administrative actions and malicious attacks in dynamic cloud environments.
- Simply prompting a large LLM with raw log data is a good first step but is insufficient because the model lacks the specific context of what is normal versus abnormal for a particular environment.
- The key to effective detection is to first establish a baseline of normal activity. Anomaly detection provides the critical context needed to differentiate between common, benign but suspicious-looking patterns and genuinely rare, potentially malicious events.
- The presented method uses an LLM "in reverse" not to generate text, but to measure the probability (or "surprise") of a given sequence of log events, effectively creating a nuanced anomaly score.
- This approach is accessible, requiring no manual data labeling and can be trained relatively quickly (~1 hour on a single GPU) using a small, open-source model like GPT-2.

## Technical Details

### Architecture & Methodology

The proposed solution involves a two-stage process that combines a fine-tuned small LLM with a large, general-purpose LLM.

1.  **Stage 1: Anomaly Detection with a Fine-Tuned LLM**
    *   **Model**: **GPT-2**, a small (approx. 100 million parameters), open-source model is used. Its size makes it practical to fine-tune on custom data.
    *   **Data Preparation**:
        *   Cloud audit logs (e.g., AWS CloudTrail) are normalized into a custom, simplified text format the speaker calls "Cloudish".
        *   This involves extracting key signals from each log entry and arranging them in a consistent sequence.
        *   Example format: `zone event_name source response_code` (e.g., `us-east-1 ListBuckets cloudtrail.amazonaws.com 200`).
        *   The speaker notes that more data could be included, such as role information, username, and account ID for richer context.
    *   **Training**:
        *   The GPT-2 model is fine-tuned on the normalized "Cloudish" log data from a specific organization's environment.
        *   **Dataset**: Requires a corpus of around **100,000 log samples** to learn the environment's unique patterns.
        *   **Process**: The training is unsupervised. The model learns the relationships and common sequences in the log data without any manual labeling of malicious or benign events.
        *   **Goal**: The model learns to predict the next "token" (part of the log entry) in a sequence. A well-trained model becomes very good at predicting normal sequences.
    *   **Inference (Scoring Anomalies)**:
        *   A new sequence of logs is fed to the fine-tuned model.
        *   The model calculates the likelihood of each token in the sequence.
        *   An **anomaly score** is generated based on how "surprised" the model is by the actual sequence of events. If the model assigned a very low probability to a token that actually occurred, that token is flagged as anomalous.
        *   This produces a "heat map" of the log session, pinpointing exactly which events or parameters are irregular.

2.  **Stage 2: Enriched Analysis with a Large LLM**
    *   The raw log data is combined with the anomaly scores generated in Stage 1.
    *   This enriched context is fed into a large, powerful LLM (e.g., GPT-4, Claude 3.5).
    *   The prompt can now ask much more effective questions, such as: "Given this stream of events and noting that the following parts were identified as highly irregular for this environment, is this activity malicious?"
    *   This allows the large LLM to make a much more accurate judgment, significantly reducing false positives.

### Implementation Notes

-   **Training Time**: The fine-tuning process for GPT-2 on 100k samples takes approximately **1 hour on a single GPU** or several hours on a Mac CPU.
-   **Multi-Account Strategy**: For an organization with many AWS accounts, the speaker recommends training **one model for the entire organization** rather than one per account, as a slightly larger model can capture the nuances of all accounts.
-   **Tooling**: The speaker used an open-source visualization tool from Georgia Tech researchers to demonstrate how the LLM processes the input and assigns probabilities.

## Full Transcript

All right, welcome back to track two. Just two more talks remain between us and lunch. Please join me in welcoming to the stage Yel Ber with his talk, Taming LLMs to detect anomalies in cloud audit logs. Give him a hand and take it away. Okay. Hello everyone. It's really great to be here. My name is Yuga El Berger. I am head of AI at sweet security where where my role is basically to unlock uh new cyber security capabilities that LLMs are making possible these days. So what I want to show you today is a practical method on how to train LLMs in order to turn them into very effective very nuanced uh anomaly detection machines. So in other words, what I want to do is to build a tool that is able to take to ingest a vast amount of cloud log data and be able to pinpoint irregular patterns in my log. Before we jump in, I just want to mention my esteemed uh colleague Idokos. So Ido came up with this idea and together for a few months, we hammered this into something that is production ready. Unfortunately, Ido couldn't be here, but I wanted to mention him. So, hat tip to Ido. Okay, but before we get into the weeds into the details, why do we care about anomaly detection? Why do we need that? Um, what is it useful for? How are we going to use that in order to do what we want to do? So, these are questions that I hope we'll answer in the next 20 minutes. Uh, but to start, our end goal is to basically reduce false positives. This is what we uh this is our this is the goal that we're set to do. And if you're building a detection engine, be it a detection engine for malicious activity in my environment or a detection engine that is meant to catch uh secrets that are exposed in public repos. Um whenever you have um um a detection engine and it's a needle in a haststack that you want to find, if that needle is something that we really want to find, we're going to be oriented at recall and mistakes are going to happen. We're going to generate false positives. Um so traditionally the way that we would build detection engines is by setting up some rules. We would have a rule base that will tell us if something is suspicious or not. So rules are fairly easy to write, efficient to implement, but very quickly uh things go out of hand. Rules become cluttered. It's very hard to maintain them and adding more and more rules becomes difficult. Um, so if rules are limited in the amount of context, depth of context that they can take in in order to make the decisions, again, mistakes are going to happen. And LLMs are changing all of that drastically. And there are several approaches of how we can use LLMs in order to drive down false positives. And I'm going to cover two main methods or two main approaches on how to do that. So the first one uh I think is a straightforward one and a lowhanging fruit type of method and I think in previous year uh in this conference this topic was covered fairly in depth. Um but the first approach is basically the the the most basic method. Let's just write a prompt um and provide the prompt provide the LLM with the context of all the data that we wanted to analyze. So it could be all our cloud log events. So, we just feed that to the prompt and we can ask very powerful questions like um does this uh stream of event contain any malicious activity? How severe is it? Score it for us. Um it could be things like uh break down this cold log of events, break it down into a story line with chain of events that we can easily understand. Another very cool example is uh we can ask it to pinpoint the smoking guns. What I mean is uh we want the LLM to tell us what are the details in the input that made the LLM determine that this activity is malicious. So all of that has been working really really well uh especially since GPT4 and Claude 3.5. This has been working really well and giving great results. And whenever we uh have the LLM participate in the scoring mechanism of the rules that I mentioned earlier, results are much much better and false positives are going down. But this is not enough and we can do better especially in cloud environments. So cloud environments are very very tricky. Uh by their nature cloud environments are are dynamic and programmatic. So whole environments can be created and altered and destroyed just by a few lines of code that have admin privileges. It's very very hard to tell the difference between friendly DevOps or uh evil hackups because basically they look the same. It's it's same it's the same operations. It's the same commands but the intention is different. So it's very hard to tell the difference between the friendly and the malicious. And one element that can help us draw the line between them and separate them are is a anomaly detection engine. So if it's a if it's a suspicious pattern in my logs that is very common. It happens a lot. It's been going on for years. I would treat that suspicious activity or the potentially suspicious activity. I would treat that completely different from a pattern there is that is very rare in my environment. So anomaly detection is critical to helping us uh or helping the LLM further analyze the input events much much better. Okay. So now let's go deep in terms of how we're actually building this and let's cover uh LLM 101 in just a few minutes. Okay. So we're all used and I'm going to use a visual demo. Uh this is an open- source project by Georgia Tech researchers. Really amazing projects that helps you visualize and see everything that is going on under the hood when an LLM works. So this is showing GPT2 the model. Um everything is running in my browser. It's it's pretty amazing. Worth uh worth checking it out. So we're all used to LLMs that we feed it with some kind of a prompt and it would generate the next token for us. Okay. So, for example, if I would say, sorry about that. If I would say my name is, what do you guys think the LLM is going to respond? What's the next token for my name is? Any guesses in the room? Feel free to shout. Which one? I I don't think the LLM knows me, so no, it's not going to be my name. But let's see what what comes out of it. So we can see here all the attention layers and how everything is built. Okay. And gladly the answer is my name is John. Okay. Disregard this part. The top choice is John. Okay. So the interesting thing to see here and and this tool is making it very visual. We don't from the LLM we don't just get the top token and the likelihood of the top token. We actually get the likelihood of all the possible responses in the dictionary. We have this right here on the right hand side. All the likelihood. Keep that in mind. We'll use that a bit later. Okay. So now if I give it a little bit more context and I say everyone calls me Tom. Did I write that correctly? Everyone calls me Tom. What's my name now? Drum roll. Okay. So obviously the LLM figured it out and it thinks that my name is Tom, but it's this is not a small feat. Uh just until recently being able to understand those relationships between the token, it's not a trivial thing. So the fact the LLM figured this out means that it has learned the relationship between the different tokens and all of this happened just by training on a lot of English. the LLM read so to speak a lot of English and figure out figured out those connections. Okay, let's take this one step further and now we're going to use LLMs in reverse. So until now we wanted the LLM to generate more tokens for us. Now our goal is to just um take a peak a sneak peek at what the LLM thinks of a given input sentence. Okay, we don't want new tokens. We want the numbers under the hood. So our sentence earlier was everyone calls me Tom. So my name is Tom. And since Tom is the most likely response as we saw before, let's put let's let's mark that with a yellow. And uh if we would have fed the LLM with a different slightly different sentence. Everyone calls me Tom. My name is Thomas. So it's not impossible that this would be the response, but it's not really likely. So let's put that as an orange. Okay. And the way that I'm calculating this color is by looking at the difference between what the LLM said that the likelihood of this given token is compared to what we know that the most likely token should be. The difference is basically the color. And by the same way, if I fed the LLM uh and said that my name is John, that would be a red. And I can do this trick for all the tokens or all the words in my input, not just to the last one. And and it works the same way. So here's another example. We have this weird sentence. It's not really a sentence. Um the LLM never trained on such a sentence. Monday, Tuesday, Wednesday, etc. But still, we can see that everything is yellow, meaning the LLM sort of understood what's going on and the relationship. It made sense to it. But when we fed it a slightly different when I flipped the order between Thursday and Wednesday, it knew that something is off. So, and we don't just know that the LLM thinks that the second sentence is off. We know exactly which token it is that threw it off. It's Thursday in this position. Okay, so now we have this sort of a a metric that we can use that we can read off the LLM when we're feed feeding it with input and we have this metric that we can understand uh how anomalous the LLM thinks or irregular the LLM thinks that my input is and since our goal was to do this on cloud log data let's go in that direction. So this is just uh a snapshot of of cloud trail and I want to see what the LLM would think of my log trail. Okay. To do that I would first normalize the log or format it in a format that is is easier to feed it to the LLM. And what I did is basically I just took row by row and extracted the signals that I wanted to to be here. So uh I put first the zone and then the event name, the source and the response code. Okay, I could have I could have uh uh made this more uh interesting and added ro information and username and account etc. But for sake of simplicity, this is going to do. And when I feed this to my LLM, not not very much surprisingly, this is what I get. Okay, and this is the LLM saying, I don't know what the heck is going on. This is completely unexpected. All of the tokens here, all of the words are words that the LLM trained on, but their position and their relationship with one another completely threw off the LLM. And it makes sense because the LLM didn't train on my new language that we just invented here. Let's call it Cloudish. LLM, the LLM never trained on Cloudish. So, this is exactly what we want to do. And um a very simple setup that anyone can use, it's very accessible and very easy to use at at uh practically any resource that you have is to use the GPT2 model. Um first of all, it's open source, so it's very uh uh convenient to use it. Plus, it's it's a fairly small model. It's still an LLM, but it's fairly small, meaning it's it's um roughly 100 million parameters in size. And in order to train it and get really good results, um, you would want to use a 100,000 samples from your log. And training it wouldn't take too much time on a GPU on a single GPU. It would take about an hour to train on this data or you can absolutely use uh your Mac CPU or other uh platforms. Uh, it might take several hours to train, but it's still very usable and and easy to use. Okay. And training, I'm not going to go too much uh into how training is done, but basically it's just a loop where you read the input data, the log data, uh the way that we normalized it earlier, and every time that the LLM uh reads this data, it adjusts uh the response in order to um um in order to be in order to fit that uh input data. So there's no labeling that needs to be done. There's no effort of that kind. You just need to gather your 100,000 samples or more um and run the training loop. Fairly simple. And once we have that, this is what it looks like. So now we get a nice heat map that is giving us a lot of information on what's going on in my logs. Okay. So what do we see here? What we see here is that for example, our LLM thinks that list buckets when it's coming from US East1, that's fine. It makes sense. But um the LLM is sort of surprised that access denied was returned uh for this list bucket and more so uh assume RO from US2 and coming after the list buckets from those zones that completely threw off the LLM. we can read a lot of interesting information and with that we can uh take it and form uh an anomaly score for the overall uh log session and we can have the anomaly score per word or per token and then uh use that. Okay. So once we train the model this is exactly what we set out to do uh by running this model on uh our production environment. um that is very noisy. Uh as you can see, we were able to really separate between the irregular patterns. Uh a tiny amount of irregular patterns versus the the vast amount of log data that the LLM figured out. This is all regular. I'm familiar with it. Here are the very few points that are irregular. Okay, so we have an anomaly detection engine. It works. Uh to build it, uh it's very accessible and easy. It's not too complicated to do. But let's put the two pieces together, okay? And and really leverage what it can do. So, one way that we can use this anomaly detection uh information that we read from the LLM is simply to go back to the original prompt that I presented earlier and now provide not just the input data but actually provide more context. And that context is which uh which event in the logs are irregular. And now our prompt is going to be is going to give us much better results, much le much less uh false positives and be uh much more effective. Okay. U basically this is what I want to show today. There are quite a few details that I masked out uh for sake of time. So for example, what happens when a new irregular pattern uh pops up in my environment? I don't want it to pop up again until at some point I will retrain my model. So there's a uh all kinds of ways on how to solve that and the exact method on how to plug that into the prompt earlier. Again, there are all kinds of methods on how to do that. So there are a few details that I didn't cover today, but all in all, this is uh what we wanted to achieve. Thank you very much. All right, we have time for probably about three questions. Thank you for the talk. I'm curious before fine-tuning the GPT2 model if you tried other ways of achieving the same thing, maybe doing a prompt that specifically explains the format that you gave and just using the kind of vanilla LLM to to do the to do the correlations. Um, I think if it works, it it could be useful because then you can use much better models than just sticking with GPT4 or a model that you can fine-tune. Absolutely. Um, great question. So, um, it is possible to do. However, any off-the-shelf LLM GPT or or GPD4 or Claude 3.7 or 4 these days, um those models are not familiar with my specific environment and I want to train the LLM on my specific environment because my environment has its own set of irregularities, its anomalies. So, I want to train it on on my model. If I wanted to make a large model at the scale of uh GPT4, be able to answer uh here's my input data. Can you tell me which is irregular in my environment in order to train the LLM and bring it to a level where it's able to respond in an accurate way? It would be uh orders of magnitude more effort in order to make that happen in a in a quality way. So this is a a much more leaner method that um uh is easy to do, practical retra you retrain all the time in order to have the LLM continue and learn your environment that is changing. That's why we chose this approach and to combine the two together an off-the-shelf LLM plus the anomaly detection information. So mine was similarly related. I was just wondering what key benefits you found from using LLMs versus traditional classifier models. Um, is it is it mostly it's just easier to to fine-tune rather than fully train a new classifier? Yeah, so it's not just the the uh how easy it is to train, but also the expertise that you would need. So many anomaly detection engines would require machine learning skills uh and and a lot of preparation. This is a new approach that one is easy to do for everyone. Plus it shows a big promise. The whole idea of the transformer is learning those rules, learning those relationships by itself without uh putting a lot of effort in in labeling data. Um so we had the good hunch that this uh this um could could continue involving this method could continue getting better. So that's why we chose uh to use this approach. All right, final question then we're going to wrap. Um, for environments with a large number of AWS accounts in the same organization, would you lean towards training a model per account or trying to train one model for all the organizations activity? I would train one model per organization but it's going to be so in our specific solution um we're training per customer but uh for a given customer if you want to do this in your own network one model that is big enough uh maybe a little bit bigger than GPT2 but not much bigger um one model for all the accounts uh it's big enough to capture all the nuances and be able to handle all the various accounts so one is good enough all right thanks so much. Give him a hand.
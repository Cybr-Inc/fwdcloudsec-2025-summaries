# Farewell False Positives: Building Trustworthy AI for IaC Analysis

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=tJrfSQx_Jeg)

- **Author**: Emily Choy Green (Clearly AI)
- **Talk Type**: AI/Security

## Summary

This presentation provides a comprehensive guide to building reliable AI systems for Infrastructure as Code (IaC) security analysis. The speaker addresses the growing need for AI-powered code review as AI-generated code proliferates, demonstrating practical techniques for creating trustworthy AI systems that minimize false positives and provide actionable security insights through advanced prompt engineering, retrieval augmented generation, and output normalization.

## Key Points

- AI code generation adoption up 215% but security review lags behind
- Software releases increased 30% industry-wide without proportional security staffing increases
- Traditional RAG fails for IaC due to semantic similarity of infrastructure configurations
- Tool use (function calling) provides superior context retrieval for security analysis
- Output normalization through structured schemas ensures consistent, actionable findings
- LLM-as-judge techniques significantly reduce hallucinations in security assessments
- BAML framework enables type enforcement and schema compliance for AI applications

## Technical Details

**AI Adoption Context:**
- Microsoft data shows 215% increase in AI agentic software adoption
- Enterprise comfort with AI tools growing due to competitive pressure
- Significant gap between AI code generation and AI-powered security review
- Jevons paradox: more AI code generation creates exponentially more need for AI security review

**When to Use AI for Security:**
- Ideal Applications: Diverse context types, consistent workflow execution, infinite patience for repetitive tasks
- Avoid AI When: Missing key information, ambiguous requirements, significant trade-offs needed, 100% correctness required
- Goal: Smart determination of when human-in-the-loop intervention is necessary

**Retrieval Augmented Generation Evolution:**
- Traditional RAG limitations with vector stores and semantic chunking
- Tool use (RAG 2.0) with programmatic API calls and MCP integration
- Context-aware retrieval based on specific analysis requirements

**Output Normalization with BAML:**
- LLM-agnostic compatibility across providers
- Type enforcement and schema compliance
- Multi-step analysis structure for architecture understanding
- Vulnerability categorization and findings structure
- Compliance reporting automation

**Hallucination Prevention:**
- Chain of thought reasoning and source citation requirements
- Prompt engineering with explicit permission for uncertainty
- LLM-as-judge secondary verification with claim extraction
- Source material verification workflows

**Implementation Challenges:**
- Quality limitations from input context
- Mixed trustworthy/untrustworthy information sources
- Security concerns including context poisoning and prompt injection
- Human-in-the-loop decision frameworks

**Framework Recommendations:**
- BAML for LLM prompting and type enforcement
- DSPy for auto-optimization through feedback learning
- Direct model integration over opinionated frameworks
- Custom embedding models for improved IaC differentiation

## Full Transcript

All right, welcome everyone. We appreciate you joining us. Uh this session is going to be farewell false positives building trustworthy AI for IA analysis. Uh just wanted to uh just to start to thank the sponsor um one of our silver sponsors cydig um if you're interested cydig.com or additionally check out the booth out there in the lobby and find out more information. Uh any questions that you have, we'll do the same uh afterwards. Uh bring the microphone around, we can answer questions. We also have the Slack thread if you choose that route as well. Um, and uh, other than that, without further ado, this is Emily Choy Green with Farewell False Positives: Building Trustworthy AI for IA analysis. Awesome. Thank you. Thanks everyone. Awesome. I'm Emily. I'm CEO and co-founder of Clearly AI. And today I'll be talking about all the tools and techniques that you need to build a reliable AI system for cloud security tasks specifically focusing on analyzing infrastructure as code. So this is a bit more of an AI AI talk than it is a security talk. Um but as engineers I feel like we should be applying the best tools to solve problems and that means applying tools like AI to solve security problems. So what I'm going to be going through today um is first you know why should we use AI to analyze and scan infrastructure as code what can AI do today what can't it do yet then go into the actual tools and techniques that you can use to build an AI powered application and finally where I think the industry is going from an AI and security perspective so I wanted to start by pulling the room who here is using AI in some form at work today okay good most people um who's using it for just like content generation email writing things like that kind of some people. What about code generation? Who's vibe coding? More people. Um what about code review or security review? Who's using AI on the review side? Okay. A subset of the people that are vibe coding. And that's part of the problem. Um so what we see is um this is actually data from Microsoft. Um AI agentic software adoption is up 215% over the last year. What we've seen over the trend from when Chad GBG came out in November 2022 to now, you know, almost three years later is an increasing level of comfort using AI tools for a variety of production related use cases. And this is across specifically like daily active users from the enterprise perspective. And so enterprises are becoming more and more comfortable allowing their teams to use AI because they realize if they don't adopt um they'll fall behind. But what that means is software releases are up 30% industrywide and they're not getting any more cloud security engineers. So while Vive coding is really magical, it gives people tools that might otherwise not be able to build things like this. So, this is a viral tweet about a SAS that was built with cursor. Um, that same person 2 days later not feeling so good. API usage maxed out, subscriptions bypassed, databases hacked, not trying to attack this guy in specific. It's an awesome and incredible that software is more accessible now than anyway than always. But it's pretty clear that with many more software producers, we're also going to see a lot more software vulnerabilities. So, it's kind of like a Jevans paradox in action. If you don't know the Jebans paradox, it's the idea that more supply paradoxically leads to more demands. And so more AI code is leading to more AI code, which leads to more need to secure that AI generated code. So the need for security is only going to increase. The need for security experts to focus on the most important problems is only going to increase further. So that's why like you really should be applying AI on the review side and not just on the generation side. Um, and why specifically use AI to solve this problem? Like when applying AI to any problem, we should stop and ask ourselves like, is this an appropriate application or are we just really excited to use this new hammer? Um, security tasks have characteristics that align really well with AI strengths. It requires understanding diverse context types like code bases, documents, outputs from cloud providers, images, rich content, many different sources. And so the squishiness of LM inputs actually makes them pretty good at that. It also requires taking that context and transforming it to some normalized output. And finally, unlike um humans, LM are are a computer. They love executing the same well- definfined consistent workflows over and over again. Adnauseium. Like you might get tired reviewing a gigantic new um like CDK diff or like cloudformation stack diff. AI does not get tired reading through all of those changes. So they have infinite patients can parallel process in parallel in on response on a schedule things like that. When should you not use AI? Um any task where you don't have enough key information is too ambiguous today for like the LMS that we have to be able to solve that problem. You see more with reasoning models being able to plan out the steps that they want to take. But often if they're missing that key information, their plans come with like some fundamental logic gaps. So any task that requires significant trade-offs or there isn't really a right answer when you're trying to juggle business priorities with security priorities, those are the things that should be escalated still to a human. And so really the goal of using building this type of AI application to analyze your infrastructure as code is to smartly determine when a human should be in the loop versus when it's not really a difficult trade-off decision. And finally, like if the task requires a 100% correctness, um AI is still not the best thing for that. A AI models are probabilistic. So you can expect a very high probability that it'll respond with the right output, but there's still going to be the edge case where it doesn't. And if you need something that is um 100% like deterministic, you should go with something more like a regax or an FST or something else that is a more like fixed deterministic output. So this next section is around how you actually build out um the right AI tooling to analyze infrastructure as code. So the first key piece is understanding ingesting all the context. So this is high level what um retrieval augmented generation is which is basically taking a query um having some retrieval component meaning going and retrieving information from some place using that to augment um the information so that the generative component has all the information that it needs in order to provide a response to the user. So traditional rag does this by using vector stores and chunking source materials. um it does lead to source issues that look like this. Um so you can build yes you can build frameworks that can better do source citation return the original document but at the end of the day semantic similarity is about as far as this like type of traditional rag goes. So if you have a generic embedding model that you're using to actually chunk the models and and or chunk the sources and then search across the vector B database, you're going to get into trouble when everything looks kind of the same, which in the case of like CDK code, it does like those like chunks of like different S3 bucket configurations end up kind of coming down to the same set of vector embeddings in the embedding space and therefore it's difficult for a retrieve augmented generation system to actually know what context to retrieve that is actually applicable to what you're reviewing and that's really where like this um approach breaks down. So the next kind of graduation from that is tool use which is also called function calling um by some API providers but at a high level the goal is to allow you to use APIs to programmatically search internal systems the web call other agents like this is really rag 2.0 know and funnily that there's a talk about MCP going on at the exact same time as this talk. Um because this is where MCP really comes into play. Um what you'll have is something like um let's say you want to be able to augment your CDK code with Jurro tickets that have been opened asking for specific changes. You could then call the use tool use to call the Atlassian MCP server to get information from those Jira tickets. That's then augmenting all of the response that you're passing out. So that this is kind of how those two things interplay. And this is really the basis to being able to pull in all the context that you need in order to fully review the infrastructure as code. Because often you're not just looking at the code itself. You're understanding okay what is everything else around it. What is the overall design of the system supposed to look like? Who's actually supposed to be accessing it? Like threat modeling the overall like place and thinking about the contextual elements. And so tool use really allows you to like build all that context. So a lot of people ask me about fine-tuning. Um what it turns out this is from OpenAI. Um you can go really far without fine-tuning. There's like two different axes that you want to build up. Um, context optimization, which is what I was just talking about, which is what the model needs to know, and then LM optimization, how the model needs to act. And once you have prompt engineering and retrieve augmented generation, you've really built up the context. You can then start thinking about adding fine-tuning in order to build out the way that the model acts when it receives that context. Um, but it should be a last step and not a first step, especially when foundational models are getting so good. All of the foundation model labs, their goal is to just make the LM optimization better and better and better. And so if you're working upwards on this graph and they're working to the right, um you may never need to fine-tune. So that's like all of the background of like how to build it. But then there's the next big piece which is how do you know that you build something that works and is reliable. So when I think about building a reliable AI system, the first thing I think about is preventing hallucinations. Um there's a bunch of ways to minimize hallucin hallucinations today. Um the two big ones is um chain of thought reasoning to improve explanability and source citation. Doing those two things is the same thing as a math teacher asking a student to show their work. Um the student is much more likely to arrive at the right number if they show their work beforehand than if they don't. Large language models are the same way. Working through that chain of thought helps it to arrive at an accurate answer. Um, doing prompt engineering, giving the LM permission to be wrong and or not to be wrong, but to know that it doesn't know the answer is actually a huge thing to prevent hallucinations because LM's have been kind of like post-tuned to want to have an answer for you. If you're asking it a question, it wants to be able to give you an answer. And so by prompt engineering to say it's okay that you don't have an answer that's actually a really great way to prevent hallucinations. Similar to humans I think in a lot of ways giving them a per giving people permission to not know makes a big difference in people's ability to respond to questions. Um and then the last big thing is LM is judge. So this is taking a second LLM to analyze the first LM's output. Um, and you might be thinking, you know, LM judging LM is not a bad idea. But it turns out the model performs much better when scoped to the narrow task of saying, does this answer actually like is this answer actually accurate, yes or no, the broader, more interesting questions. So specifically, this is an example of using LMS as judge. So you'll see kind of all of the prompt that we put in to actually extract all the different claims from the initial LM's response. So you know if the question is you know what security controls protect this API and the API uses JT tokens with RSA 256 and rate limiting the claims are two pieces the JWT tokens and the rate limiting. And once you have those claims extracted, you can actually then go back to the source material and say where in the source material are each of these claims actually highlighted and use use that to determine if any of these claims were hallucinated. The next big piece is to go through the process of output normalization. And so the goal here is to have consistent, actionable, and grounded output. So um I we use a tool called BAML. It's um created by a company called Boundary ML. It's an open source framework. It's a language for um prompting large language models. And it's great because it's LM agnostic. You can switch across any LM provider and still use BAML. And it allows you to do type enforcement, schema compliance, and actually be able to build all of the kind of output normalizations that you're expecting directly into your code. Um, and this allows you to then pull any of your outputs from your findings from scanning IA into any sort of like CSPM or a findings DB or wherever you currently expect those findings to go. So this is like the first step of that process. In reality, you'd never ask an LM to do all of these things in a single call. Um, but this shows kind of how extensive the typing system is. So the first step is structuring the LM's understanding of the architecture. So breaking down for any given infrastructure component, what are all the infrastructure types? Um where is the file path and line number so you can actually site the sources? Um and then what are all the dependencies and tags and all the different components that you're pulling out. The next step is to structure the LM's understanding of all the vulnerability categorizations that you're looking for. So things like excessive permissions or open ports, broad IP ranges, anything that's specific to what you want it to be looking for within your IC. Like if you went to the very first talk today um about amies, you could have it look for you know missing owner or things like that so that there's no um ability to to compromise that way. So that's like next step. Step three is structuring the outputs of the findings. So this is how you want to structure it based on whatever your findings DB schema is. And this will allow you to then uh be able to um output everything from the large language model directly into a structured store that you can then query across um or use directly in Python code, TypeScript, anything like that. And the final optional step for those of you that have compliance mandates is to be able to have the LM create the whole overall report for you. Um so taking all of the findings and putting it out into like an analysis report that you could give to an auditor. um or something similar. So um now that we've sprinted to through a bunch of AI engineering techniques that you can use for security tasks um and to make your AI applications more reliable, let's talk about some of the challenges. Um the biggest challenge here is the classic like garbage in garbage out. Like LM are only as good as the context that you give them. And it's really hard in environments where you have a mix of information that is trustworthy and information that is untrustworthy. And so if you have a mix of infrastructure as code and infrastructure that was deployed in more of a point-and-click um fashion, you might then have this concept of like staleness where your infrastructure code isn't actually accurate to what your cloud environment looks like. And so therefore, is this scan actually accurate? And how can you add additional context? Um, so that's like a key challenge that we continue to face. Um, there's a lot of misinformation about using AI. I think there's still a ton of skeptics on, especially in the security community, about when and how um, AI should be applied to security tasks. Actually think that's probably why there's still many more Vibe coders than there are Vibe code reviewers. Um, because people like have kind of a lot of basic concerns around the fundamentals of the systems. Um there's obviously AI security threats um things like context poisoning, prompt injection, especially via MCPS because you're basically when you're calling an MCP, whatever that response is is being loaded into your prompt. So it is a way for kind of a stored prompt injection so to speak. Um and then you know the overall concerns around the role of the engineer in this and when do you add the human in loop when don't you when is automation something that is a replacement versus a support for manual work. Um, and finally, I think the big future that I hope to see is tying it all together of saying, okay, let's threat the model this system. Um, pull it into the secure code analysis that I went through today, pull it into the pentest, and have all of those pieces be aligned so that you can actually have a true cloud security engineering assistant. Um, so my goal today was to equip you with everything you needed to build AI powered cloud security automation inhouse. If you scale the scan the QR code, you'll get all of our AI um engineering tool recommendations. So, um mostly open source or free. Um we noted companies that have better privacy and security commitments um since that's always really important. Um so hopefully this can help you build this all internally. Thank you. Nice work, Emily. Um, okay. So now, um, any questions? Um, if you just want to feel free to raise your hand if you've got a question. I'll bring the microphone around. Anybody? No, you've stumped them. Okay. Oh, we got a question. Okay, cool. Hey, uh can you clarify to what extent the analysis is security related or um I guess I'm a little more interested in in uh what part of the analysis might be qualitative? I see a lot more correct IA written badly than I do good scalable non- tech debt IA. Yeah. Have you used claude code at all? I have not. So claude code is incredible at like the qualitative feedback. So if you add it as it's we have cloud code as like a PR reviewer on all our PRs within our company and it will literally be like hey for readability you should restructure it like this and hey and so you can actually tell cloud code this is what we this is what good um like CDK looks like. This is the way that we expect it to be structured. this is what our kind of general um you you basically want to add it's almost like an LM.txt inside your codebase that says like this is the way that you should read and understand and structure your code. Um and it'll give you all that feedback on your PRs from like a qualitative perspective as well. Um I will say that Terraform is like what LM are still the worst at. Um both generating and reading Terraform. Um so that I will say is like still a challenge. Um, but from a correctness perspective, it is generally able to do that piece. Awesome. Uh, got a couple more questions. Right on. Do you have any suggestions on um on measuring the the toil reduction benefits that comes from when you when you get to roll this out, right? like before and after in terms of quality, in terms of time saved, in terms of just uh the iterations back and forth and all that. Yeah. Um metrics are your best friend always like showing how long tickets stay in a queue or how long someone is spending on a task is like always going to be really worthwhile to sheet or show that your team is like under the pressure that you feel like. Like at Amazon, we used to have all of our questions being answered. like people would ping us on Slack and ask all these consultative questions and now and like it sucks to be like now you have to open up a ticket, but it made a huge difference for our leadership to see how underwater we all were. Um, so the more you can do those pieces and measure the before state that you're currently in, you'll be able to then show the reduction in the after state cuz like time to first like comment on the ticket will go down and time to remediation will go down and you can like you can measure all those things in a ticketing system. And so the more you can drive engagement with your engineers into some place where it's measurable, um, the better you'll be at being able to show like the return on investment of like tooling that is meant to save your engineers time. It's a hard problem. I will say. Awesome. We have another question here. You mentioned that rag worked less well when things looks very similar. But IC repositories can sometimes be huge in terms of the number of tokens. So I wanted to ask you what your thoughts are on how that could be approached uh in a better way. Yeah. So there's kind of two pieces. One is to use a custom embedding model that is like very much tuned on like your vector space. And by doing that you can then it's kind of like a rainbow table problem. Like if you are using like a generalized embedding they'll all kind of hash towards the same area. But if you're using a custom embedding model that is very specific in IC, then you'll be able to have them all hash like a little bit more like away from each other in the embedding space. So that's one option. The other option is like really the tool use piece. Like what the goal of tool use is is you're pulling information in at runtime that is contextually relevant to the question that's being asked. And so it's basically saying, "Hey, go do a search on my codebase and look for all of the things that are connected to this thing. instead of trying to load everything in to a vector DB or any other type of semantic search. Um, and so this has generally been a much more successful output. Um, this is what tools like cursor use as well is like all tool use to search across your um, uh, codebase as opposed to like rag. Awesome. Got a couple more questions. go who I suffered. Um, some of the Terraform security issues I've noticed seem to exist between a large number of files. It's um, you know, contextual, it's jumping between variables and locals and uh, different resources. Um, have you had issues with that maybe uh, rag underperforming or just it leading to a large number of tool calls? I think what has what I've seen at least anecdotally is like and the reason why I kind of titled this like a like farewell false positives is that LLMs are actually much much better at contextualizing than like traditional scanners are um because traditional scanners often only are looking at like specific patterns in specific files whereas LMS are able to kind of build that overall talk um like across everything. Um, LMS are also best when you have like a few short example. So, um, where we are able to make it the most accurate is when we have a few examples of like potential attack chains. So, then it knows like, hey, I should be looking across this set of things in order to determine if I have like a successful attack chain here. Um, so it's kind of a not a non-answer as much as like I think you can tune it to work better than than you'd think. Um thank you. Okay, we'll do this last uh question here for now. Um can you tell us uh what are the frameworks you're using like TensorFlow? Um so we our biggest frame framework that we use is BAML. Um I am a like absolute huge BAML fan. Um and then everything else is like prompted directly um to the LMS like via BAML. Um we don't I've found that like things like lame chain lang chain, llama index, tensorflow, they're all too opinionated so that you then kind of run out of the boundary of the way that they've done it. Um so we we generally work directly with like the base model providers and use BAML on top. Um this has a bunch of other suggestions. DSP is a big one um that we're working to incorporate right now because you can then autolearn from feedback um to autom autooptimize your prompts um without actually training on anyone's data which is awesome. So it's a way to customize output for individuals without actually training and learning on their data. Um and then there's like different eval frameworks on there as well. Cool. Awesome. Well, if uh if you've got any more questions, I would say please uh continue afterwards and and reach out to uh to Emily and and go through any additional questions you've got. But let's go ahead and give Emily one round of applause here. Thank you. Nice work.
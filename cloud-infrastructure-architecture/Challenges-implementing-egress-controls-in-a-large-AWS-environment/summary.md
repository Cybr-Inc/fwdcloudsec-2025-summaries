# Challenges implementing egress controls in a large AWS environment

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=W5yH-2mf0o4)

- **Author**: Greg Almond
- **Talk Type**: Cloud Security Architecture

## Summary

Greg Almond discusses the practical challenges and solutions for implementing centralized egress controls in a large-scale AWS environment processing 25 terabytes of traffic daily across nearly 200 VPCs. The talk covers the architecture using AWS Network Firewall, the significant issues encountered with log volume and cost, the complexities of creating and maintaining allow lists, and various methods for bypassing and securing the firewall. Key takeaways emphasize the need for deep visibility to optimize costs and the importance of a multi-layered security approach to effectively mitigate data exfiltration and command-and-control threats.

## Key Points

- **Centralized Egress Architecture**: The solution involves routing traffic from workload VPCs through a Transit Gateway to a central inspection VPC in each region, which contains an AWS Network Firewall.
- **Log Volume & Cost**: The system generates massive log volumes (280 GB/day of alert logs), making analysis and cost management a primary challenge. Converting logs to Parquet format was necessary to make querying with Athena feasible.
- **Cost Optimization**: Visibility from the firewall allows for cost attribution. A significant portion of egress costs came from vendor traffic (e.g., 48% to DataDog in Kubernetes VPCs), which could be rerouted via Private Link, dramatically reducing costs to the point where the security solution could pay for itself.
- **Allow List Complexity**: Application owners often don't fully know where their services send traffic, making it impossible to create an allow list without first observing traffic in a monitoring-only mode for several months.
- **Firewall Bypass Techniques**: Attackers can bypass controls using methods like forged SNI, Encrypted Client Hello (ECH), DNS exfiltration, and VPC endpoints. Mitigations require a multi-layered approach, including blocking ECH, using DNS Firewall, and carefully crafting endpoint policies.
- **Tooling and AWS Limitations**: The project ran into challenges with AWS services, including a Terraform provider bug, AWS Glue's inability to detect network paths through a transit gateway, and the lack of certain features in AWS Network Firewall (like Suricata's `threshold` rule) that would help manage log volume.

## Technical Details

- **Architecture Components**:
    - **Initial State**: Workload VPCs with individual NAT Gateways.
    - **New Architecture**: A centralized egress model using a **Transit Gateway** to connect workload VPCs to a central **Inspection VPC** in each of the six regions.
    - **Inspection VPC**: Contains **AWS Network Firewall** and **NAT Gateways**. The firewall is placed *before* the NAT gateway to see the private source IP addresses for easier traffic attribution.
    - **Logging**: Logs are sent to an S3 bucket in a dedicated logging account. Two types of logs are generated: **flow logs** (similar to VPC flow logs but with a flow ID) and **alert logs** (protocol-specific details, e.g., SNI for HTTPS).

- **Implementation Steps & Methodologies**:
    - **Log Analysis**: Due to massive log volume (280 GB/day alert logs vs. 3 GB/day flow logs), raw JSON logs caused Athena queries to time out. Logs were converted to **Parquet format** using **AWS Glue**, which improved query performance by 30x.
    - **Allow List Creation**:
        1.  Initially run the firewall in a "monitoring-only" mode to log all traffic without blocking.
        2.  Observe traffic for several months to build a comprehensive allow list.
        3.  Log *passed* traffic to know when a rule in an allow list is no longer used and can be safely removed.
    - **Firewall Rule Ordering**: To log passed traffic, the firewall must be set to **"strict rule ordering"** instead of the default. The talk notes a bug in the Terraform provider that required manually deleting and recreating the firewall to change this setting.
    - **Cost Reduction**:
        - Use firewall visibility to identify high-cost traffic flows.
        - Reroute traffic to trusted vendors (like DataDog) through **Private Link** connections, bypassing the firewall and NAT gateway, which significantly reduces cost.

- **Technologies and Tools Mentioned**:
    - **AWS Services**: AWS Network Firewall, Transit Gateway, VPC, NAT Gateway, S3, AWS Glue, Amazon Athena, Route 53 Resolver DNS Firewall, CloudTrail, VPC Endpoints, Private Link.
    - **Open Source**: **Suricata** (the engine for AWS Network Firewall).
    - **Infrastructure as Code**: **Terraform** (mentioned for a bug in its AWS provider).
    - **Third-Party**: **DataDog** (a major destination for egress traffic and a tool used for traffic attribution in Kubernetes).

- **Bypass Methods & Mitigations**:
    - **Forged SNI**: Network Firewall cannot detect this.
    - **Encrypted Client Hello (ECH)**: Hides the destination domain. The only mitigation is to block ECH traffic entirely.
    - **DNS Exfiltration**: Mitigated by blocking traffic to external DNS resolvers, logging queries to the internal Route 53 resolver, and using **DNS Firewall** with the same allow list as the network firewall.
    - **VPC Endpoints**: Can be used for exfiltration. Policies must be carefully configured to restrict access to internal resources, but this is complex as many AWS services require access to public Amazon S3 buckets.

- **AWS Feature Gaps Identified**:
    - **AWS Network Firewall**: Lack of support for Suricata's `threshold` rules, which would help reduce repetitive log entries. No option to save logs directly in Parquet format.
    - **AWS Networking**: No "egress-only" mode for `Block VPC access` to prevent bypassing centralized egress. An "ingress-only" Internet Gateway would also be useful.
    - **AWS Glue**: Glue's connection check for S3 is naive; it only looks for a NAT Gateway or S3 VPC Endpoint within its own VPC and fails to recognize a valid network path through a Transit Gateway.

## Full Transcript

It's my pleasure to introduce Greg Almond, our next speaker. He's going to be talking about challenges implementing egress controls in a large AWS environment. We'd like to thank our gold sponsor, Data Dog. As always, we'll have the question thread in there. Please also remember to scan the QR codes outside and send your session feedback. It really helps the organizers uh pick and uh sort the talks for next year. Well, I hope you all had a good lunch and are feeling refreshed, not too sleepy. Um, so I'm I'm going to be talking about challengings challenges implementing egress controls in a large AWS environment. Um I'm a member of blocks cloud security team but most of this work has been done when I was done when I was a member of the um product security engineering team and until recently that team was responsible for managing the afterpay AWS environment. So Afterpay was acquired by block three and a half years ago. So um and what so what I'm describing is the afterpay AWS environment which from a networking point of view looks very different to the block environment. So you can't um so former block employees what probably won't recognize it at all. Um a little bit about me um I've been securing AWS environments professionally for the last eight years. Before that I was an AWS infrastructure or platform engineer and further back I've had several roles um where I was basically doing networking which was why I ended up with this project. Um what do I mean by large? So this uh this um tooling is processing approximately 25 terabytes of um egress traffic per day. and um consists of nearly 200 VPCs spread across six regions. Okay. So I'm going to start by um discussing the threats. This um control is intended to mitigate the architecture that I used and give a brief intro to AWS network firewall so that you can understand some of the challenges that I faced. um I'll talk about the the volume of the logs and the cost of the logs and also um something about um the cost of the egress solution. Um so the volume of the logs posed quite a challenge um both in terms of the cost of them and in analyzing them and but there um also there were ways to reduce the cost um allow lists I'll talk about some of the challenge of in in creating and maintaining them then I'll talk about a few ways to bypass the network firewall and maybe some and some ways that you can prevent people from bypassing the network firewall. Um the volume of the logs meant that I needed to use AWS glue to help analyzing them. Um and there and also um there was an oddity I found when migrating VPCs using AWS glue to network egress. So uh that wasted a fair bit of time. So I'll hopefully save you some time if you ever encounter that. And then I'll talk about some missing AWS features. Um so missing from network firewall and also from AWS networking things that would have made the project easier if they have been there. So the threats that um this control is intended to um mitigate is um ex so excfiltration of sensitive data um customer data in particular and um command we want to also block command and control traffic. Um and there um the means the mechanisms by which um the main mechanisms by which people might um do some of these things are network traffic um DNS traffic DNS fixtiltration which is fairly well known and using VPC endpoints um that they're the primary mechanisms that you might um excfiltrate data out of AWS but by no means the only ones. Um, so this shows a typical VPC architecture of before this project. So we've got ingress traffic going to a load balancer going to EKS. Um, this is just a t a typical example. Most of our VPCs didn't have EKS. They had lots of various things in them. And then they had NAT gateways and egress traffic going out the net gateways. And we actually had a pre-existing transit gateway network so that the VPCs could talk to each other. Um, and that is fundamental in the solution that I chose. So the current infrastructure um deleted the net gateways out of the um workload VPCs and also um changed the route tables to send egress traffic into the transit gateway network. And then we have uh six inspection VPCs one in each region um with a network firewall and that gateways. So um the tra the and then uh the logs are going to um an S3 bucket in a separate logging account. Um, so note that I've I've put the uh the firewall before the NAT gateways. That's fairly important because that means that the firewall is seeing the private IP addresses of the um traffic of the source of the traffic and that makes it very easy to identify which um VPC or which inst EC2 instance or whatever is the source of the traffic. If you're if the source of the traffic is Kubernetes, sometimes it's a bit harder than that. But uh for other things it makes it quite easy and it also means that you can uh create you can have different rule sets in the firewall for the different VPCs. So you could have a um you could have completely separate allow list for each VPC um which you probably want to do. Uh so uh network firewall produces two kinds of logs. flow logs which are very like VPC flow logs except they have a flow ID in them as well and uh alert logs which are more oriented around the protocol. So if for for uh for HTTPS connections it will tell you the the SNI and it will tell you the version of TLS and it will give you the server certificate and other stuff too. It's too much to enumerate. Um and other protocols have similar levels of information. Um so AWS network firewall is um based on the open-source Suriricarta intrusion detection prevention system and AWS supports siricarta's rule syntax. It also provides a simplified syntax tailored for ease of use. Um AWS documentation tends to focus on standalone deployments using simplified rules. So like the beginner get getting started when you don't know much about what you're doing. Um rather than the large scale environments where um surartis uh full capabilities are more useful. Um okay. Oh and the alert logs have flow IDs as well. So that makes it very easy to join the two log types together to correlate them. So the log volume and egress crap um cost. So it's very difficult to estimate the cost of the logs because you don't because the volume of the lo of the alert logs is very is very dependent on the particular rule set that you're using and it can be dramatically different. And so I the only way I know how to estimate is to put a particular rule set in place, send some traffic to it and just assume that the volume of the logs is prop proportional to the volume of the traffic. Uh it seems to roughly work but only roughly cuz I've got six regions to compare and I can see that the ratios are not the same. um in a busy day, so probably the highest traffic day of the year, uh this system is generating 280 g gigabytes a day of alert logs and uh flow logs about 3 GB a day. So that ratio really surprised me. I was expecting the flow logs to be much more much bigger than the alert logs, but um it wasn't the case. And this is because I've got the rule set I'm using um was using at this time is uh a fully monitoring rule set. I'm logging all the traffic so that I can see what's going through and that's part one of the preparatory steps to creating allow lists. So one of the problems is that uh the surakarta has a nice threshold feature where you can do exponential backup on the volume of the alert logs. Um, and that would have been extremely helpful because the alert logs are have a ma a massive amount of repetitive information and it's so uh it's not adding all that much and we can dramatically we could dramatically reduce the volume if this this feature had have been implemented. Uh the volume of the logs was such that uh analyzing them was very difficult. So when I first try started um trying to use Athena to um analyze them, I could only query 12 hours in a that was before Aquina um timed out. So uh I had to convert them to into par format which is a log format often used by which is a format often used by data engineers. Um which let me and the uh that ch that improved the querying speed by about 30 times. which meant that I could generally query a month at a time which was more what I needed to be able to do. Um and as part of that uh conversion to par I also annotated all the logs with the name of the VP the um VPC that originated the traffic that made it much easier to query because uh trying to query um random cider blocks in like doesn't works well when it's a SL16 or SL24 but if it's somewhere in between it's quite difficult and with the visibility Sorry, I'm getting behind. With the visibility that I um that the network firewall gave me, I was able to to attribute costs to particular traffic flow. So when the the data lake was c when um the data team was copying caser events into the data lake, then I could say exactly how much um that traffic contributed to the cost of the of the network egress solution. And that's the kind of visibility that um I don't know how else you can get. And and um because of that, I was um publishing charts of what was the most expensive traffic each month as a way to encourage people to uh rearchitect their solutions and get the traffic down. And I found that um one of the biggest contributors to to the egress costs was our vendors. So um in particular in the Kubernetes um VPCs 48% of the egress traffic was going to data dog domains and some of our other vendors were also um big consumers. But the good thing is that most of those vendors enabled you to have um support private link connections because we because they're our vendors and they've gone through the due diligence process. We don't need we know exactly where it's going and we don't need to send it through the the network firewall. So we just send it via private link connections bypasses the network firewall and dramatically reduces the cost. And actually I found that if all the cost-saving opportunities that I identified were actually implemented, we could reduce the cost of the network egress solution to below the cost of not having it because of the increased visibility. Significantly below the cost. So like it was half of the cost of just having that gateways and no network firewall. Yeah. So creating and maintaining allow lists what's sorry um there's lot when you start looking at the traffic you're going to find lots of surprises people doing all kinds of crazy things um and in my experience application owners seldom have an in-depth understanding of where their eress traffic is going that means that you can't ask them what should be in an allow list. They will know some of it, but they will miss a whole bunch of stuff. Um, so that means that the only way to create an allow list without breaking everything is to observe for several months. So that's why the I was using monitoring rules and um and then there will be some kinds of traffic that you won't like and you want them to modify their application or configure them differently. Uh maybe they're sending unencrypted traffic. Um stuff like that. Um also once you once you actually put allow lists in place, you can't maintain those allow lists unless you're actually logging the traffic that's passed. because say an allow list has been in place for a while uh you want you can't remove a domain unless you know that um people it's no longer being used otherwise you're going to break something. So it's very important to log the the traffic that is that is uh actually allowed and uh network firewall makes that is is a little bit difficult because um the the default rule set is um the default rule ordering that you get out of the box is um groups act groups the rules by the action type and the past rule comes before alert rules. So if you're using default rule ordering, it's impossible to log the traffic that is passed by the allow list. So that means you've got to switch to uh strict rule ordering. And um if you need to do that, be warned that the Terraform provider is buggy and you I had to manually delete everything and recreate it with the different rule ordering. It couldn't actually change it. Okay. Bypassing network egress controls. So you can forge the SNI field. Um network firewall can't detect that. You can use encrypted client hello. So this is something promoted by Cloudflare and other people. So to in to encourage to support private browsing um that uses encrypted DNS and even if you're actually decryting the HTTPS traffic, you can't find out where it's going. So the only mitigation is to just block encrypted client hello traffic, which is what the great firewalls of Russia and China are doing. uh DNS exfiltration. You'd want to uh block DNS traffic to external DNS resolvers and you want to have DNS logs of the 2 resolver. And maybe you might want to use the DNS firewall as well and use the same allow list in the DNS firewall as you're using in the in the firewall. Uh VPC endpoints are a great place to extract data. Um, and but they're quite tricky. So the obvious thing is to put something in the in the policy that only allows communication with uh other parts of your organization, but that will break a whole bunch of services, especially in an S3 endpoint because a lot of AWS services depend on being able to access special Amazon buckets and that what's ne what and that is not very well documented. I uh a colleague of mine recently published a blog post explaining how you can exfiltrate data using event bridge. Uh there's a link to that in the references section. Um and I think in in order to get good um protection you need to use multi a multi-ervice approach. So I would be using network firewall, DNS firewall, um cloud trail network activity logs and uh combine it all. Okay, glue challenges. So uh I deployed um the network egress to a VPC that had AWS Glue in it. Glue was extracting data from a database in the VPC. All glue jobs immediately failed. And why has that happened? Because Glue needs an a connection to S3. It had a connection to S3 via the network egress that was working. But Glue's way of detecting that it has a path to S3 is to say, is there a net gateway in the VPC or is there a VPC S3 VPC endpoint? It doesn't actually test that the networking works. Missing features in AWS. So uh so there's a block VPC access. It would be really nice if um because we're using distributed ingress, centralized egress. be really nice if uh the block VPC access supported that but it doesn't because AWS is much more concerned about ingress traffic than egress traffic. Um, yeah. So, it would be nice if that had an egresson mode, which means that uh I can prevent people from bypassing the centralized egress. Uh, if there was an ingresson internet gateway, that would be really nice. And um, there's an egress only one, so should be possible. uh if they implemented siricarta threshold rules we could deal with the log volume much better and firewall logs in park format so VPC flow logs there's an option to save them in park format that would have saved quite a bit okay this is the references I'll leave that for a bit and we better take questions because I'm slightly All right, we're going to start with a question from the room. Let me pull this one up. Uh, and this one comes from Eli. Says, uh, during analysis, how did you do traffic attribution to specific applications? I found it challenging to translate the source IP in, for example, the BPC flow logs to specific instances or nodes due to ephemeral workloads and IP turnover. So in Kubernetes it's very difficult um because our Kubernetes architecture um has multiple pods on a node. So the firewall gives you the IP address of the node but then mapping it to the pod is a is a massive challenge but data dog was very helpful with that. Um other for other kinds of traffic you have the domain and and the source IP. So it was pretty straightforward. And our next question comes from David. How are you layering the various firewall services DNS and network to create a more comprehensive solution? And wouldn't this introduce potential issues? Um, so I haven't actually uh layered I haven't actually combined them all yet, but uh I'm the um I think the DNS firewall needs to be in the actual source VPC, whereas the firewall is in in a separate VPC and you need to match the allow list, the perVPC allow list to the um VP the um DNS firewall allow list in the VPC. Uh any questions in the room? Thanks. Um I imagine this is pretty expensive with the volume of traffic that you're working with here. Um have you uh investigated alternate options for either the um NAT gateways or those network firewalls just due to the the traffic costs? Like have you have you looked at something like alternat or any of those kind of projects? uh so uh we have deployed DNS f the used just the DNS firewall in one VPC. The DNS firewall is very cost effective um but the protection is not as high. Um but also this is where the cost savings come in in place. Um, so it's possible to actually save money by putting network egress in because you find all this traffic that no longer needs to go through the knack gateways. So the savings from the decreased net gateway traffic outweigh the cost of the firewall processing. I've got a two-part question. Um, so one of the threats you mentioned was excfiltrating sensitive data. Yes. Um how how how did you actually get in the middle of understanding if the data that's being uh moved out was sensitive or not? Uh and you were talking about bypassing using like um ECH or uh you know TLS even. So how do you kind of get in between that to know if sensitive data is being excfiltrated? Uh so we're not um we're just trying to block all movement of data out except to known destinations. So we're not worried about whether it's sensitive or not because um command and control the same thing blocks um C2 traffic and that's generally not going to be sensitive data just harmful data. Um just question regarding known destinations that you just mentioned. Have you had the uh the case where you encountered like for example jump hosts or like host that we cannot um predict the destinations where there's like a lot of potential destinations out there. Oh so um so we're just looking at u destinations outside of our environment. So they're um sent traffic by um domain names DNS queries. So um we're just doing that. We're not looking at um it's not analyzing traffic within our network where that would be much bigger problem. These type of workloads that you cannot restrict because they might request a lot of domains. Uh they're not going to that many different destinations. It's usually massive numbers of requests to a smaller number of destinations. Just two more. Okay. On that on that same topic, like for a build environment where there could be a lot of different URLs or external repositories that I would need to reach out to, does it also go through the centralized egress point? Uh that's the case where we're using the DNS firewall because of the volume of the traffic. uh uh if you're worried about I mean you probably want uh a lot of build artifacts to come from your own uh artifact repository rather than publicly available in order to address supply chain attacks. And last question, did you run into anything where the domains will be dynamic because they're fronted by a CDN or anything where there's lots of extra hops? We've we've had issues with that trying to implement allow list, but there's lots of secondary domains that you'd have to capture as well. Well, that's essentially what the uh encrypted client hello is does. So, it goes to one domain such as Cloudflare and then encoded in the traffic is the the real destination. So, uh you want to probably block that traffic. All right. Thank you very much.

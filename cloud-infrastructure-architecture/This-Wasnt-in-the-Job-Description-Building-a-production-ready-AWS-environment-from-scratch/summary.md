# This Wasnâ€™t in the Job Description: Building a production-ready AWS environment from scratch

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=rai0bTOamG0)

- **Author**: Nick Jones and Moit Gupta
- **Talk Type**: Cloud Architecture

## Summary

Two penetration testers, Nick Jones and Moit Gupta, share their journey of building a production-ready AWS environment from the ground up for their security consultancy. Lacking a dedicated infrastructure team and a large budget, they leveraged their security expertise to design a low-maintenance, secure, and flexible multi-account architecture. The talk details their pragmatic approach, focusing on strong account segregation, infrastructure as code with Terraform, and a unique strategy for balancing security controls with the operational needs of consultants.

## Key Points

- **Penetester-led Build**: The environment was built by security consultants, not a traditional cloud/infrastructure team, leading to a security-first but highly pragmatic design.
- **Workload-Driven Architecture**: The design accommodates three distinct and conflicting use cases: flexible client delivery, a lax research sandbox, and a hardened production environment for internal tools and data.
- **Account Segregation is Key**: A multi-account strategy is the primary security control, used to create strong blast-radius containment between clients, research, and production workloads.
- **Pragmatic Security Controls**: They made deliberate choices to reduce operational overhead, such as avoiding custom KMS keys in favor of strong IAM controls and forgoing AWS Control Tower due to its opinionated nature.
- **Automation with Terraform**: The entire organization, including account creation, networking, and IAM, is managed as code using Terraform to ensure consistency and reduce manual effort.
- **Unique Root Account Handling**: Root accounts are secured by creating them via AWS Organizations and immediately discarding the password, with an SCP preventing any root user activity.
- **Hybrid Networking Model**: They combine a shared VPC model (using AWS RAM) for core corporate workloads with isolated VPCs connected via Tailscale for other environments, ensuring network separation.
- **Federation with a Caveat**: While using Entra ID for federation with AWS Identity Center, they intentionally keep the management account isolated, relying on a few break-glass IAM users to prevent a full takeover via IdP compromise.

## Technical Details

- **Architecture Components**:
    - **AWS Organizations**: Used to structure the environment with a clear OU hierarchy (Core, Workloads, Research, Client, General Service).
    - **Terraform**: The primary tool for Infrastructure as Code (IaC), managing account creation, OUs, StackSets, networking, and IAM policies.
    - **AWS Identity Center (SSO)**: Federated with Entra ID for user authentication to most accounts.
    - **GitLab CI/CD**: Used for automation, with OIDC for secure, keyless deployments into AWS accounts.
    - **AWS Backup**: Implemented for a 3-2-1 backup strategy, with cross-account and cross-region backup vaults.
    - **Tailscale**: Used as a VPN and to provide one-way connectivity to isolated VPCs without a full transit gateway.
    - **Prowler**: Deployed in an ECS task for continuous security posture monitoring, with results heavily filtered to focus on critical issues.

- **Implementation Steps**:
    - **Account Creation**: Automated via a Terraform module that creates the account, sets up default Identity Center roles, creates corresponding Entra ID groups, and deploys baseline resources via StackSets.
    - **Networking**: A central networking account manages a shared VPC. Terraform automatically creates and shares subnets (public, private, internal) with workload accounts using AWS RAM. Tags are copied to shared subnets using a management role.
    - **DNS Management**: A central DNS account holds all domains. Terraform is used to manage records, with delegation of specific subdomains (e.g., for ACM validation) to other accounts via carefully scoped IAM roles.
    - **CI/CD Setup**: GitLab repositories mirror the AWS OU structure. A CI/CD role is automatically created in new accounts with a trust policy scoped to the specific repository and branch.

- **Technologies and Tools Mentioned**:
    - **AWS Services**: Organizations, Identity Center, IAM, S3, EBS, CloudTrail, GuardDuty, VPC (with RAM), Route 53, AWS Backup, ECS, EventBridge, SNS.
    - **Third-Party Tools**: Terraform, GitLab, Entra ID (formerly Azure AD), Tailscale, Prowler.
    - **Security Policies**: Service Control Policies (SCPs) are used extensively to enforce guardrails (e.g., block root usage, restrict expensive services, prevent leaving the organization). Resource Control Policies (RCPs) are used sparingly, primarily for OIDC provider restrictions.

- **Specific Methodologies**:
    - **Root Account Security**: Passwords are discarded upon creation, and an SCP blocks all root user actions. Access requires an AWS support request to reset the password.
    - **KMS Strategy**: They deliberately use AWS-managed keys instead of customer-managed keys (CMKs) to reduce operational overhead, arguing that IAM is a more effective and primary control against data breaches.
    - **Monitoring Strategy**: A "monitoring on a budget" approach that combines a Managed Detection and Response (MDR) provider, internal consultants for threat hunting, "rage-based monitoring" for non-critical internal tools (i.e., waiting for users to complain), and automated alerts for critical failures like backups.

## Full Transcript

All right, everyone. Welcome back to our next talk of the day. Uh before we begin, as always, we'd like to thank all of our sponsors who make Forward Cloud Stack possible. And particular uh in particular, we'd like to thank our bronze sponsor, Clear Vector. So, please uh go out and give them a uh visit at their booth in the hallway and just let them know how much you appreciate their support in making all of Ford Cloud possible. Uh with that, I'd like to introduce our next talk. This wasn't in the job description. Building a productionready AWS environment from scratch with Nick Jones and Moit Gupta. Unfortunately, Moit wasn't able to be here in person, but will be joining us remotely. Uh so, please welcome uh Nick and Mo to the conference. Thank you. So, uh as Joel says, this is uh yeah, this wasn't in the job description. And the reason we call it that is because uh Moit and I are security consultants by trade uh mostly penetration testing red team and these sorts of things. And so yeah when someone says hey can you build some new AWS environments uh was a bit of a shock. So what where do we come from? What's the backstory here? Um some of you might have remembered from forward clouds 2020 when I presented as F-Secure. We then had a rebrand. We became with Secure, then Withsecure Consulting, and eventually our parent company over in Finland decided they'd had enough of us entirely and so sold us, uh, out to a Swedish private equity firm. So, uh, away goes with Secure Consulting. Um, and in comes Reverse, same name, new game in theory. Um, now alongside all of this comes what always comes with a divestment or a merger or any of these kinds of things. lots of activity to either split off in our case or or set up um a entirely new organization. Obviously all the finance and and HR and all that stuff happens but from our perspective it was the IT that was the challenge and as ever with these things um you think you're going to get some support um and there's never enough money, there's never enough time. So now most people would think okay cool a solid infrastructure or cloud team can probably whip up something quite good in no time at all. was the issue here except we didn't have that. We had three IT people all of whom did not have the requisite experience in AWS. We also did not have a budget to bring in external consultants or anything like that. We did however have a cloud security team, the kind that help clients with their cloud security, not the kind that build our own cloud security. That included people like myself and Nick and both of us have had some experience building things in AWS and I have maintained and developed a bunch of our consulting infrastructure which we had even while we were with secure. We probably don't hold a candle to some of you in this room who do this as your day job but we think we can probably figure it out. So here are our challenges. We have Nick the guy standing on the stage right now. He does a worse things funnily enough. Do a funny person Nick. One thing to note about him, he hates IM users with such a passion that it's got to be personal somehow. No idea what they did to him, but there's got to be something. The voice you're all hearing is me. At least I hope you can hear me. Anyway, I'm Moit and as you may guess, I am not on the stage right now. I couldn't make it to the US, so now we have this weird hybrid talk. For some reason, people don't think I know what documentation is. I disagree. I just think code should be self-documenting enough and may or may not get lazy writing documentation. Or you could take that as I'm too busy doing other important work to write it. Whichever works more in my favor in your head. Both of us have been doing as cloud security for a while. Things like pentests, purple teams, design reviews, just talking to clients to help them figure out security related bits. So I think we at least somewhat know what we're talking about when it comes to AWS. So engineering an entire environment by pentesters. How hard could it be? We just do what we tell clients to do. There could be absolutely nothing that we need to consider that isn't either security related or that we've not come across not having maintained production environments. Right? Today we are only talking about the AWS side of things. There's a whole other side of setting up Entra and everything related to that which is own beast. We kind of just left that to Chris who is our resident enter your guy. He is also a pentester. So similar story there of how hard can it be but let's just leave him to it. He'll be fine. probably. So, what are we covering today? Now that we know where we're we're coming from and what the point of all this is. Um, first off, some background as to us as an organization and what makes us interesting when it comes to trying to build a functional AWS environment for us. Um, some details on the approach we took and why. Um, the organization structure, how we've hardened the organizations and our account strategies. um authorization and authentication strategy to go along with that. Um CI/CD, infrastructure automation, um all of those good things. Infrastructure is code, how we've approached that. Uh monitoring on a budget. Um I mean the summary there is monitoring what monitoring, but we'll get into that a bit later on. Um and then as ever with these things, you know, there's never enough time, there's never enough money. We've got a long list of things we'd like to do or that we haven't gotten around to yet. Um and so I'll cover off a few of the things that we're we're planning on doing still. So, first off, where where are we coming from? Um, Reverse X's an offensive security consultancy, which means that the vast majority of what we do is, yeah, penetration testing, offensive security engagements, assume a breach, see how far we can get, these kinds of things. Um, we've also got an advisory team um, who do some more conventional sort of advisory consultancy. Um, but we're about 160 consultants across uh, several countries in Europe, small teams in the US and Singapore as well. Um, what we're not a software company. We used to be attached to one. I'm glad we're not anymore. Um, we don't have a bunch of uh SLAs's embedded in contracts that we have to abide by by and large. Um, because we don't have uptime requirements and things because we're not running a software company. Um, and the other thing which will come up repeatedly through all of this is that we are not particularly wellunded. Most consultancies aren't. Um, but you know, we haven't got a um a big series A powering all of this or anything. Uh, you know, we've got some cash flow and and that's about what we got to work with. But what makes us not normal compared to a lot of organizations? Fundamentally, our workloads fit into three different buckets. Um, sorry for those who can't read at the back. Actually, that's smaller than I expected it to be. But, um, the first one is client delivery. So we have a requirement for consultants to be able to stand up systems in AWS on demand to host things like uh command and control infrastructure for malware or um fishing redirections or um even just systems to do port scanning with over the internet for for various types of engagements we do. Now that means that consultants need a lot of freedom to be able to spin up all kinds of systems as they need depending on the job requirements for what they're doing. But we also have a requirement there to make sure that the data itself that lives in those uh is is adequately secure because fundamentally we're gathering information on our client's security posture. Right? We don't really want that getting out. We also have a research segment um which is essentially just consultants doing all kinds of mad R&D. Um, a lot of what you see at forward cloud sec in terms of people presenting on awesome AWS research or um, new offensive tricks they found, a lot of our consultants like to spend their downtime when they're not working with clients doing exactly that kind of research. Um, you know, we've been up here presenting about it previously. Hopefully we'll be we'll be back again. Um, but it has to be an extremely lax sandbox otherwise the consultants don't have the freedom to be able to affect this research in the way that they want to. There are some things that we'll fundamentally never be able to give them in there. If you want to experiment with AWS organizations, then we'll just have to stand up a new organization. But we've done our best to make that as lax as possible to enable that. And then there's a third bucket which is our sort of production grade systems that are things like uh stores of client data that we've gathered on engagements. Um the tools that we use to produce the reports and and other data sets that we share with our clients. Um our source code hosting, CI/CD infrastructure, all of these kinds of things. Um and that has the same kind of hardening requirements that you'd expect of a production software house, right? Um where we have a lot of sensitive data, we need to ensure that we maintain the security of that in order to maintain the confidence of our clients so they keep buying from us. So um given that we've got those three rather diametrically opposed sets of um workloads and and the requirements that come with that, what what are we trying to build and and how? Um, so our core requirements like low maintenance overhead was probably the biggest for all of this. Um, as Mo hit says, we don't have a whole team of people behind us. Um, a lot of this stuff ends up falling on consultants in between their their workloads. We're trying to train our IT guys up. We'll get there. Um, but the core goal needs to be um, minimal overhead, right? So then alongside that, strong security enforcement around the critical components to a level that we can defend it to our client base. And because of the kinds of work we do, a lot of our clients themselves when they come and say, "Hey, how are you keeping uh keeping our data safe?" They'll bring some of their AWS experts. So, we've got to be able to defend anything we choose to do um to these organizations we work with. Um and ideally, it should be an enabler for the team. Most of our consultants are not AWS experts by any stretch of the imagination. And what they want to be able to do is stand up a box or two to to run whatever they need. um and they want to do that with as little friction and as little needing to get to know AWS as they can. Um so ideally it ought to be all simple and straightforward to work with. Um whether we've achieved that or not, I guess we'll find out in time. But but that's also pretty important. Um so the second that we said uh you know low maintenance overhead and needs to enable a bunch of consultants to do mad stuff. Um that immediately meant we threw control tower out the window. Um so we've had to make a lot of decisions around uh what to take from AWS. They provide a lot of really good stuff. Control tower unfortunately is just one of those things where it's so opinionated as to how it works that if you want to start tearing it apart and you know adding your own stuff in um you run into some limitations fairly quickly. So we borrowed some SCPs and things but by and large we we left that alone. So on to how we structured and hardened the the organization itself. Cheers Nick. All right let's look at the organized structure and how we approached that and then subsequently hardened it. So this is what our structure looks like from an OU perspective. Overall, it's relatively simple and helps us separate out all the possible workloads we may have within the environment. Under the root OU, we have five main U groups. Core is basically essential workloads that we need to operate the environment itself. So this would include things like the security account, logging, identity, networking, DNS, so on and so forth. Next, we have workloads. This is split into product and dev. I'll leave you to guess what that means. These in general encompass all of our internal and public facing applications that are hosted for the business. Research is exactly what it sounds like, an area for consultants to experiment with things with the utmost of freedoms. The client OU contains accounts we use for client engagements. These are the accounts we use to run EC2s for tests or for cross account role assumptions in the client accounts. Each account in this OU is for a single client. So if one account were to be somehow compromised, the blast radius would be quite minimal. Finally, we have a general service area OU which was added after the others. These accounts are higher level support for our services. For example, a B collaborator instance which is used by consultants across multiple clients. So it doesn't really fit into the client OU, but also it's testing infrastructure. So doesn't necessarily make it into our workloads OU either. Hence this OU was needed. This level of separation allows us to assign different policies to each of these OUS and have a decent understanding of what data is within each the rough expected usages and data lifetimes. Now our account segregation I think is pretty good. We are quite liberal with creating new accounts for things and even our GitLab runners are in a separate account to GitLab itself. In this manner, we have pretty good barriers between our different workloads and account compromise is generally limited to the account itself, which by itself does not get that much. Managing all accounts is a slight pain and we're still working on ways to optimize it. At the moment, they're defined within our Terraform. We have modules to help automate most things when it comes to account creation and management with a terraform locals variable basically reduced down to the name of the account NOU and optionally a GitLab repository for CI/CD deployment permissions and the owner of the intro groupoups that manage access to the account. We have seen plenty of others use Lambda functions and API endpoints to allow for more dynamic creation of new accounts. However, we didn't think this was suitable for us as are a bunch of other resources that we want to be made alongside an account which we wanted to maintain the drift detection and resource management functionality that Terraform provides. For those curious, those extra resources include creating groups automatically in Entra, creating default entries into identity center for admin and readonly roles as well as some custom IM roles which I'll talk about a little bit later. Additionally, when it comes to stack sets, we have some primary stack sets that apply to all accounts, but we also have some bespoke ones that target specific accounts. Global stack sets include things like a Terraform state file bucket. In this way, all accounts automatically have an S3 bucket that can be used for state file storage. And this allows us to essentially manage the security controls around these buckets as they considered a sensitive resource. Also, this reduces the manual setup steps for accounts where they need to deploy a statefile bucket before they can run Terraform. This way, that's out of the way. Also, we automatically set up our GitLab as an OIDC provider in these accounts. Even if not used, having it set up allows people to quickly make roles using it as and when needed. Plus, we make a bunch of IM ROS. Most of these are management related, including roles for network management, security access, etc. There are other roles such as the backup and restore roles as well. However, as we're giving this talk, those are still being figured out. For CI/CD roles, we automatically create a read roll and a deploy admin role into the account if and only if the GitLab repository is configured during account creation. There are a bunch of parameters given for this, but essentially it will deploy a role with custom trust relationships that account for the repository, the branch, and potentially a few other things based on the parameters given. That's why this is one that is done as a bespoke stack set within the account creation terapform module just because of the variability that the stack tech configuration can have based on the parameters passed in. So then looking at service control policies, resource control policies, everything else we've got in the organization. Out of interest, how many people here have got resource control policies deployed? Oh, that's more hands than I was expecting actually. That's quite a few. Um, so the resource control policies are an interesting place to start here because what we found is immediately you look at all of the examples and everything that AWS started putting out when they started talking about resource control policies and a lot of it is just things that make a lot of sense in a complianceheavy environment or where you have a very large very complex um set of data heavy workloads where you need a strong data perimeter but a lot of them just don't really fit our use case and the end result is that several of the ones we looked at, so for instance, some of the ones around harding access to S3 buckets and things like this, uh would actually then break uh a lot of our consultant work because a lot of our consultants rely on being able to do cross organization access between uh ROS and S3 buckets in order to transfer data into and out of um the various client uh targets that we're we're going after. Um so we're still working out what we're doing with RCPs really. Um, we got one in place for uh trusted OIDC assumption. Um, that seemed like an obvious place to start and not one that was going to be um we were going to be playing with too much. Um, but by and large that's about as far as we've gotten. Um, so I'd love to hear thoughts on what people have done with them so far. Um, if you want to come grab me afterwards outside in the break. Um, from an SCP perspective, a lot of it is pretty standard. Um, thanks to Chris Ferris for his Prime Harvest set. We brought a lot from that. Um, and the production OU is pretty well hardened with most of what you'd expect. Um, the client and research one significantly less. So, the research ones essentially just got um, you know, stop it leaving the organization, no root usage, no new SSOs and block all of the things that Ian McKay thinks are super expensive. Um, because it turns out that yeah, as uh, as Nick Fett will tell you, uh, if you make the wrong API call, suddenly you end up with a $36,000 AWS bill. Um, and we don't want to put our consultants in a position where they do that accidentally and then a whole lo of internal drama, right? So, we've got to a block expensive stuff SCP in place as well. Um, backup policies we'll get into later on or rather rather moit will, but we we've got a few of those configured. And tagging policy is a really interesting one for us because again, a lot of the things that most people use tagging for don't have a huge amount of value to us. Um even where we've got cost center tagging but for instance our cost centers are so um not granular basically it's you know it's either consulting or it's not essentially at this point um that there's very little value in that for us. It's useful to know uh when something was created for a lot of the consulting workloads um because consultants are really really terrible at going around and turning off stuff once they're done with it turns out. Um so I'll get on to some of the plans we got for that later on. Um but again a lot of the standard tagging policies don't really fit. So, we're still experimenting to actually work out what's of value to us there. Root account handling is an interesting one, and this is something that um I've ended up having debates over beers with with quite a few different people over time. Um we do not have MFA on the root account in any of our uh accounts in the organization. And the reason for that is that we create the accounts through the organization and then immediately throw away the password. So the only way to get into the account is to open up an AWS support request or to to get access to the root credentials rather you have to open up an AWS support request, get the password reset. Um now we also have an SCP in place to deny root usage. Um and the reason for this is that there are very few things that you really need root for now and the majority of them are things that we don't anticipate running into given our workloads and what we do. So at that point, just making it so that you just don't have any access to root is the easiest thing for us to do really. Um likewise, assume root we're still kind of looking at. Um but we haven't seen a a use for it. So at the moment, assume route is just blocked again by SCP across the organization. Now, from a data security standpoint, obviously this is this is where it gets quite interesting because we do host quite a lot of quite sensitive data. You know, your average large bank is going to be pretty unhappy if we start leaking all their vulnerability data onto the internet. So our primary control here really is the account segregation. As Mo hit mentioned, we run per client accounts so that any individual breach is relatively limited in in the damage that it'll do. Um and that gets us an awful lot we found in terms of um limiting the potential damage of any any one breach. Um because the accounts for the most part seem to work as a pretty good hard barrier um in between um access across workloads. Now the the challenge there is that AWS keep inventing new ways to creatively punch holes in that account barrier. Um so we're having to keep a bit of an eye on on things as they change. Uh but so far that seems to be the most effective one we've we've found. Now on top of that we've got the SCPs and RCPs as I mentioned. Um we've done all the turn the default encryption on for S3 and EBS got public access blocks on most accounts for for S3 buckets. Um, we haven't done that for the research accounts purely because there shouldn't be anything sensitive in there and sometimes you want a public S3 bucket. Um, so we'll we'll see what happens there. We're keeping a bit of an eye on that, but that's not there for now. Um, and then lastly, from an encryption at rest and encryption in transit perspective, encryption in transit, we essentially rely on AWS having sensible uh configurations on their API endpoints. Um, in theory, you can open an HTTP connection to S3 if you really want to. Um, but it's not really something that um is a worry inside AWS itself. Um, those sort of downgrade attacks aren't something you're going to be able to run very easily inside AWS. Um, and we don't do a huge amount of data transfer using uh tools that don't have that HTTPS connection built into it. The AWS SDKs default to it and so on. So I've always seen that um you know enforce HTTPS on all your S3 buckets thing as a bit of a tick box for the auditors but not something that really brings you any extra value from a security perspective. So at the moment we haven't bothered rolling that control out. We might in future but for now we haven't seen the need and from an encryption at rest perspective um we're pretty strong subscribers to the Chris Farris school of thought on this which is that um KMS keys are largely a waste of time um which I'll get on to in a minute. Um, oh, sorry. There we go. Um, there's a lot of operational overhead in running custom KMS keys for everything. we find um I don't think I've ever seen a single breach or a single case where we've been attacking client environments where the best thing you could have done to present prevent what happened would have been to have custom KMS keys with proper key policies and all these things applied um in order to restrict the attacker from gaining access to that data. There's almost always in my experience a better thing you could have done with IM or some other set of controls that could have been in place that meant that KMS is sort of your last line of defense, but if it gets to the point where that really is what you're relying on, you're in trouble usually. Um because an attacker will target the workload, they'll breach the application and the application will have decrypt rights on the KMS keys it needs to access its data. So at that point, you know, you're then the attacker is then working with the IM RO that's got the access anyway for the most part. Um, so that level of control over your encryption at rest just isn't worth it to us given that we're trying to operate on as low an overhead as possible. Um, and by and large in my experience, it's really a defense against auditors. Auditors run around and love to say, "Oh, have you got all your KMS keys done right?" Um, and if you're in a heavy heavily regulated industry where that helps you, it's absolutely worth doing it just for keeping the auditors off your back. But from a sort of a threat defense perspective, we really don't see the point by and large. So next up is Moit on uh the networking. There are generally a couple of highle methods we have seen people use for cross account networking. One of those is through AWS RAM and sharing the subnets with the accounts encompassing the network. The other is usually using a transit gateway or something similar to establish connectivity between VPCs. We kind of have both. We started with a shared VPC model. This model is highly reliant on security groups being done properly to ensure proper network segregation which we have seen multiple clients fail in the past. However, the workloads within here are at least partially managed by us. So, we can at least try to enforce strong security group practices from the get-go when deploying resources. Our main corporate VPC is shared with accounts that need to deploy workloads into it. As an aside, we are very selective about which workloads this includes. Anyway, there are some Terraform automations and how this is done. Effectively, we assign each workload a number and Terraform automatically creates the subnets in both as we operate in for that workload. The number being the offset from the base side range assigned to that VPC. For each workload, we can also say what type of subnets we want. This ranges from public, private, and internal. And whatever subnet configuration we specify, Terraform will automatically make those subnets with bespoke routting tables. This way, we can also assign unique route to specific subnets on specific workloads if we so need to. On top of our default global routting, the subnets were automatically shared with the requisite accounts. And because RAM doesn't share tags, because of course it doesn't, Terraform assumes a networking management role in the target account to copy over all the tags. With it being a shared VPC, it is super easy for us to set up all the monitoring and security features we want centrally. For example, flow logs. And once that's done, we know we have visibility of all our important workloads over the network. We have tail scale as our VPN as well. So a tail scale subnet router in this VPC allows our users to access resources based on the combination of tail scale ACL and AWS security groups where both of these are required for a connection to be permitted. We have some other VPCs where there are isolated from our other VPCs but we deploy a tail scale router within them. Thereby we maintain access to that network from our corporate networks but there is no way it can talk back and access any of our corporate resources even if the security groups were completely misconfigured. This is kind of like the transit gateway model, but for us, we are mainly using it as a one-way connection and via TSC scale and not transit gateways. These two models mainly cover our core and workload OUS. For all other OUS, they're predominantly isolated VPCs that are in no way connected to our corporate networks as these are all going to be either research or testing networks. For those, we definitely don't want them connected to our corporate network due to the nature of what occurs in these networks. We can still enforce access from our networks to these networks. And we do that by sharing a prefix list with all of our public IPs to every account in our organization through RAP. So for example, people can still easily restrict SSH access to our public IPs for their scanning EC2s. DNS is one of those I had to rearchitect and rebuild like four times before getting something we were somewhat okay with. Older methods would work great for a bit before hitting a snag that broke the entire model. for example, size limits on impulses or the inability to set CNAMES from DNS apex records. I don't think I have the time to go over all the iterations in this talk, so feel free to message me on Slack if you're interested. The final method had a central DNS account where we managed all of our domains. In Terraform, we defined all these zones and then all the records we wanted to have for those zones. There were cases where we kind of wanted to delegate control over certain records to other accounts. For example, the A and quad A records for the git subdomain would need to be set by the GitLab account. In these cases, we set it up so that via config, we can specify an account ID and which subdomains to delegate and appropriate imrolls are automatically made that allow that account to assume in and grant either readr or read access to those particular records. This way we can centrally set static records like DKIM, demark etc. But for other things like ACMD DNS validation or general a cord a records that might change those can be managed by the relevant accounts minimizing the friction in things working. Of course we still need to consider security and luckily for us there are relevant IM conditions that let us constrain what is permitted with these roles. Chiefly we are using these two IM conditions to restrict exactly which subdomains and what record types we allow to be set which we set by default to a quad A and C name. This way we can delegate records for a subdomain and not worry that someone sets the NS record and gains access to an entire subzone of the domain. The Terraform we have automatically adds the ACM prefix required for validation. However, we didn't reverse how AWS calculates this prefix. More so, we wanted to use regex to match something that looks like ACM. For those of you who don't know, it's a subdomain of the FQDNU on the certificate of the subdomain being an underscore followed by 32 random hexadimal characters. Now, I did notice this tends to be the same for an account. So there's probably some way it is calculated, but for our use case, we just had an underscore followed by 32 question marks because of course if doesn't support full reg x in this case a question mark matched a single character. So this matched the relevant ACM subdomain. Now this does add a lot of characters to the policy that plus the fact these conditions are quite long can easily lead to IM policies being too long and hitting the limit. In our initial version of this, I had a policy per overarching domain. This added up and the delegations for all of our park domains for our redirector account ended up taking zo for the limit quite easily. So now it's one policy with all the conditions and zones in a single statement. However, I think it's fine as I don't think you can create a record for a domain under a completely different zone. Moving on to authentication. I don't think anyone in this room would be surprised. We used identity center and federated authentication. We decided to federate with Entra as that is our primary identity provider within the business. So of course we go to our friendly neighborhood Chris to get everything set up there. The general approach we have is that all accounts aside from the root account is accessible through AWS SSO. I think a lot of people follow this method in general, but we create groups in Entra and these groups are mapped to a single role in a single account on the AWS side. This way is very easy to grant access to an account based on preset roles. And for the standard readonly and admin roles, we have those automated through Terraform. Effectively, when an account is made, it also autocrates the relevant entry groups. Chris did some magical things to make himself happy on the security of our service principle having the ability to make groups which feel free to chat to him to know more about that if you're curious. Although this did mean that there are 40 minutes between the first part of the terraform running which makes the groups and the second part which associates those groups in identity center while we wait for the groups to sync with AWS. We've been meaning to figure out a way we can force the syncs through sooner but we haven't got that yet. Those are just the default roles of course we can always configure extra ones as needed on a per account basis but because of our heavy account splitting there's less of a need of constrained access to a small part of the account as an account generally has that one thing in it and so access is to the one thing and there's not really much else. Also we ended up making sure to separate who can authenticate to what and with which account. Access to core or workload accounts require cloud admin accounts which Chris configured with stricter caps compared to people's regular accounts used for research or testing accounts for the management account. We did not want Entro to have any access to this. SSO was meant to be used for regular access and I think we can all agree that in general the less the management account does the better especially day-to-day things. So why would we want to connect Entra to that? We also have had enough experience of compromising entire AWS estates through Entra or some other federation that we wanted to ensure some level of separation. Now we do understand that compromising Entra would still get most critical business applications within AWS anyway. However, we still felt it was worth not linking the management account in the same manner. There can be some security benefit to them. For example, we haven't delegated down SCP management as those are very infrequently changed, which yes, I know defeats the do nothing and management philosophy. But it does mean that a compromised event wouldn't let an attacker modify the SCPs to either remove the restrictions we have, killer access to everything in the org, or whatever else they were planning. So instead for the management account we used and close your ears Nick IM users there are a few of them just so we have break glass etc and if one person does get hit by a bus it's not game over and of course those are definitely being marked as alert galore if they do anything this is probably one of the few times that IM users still somewhat have a use we think if you look at the carveouts alers have for when to use IM users as shown on the slide this isn't exactly one of them however I do think it warrants considerations for environments Moving on to automations and CI/CD. As you have probably guessed and are constant reference to it, we do have a bunch of automations set up for this environment through CI/CD. In general, it does help keep everything tracked as to what is deployed and simplifies a bunch of steps. Considering Nick and I are only moonlighting as engineers, the more automations the better for our sanity. So to enable this, there were a bunch of things we wanted to make sure existed in GitLab. There's a group that basically mirrors the OU structure in AWS. This way, it's easy to find a repository for an account and when creating accounts, it's easy to know what repository to set as a CI/CD deployment repo. Due to the automatic IM roll creation we talked about earlier, these repos have the relevant permissions and are scoped through trust relationships in a manner to stop a variety of CI/CD related attacks. Hilariously, we did find an issue in that which had to be raised for fixing. We can't talk about that yet as it's still not patched, but keep an eye on our blog post. As we have a bunch of repositories all needing to do the same thing, we have generalized a bunch of the required files and configs. These can just be copy pasted into a repo to kickstart it. The plan is to eventually have a template repository that can be forked. However, the current method still works. It's just a tad more manual. We also plan to have a library of terraform modules to make people's lives easier. Not all of our consultants are AWS experts and even things like deploying a VPC with subnets, they don't have the time to figure out how to do that in terform. The more we have in a library that they can just drop in, the more likely they use IA and less likely to use click ops. On another note, earlier we mentioned that we did some things by the root account. CI/CD for the root account is completely separate to any other account. In fact, it has its own dedicated GitLab instance and runners. Basically, the more isolated we can make that from everything else, the better. One thing we learned from this though, you can't validate everything in an OIDC claim in trust relationships. This is a table from the docs that states which claims can be passed from a JWT and under which IM condition key. Notice the list is quite small. I originally thought and people I talked to about this thought the same that IM could check any claim in a JWT which turns out not to be the case. When we cross referenced this table with the claims in a GitLab or IDC token, we found that the only claim we could really use was the sub claim. This claim by default contains just a repulsory path and branch name which in most cases is enough but sometimes not. For example, we can't check if the pipeline is executed on a protected branch if we were to need that for some reason. Backups are definitely one of the later items we properly approached. Considering being up and running was heavily time constrained, we did implement a short-term backup solution in the interim. Basically, for accounts that held data we deemed critical enough, we hacked together an event bridge cron and some backup tasks, for example, snapshotting an EBS volume. However, this method definitely falls a foul of the 321 rule. For those of you who don't know, that's basically one of the def factor strategies, which is where you have three copies of the data on two different mediums and one copy of site. The approach we are in the middle of implementing as we give this talk uses AWS backup, which is AWS's services backups, which well named from AWS for once. So, got to give them props for that. When thinking about it, we wanted the ability to recover from a wide variety of incidents, including things like ransomware, an a going down, or even full account compromise. So, the thought for backups was we need them to be both cross region and cross account in a different organization. That way, the data is as far away as possible from the original data. When we were discussing this, we also considered what would happen if AWS itself went down or up and vanished or whatever. We considered having backups outside of AWS as our quote unquote off-site backup. However, the practicalities of this were a bit too much. We would need active interaction methods to extract the data for each of these services in AWS. For example, cloning an EBS volume through a running EC2 instance. We don't have the time to implement something like that ourselves at least and we haven't yet looked in third party solutions for that. But there may be something there. But also, if AWS does up and vanish, I think the world may have bigger problems. for example, how would we all watch Netflix? So, um, with a couple of minutes left, I'm going to whip through our monitoring strategy, um, and highlight a couple of things we'd like to do for the future. So, first up, frankly, monitoring, you know, what what's that? Again, this goes back to the low overhead concepts, right? We we have very few client-facing systems with real hard SLAs's. And where at all possible, we've done our best to offload that to uh SAS platforms rather than managing it ourselves. So our core uh client data sharing systems now are all based on SharePoint. So if that falls over, it's Microsoft's problem and they're monitoring it, you know, more so than we are. Um for a lot of our other systems, things like our reporting tools that consultants use to produce the reports we give to clients. Um ragebased monitoring is surprisingly effective. You pay attention on Slack. If it goes down, people get angry. Uh it tells you you got to jump on and fix it. um that that honestly works well enough with our consultant base that um I think that's what we'll stick with probably for a while. Um the key exception are backups. Um so we've hooked AWS backup up to to SNS um and that'll be sending Slack messages soon so that we can keep an eye on it that way. From a security monitoring perspective though detection and response obviously we do the usual you know cloud trails everywhere with an orwide trail. Guard duties also rolled out orwide delegated admin down to the security account. VPC flow logs on the um production VPCs but not on the testing and research ones. Um and the vast majority of the heavy lifting here is done by our managed detection and response provider. Um we are lucky that we have people who go up against professional socks and so on on a regular basis from an offensive perspective. Made it very easy for us to evaluate whether the partners we were engaging with were actually capable of doing the job. we found one that was um and I was really happy to see this space was a lot more mature than the last time I looked a few years ago. A lot of the providers are getting really quite good. Um we then also have our own consultants as a second line for that in that we have a lot of consultants who do detection engineering and uh work with our clients to improve their security monitoring and so they can threat hunt and all these other things on our on our data if we need them to or when they got downtime between client jobs. Um good way of dog fooding and practicing what we preach, right? Um, so then from a preventative security monitoring perspective, I don't know if Tony's in the room, but thank you for Prowler. Uh, Prowler's pretty fantastic. We have that set up in a, uh, ECS task, um, just churning out results to an S3 bucket, but we've really aggressively tuned that down now, so it's only reporting on issues we really care about. Um, there's also infrastructure as code scanning for the the org um, Terraform. Um, that's proven useful. Again, pretty basic, but it's quite a few things. So, um, the future, we'd love to get some proper CI/CD in place for the organization stuff. Um, much better automation on a lot of this. That's the big one. We're going to need to do some cost management. Um, there's probably going to be an EC2 murderbot that runs around and kills old instances at some point. Um, that's one of the the key things. Um, and a proper security data lake. Well outside of my my current understanding, but it's a it's an aspirational goal for us to be able to look at everything all in one place like that. Right. So um conclusions um actually creating a new AWS estate gets easier every year. I find every time we see see clients doing it now we're trying it ourselves more and more comes out both from the community and AWS to help with that. The right security controls go a lot further than lots of security controls and it really is important to prioritize based on actual threats not on shinies that get pushed out or compliance or these kinds of things if you want to be able to defend against real world breaches. And with that, I'm sure we missed a ton. You know, we're a bunch of pentesters trying to build some uh some real world infrastructure. Um, so feel free to come tell us afterwards. And equally, I think we've got some time for questions now, Joel. Yeah. Fantastic. All right. Uh, thank you very much, Tech. As always, we'll have questions in Slack, and if anyone here has questions, uh, please raise your hand and we'll try to get to as many of them as possible. Um so a couple just coming from the Slack um kind of revolve around insider threats and compliance. So like how are you protecting yourselves against pre-signed S3 URLs and for your client accounts for client engagements? Um are those compliance ready? How do you handle um client compliance asks for those? Sure. Okay. So we got Mo here on Zoom. Hello. Fantastic. Um do you want to take a crack at either of those or shall I? Uh go for it. Um sure. So for the the client specific accounts um they are deliberately designed to be as ephemeral as possible in order to minimize our compliance burden. So people stand up infrastructure that lasts for a week or two at a time and then we blow it away. And between that prowler um and um the the sort of the cloud trail logging and things we do that's been enough so far to keep our our clients happy that the the risk windows are short um and that we can operate like that. Um S3 presigned URLs I think Moit is still on the to-do list. Yeah, like it's still something we're considering in terms of what we're trying to protect. So mainly like presigned URLs help you know where is that data being downloaded? like you generate the pre-signed URLs within our environment and download them from outside. However, with we're thinking all right, if they've exfiltrated that those pre-signed URLs, they could have exfiltrated all other things as well. And you know, to create a presign URL, you need get objects. So, it goes from excfiltrating large amounts of data to smaller amounts of data, which is something. However, I I think we're still investigating exactly what we want to do to mitigate that. Uh, thank you. Are you are you using data trails at the moment? No, I don't think we are m are we? Nope. Um and again, some of that comes back to the the cost trade-off, right? Um we are constantly getting nudged by our upper management saying, "Are you sure you need to spend this much on AWS?" Um and so there's a certain amount of trade-offs that we have we have to make there. Um and data trails at the moment. Um we're we're keen to look at it for a couple of our key data stores um with a goal of um tuning it well enough that we can eat the cost trade-off but um for the moment that's not not been a priority versus the other things we've been working on. Thank you. So we mostly use Terraform for AWS organizations management and but found inevitably using cloud form in some you know bits and pieces. So I'm curious like what's the ratio for you how much of cloud form you still have to use in bits. So, I think Mo, correct me if I'm wrong, but I think it's just a single stack set at the moment, isn't it? Uh, there's one global stack set and then a stack set per account. So, that many uh everything else is terraform. Um, but as a in terms of ratios, it's probably 80% Terraform, something like that. Probably even higher than that. Um, like the things in cloud form through those stack sets are quite minimal. I think there's a total of like six resources or something per account. Um, we also had a question in the Slack about the usage of access into your org management account. So, is it only IM users or do you have them in addition to SSO? Um, and then what's the value of long-term creds in your org management account? Sure. Okay. So, I'm going to take this one cuz Mo knows how much I hate the fact that we've still got some IM users, but we haven't found anything better yet. Um, the reason that we do them is because so that's the only way into the or management account. There's no um SSO, there's no ROS, there's no nothing. It's just those IM users. Right now there's two of them, one for me, one for Moit. Um and the reason for that is that we frequently found that yeah, as Moit says, you often as an attacker are able to pivot into the identity management systems, get in through single sign on or identity center as I guess it's now called. Um and then you're in the management account and that's a a really bad time. And we've seen two or three of our clients work like this with IM users where that is the only way into the management account. And the end result is that you then actually have to go target one of the two people who holds that bypass their MFA all over again. It adds significant extra steps to the attack chain. And if you are not going to federate in to that account, every other way of doing uh access or providing access comes with some kind of long-term credential. You could do it with IM rolls anywhere, but then you've got certificates that you got to store somewhere instead of the um IM user access keys. And so long as the access keys are in a proper system key chain on your laptop and things like that, I I don't think it's any more of a risk than the the certificate route. And if you go the certificate route, you got to stand up PKI and a whole load of other stuff, right? So, um we actually think it works pretty well. All right. Uh I think we're out of time. If anyone has more questions, uh please feel free to drop them in Slack or find Nick afterwards. Uh, one more round of applause. Thank you, Nick and Mohead. Thank you

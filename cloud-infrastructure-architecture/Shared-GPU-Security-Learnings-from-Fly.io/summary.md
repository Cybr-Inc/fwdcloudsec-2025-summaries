# Shared-GPU Security Learnings from Fly.io

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=_W4NHw-4P8A)

- **Author**: Matt Braun
- **Talk Type**: Cloud Security / Infrastructure

## Summary

This talk details the journey of Fly.io to securely provide shared GPU access to its customers. Matt Braun explains the significant security risks associated with giving users direct access to hardware, such as DMA attacks and firmware persistence. He outlines why standard solutions like NVIDIA MIG and vGPU were unsuitable and describes the alternative path they took: using VFIO and IOMMU for direct PCIe passthrough, and the low-level security controls they had to implement to make it safe.

## Key Points

- Offering shared GPUs is a significant security challenge, functionally equivalent to giving users remote physical access to the host machine.
- Standard NVIDIA solutions like MIG (hardware virtualization) and vGPU (software virtualization) were not viable for Fly.io due to platform constraints and a prohibitive licensing model.
- Fly.io's solution was to "cheat" by not virtualizing at all, instead passing physical GPUs directly to guest VMs using VFIO (Virtual Function I/O).
- The core security boundary for this approach is the IOMMU (Input-Output Memory Management Unit), which prevents unauthorized Direct Memory Access (DMA) from the guest-controlled GPU to host memory.
- Additional risks arise from the PCIe architecture itself, such as peer-to-peer attacks where devices can communicate directly, bypassing the IOMMU. These must be mitigated using Access Control Services (ACS).
- Firmware on the GPU (VBIOS, InfoROM) and the GPU System Processor (GSP) present vectors for persistent attacks, as they can be modified by a malicious user.
- Constant monitoring of IOMMU groups, device memory regions (BARs), and PCIe configurations (ACS, ATS) is critical to maintaining security.

## Technical Details

- **Initial Problem**: Fly.io's standard hypervisor, **Firecracker**, does not support PCIe passthrough, which is necessary for GPU access.
- **Alternative Hypervisor**: They adopted **Cloud Hypervisor**, which shares components with Firecracker but has expanded functionality, including PCIe passthrough support.
- **Virtualization Technologies Considered**:
    - **NVIDIA MIG (Multi-Instance GPU)**: Hardware-level partitioning on the GPU itself. Incompatible because it required a Red Hat and VMware stack.
    - **NVIDIA vGPU**: Software-level partitioning at the host driver level. Unsuitable due to a licensing model that charged a fee every time a vGPU was provisioned, which was incompatible with Fly.io's ephemeral compute model.
- **Chosen Implementation**:
    - **VFIO (Virtual Function I/O)**: A Linux kernel framework used to pass entire PCIe devices (or virtual functions of a device) directly to a user-space process, in this case, the guest VM.
    - **IOMMU (Input-Output Memory Management Unit)**: The critical hardware component that enforces memory isolation. It remaps a device's virtual address space to the host's physical memory, ensuring the GPU passed to a guest can only perform DMA within its explicitly allowed memory regions. It operates on **IOMMU groups**.
- **PCIe Security Risks & Mitigations**:
    - **Peer-to-Peer Attacks**: PCIe switches can route traffic directly between devices, bypassing the root complex and IOMMU checks. This is a feature (e.g., NVIDIA's GPU Direct Storage) but a security risk in a multi-tenant environment.
    - **ACS (Access Control Services)**: A set of PCIe capabilities to control traffic routing and mitigate peer-to-peer risks. Key rules mentioned:
        - `Source Validation`: Prevents spoofing of requester IDs.
        - `Request Redirection`: Forces all requests up to the root complex for IOMMU validation.
        - `Egress Control`: Controls which downstream devices can receive forwarded traffic.
        - `Direct Translation Disabling`: Disables peer-to-peer communication entirely.
    - **ATS (Address Translation Services)**: A local cache on a device for virtual-to-physical address translations. It's a risk because a compromised device could potentially write arbitrary physical addresses into transaction packets. The recommendation is to turn it off.
    - **NVLink**: A physical interconnect between GPUs that bypasses the PCIe fabric entirely, creating another potential DMA channel that must be secured or disabled.
- **Firmware & Persistence Risks**:
    - **VBIOS**: Firmware blobs on the card, not all of which are signed. An attacker could modify these for persistence.
    - **InfoROM**: Writable area for metadata (logs, temps) that could also be abused for persistence.
    - **GSP (GPU System Processor)**: The microcontroller that boots the GPU. Its driver is loaded from the **guest VM**, creating a large attack surface (parsing vulnerabilities, downgrade attacks).
- **Monitoring & Tooling**:
    - **Command**: `lspci -kvn` is used to inspect device configurations, including IOMMU groups and BARs.
    - **BARs (Base Address Registers)**: These define the memory-mapped I/O regions for a device. Monitoring these is crucial to detect unauthorized access or configuration changes.
    - **eBPF**: Mentioned as a technology that can be used to monitor and protect BARs.
    - **Driver Shimming**: Intercepting calls to the VFIO PCI driver to add custom security checks.

## Full Transcript

Uh well, first of all, this is my first time at Forward CloudStech and this is a great conference. Uh I just wish the afterparty was today rather than yesterday, but so it goes. Uh just kind of a quick show of hands. How many people here are dealing with their own metal like running their own servers maybe hybrid? Okay, so this will be a bit niche for y'all, but uh thanks for sticking around. So this is the journey of what we at Fly.io IO did to securely provide GPUs uh to our customers. Uh which is a somewhat odd thing to worry about, but if you're doing if you are doing GPU e things yourself or whoever is watching uh or working close to the metal uh or just wondered what it's like behind the scenes of a cloud provider, this might be of interest. So, who am I? Uh Matt Braun. I am a securityish person at Fly. Uh, I wear many hats. Compliance, pen testing every now and then. Uh, legal. Uh, hit me up on Blue Sky, Mastadon, LinkedIn. Love questions. Uh, I have twin seven-year-olds, so I'm not super active online, but, uh, cuz I've got them eating up my time, but I love to hear from people. So, just sort of setting some context, what is Fly? You know, we're we're a developer focused public cloud. What does that mean? Uh, so yes, it's we're a cloud. you put in a credit card, you get compute, storage, networking. We're primarily focused more on the user experience, developer experience for rapid iteration of product. Um, but we're running our own hardware. So there's other people out there who, you know, they resell AWS, GCP, etc. Uh, we have the challenges associated with running a global fleet of our own servers across multiple data centers, multiple providers, multiple jurisdictions. Uh but we're also leveraging the work that AWS did building Firecracker which for those you know who are AWS centric uh that's what powers Lambda and Fargate. Shameless plug we're giving you 50 bucks just to play around with explore it check it out. Um and you have my contact info if you want more holler. Okay so that out of the way why is this a deal a big deal? Why is this interesting to anyone? So look back in time 2023 2024 you're hearing GPU this AI that not much different from today true but at the time we decided to give the people the market what we thought they wanted which was to actually have their own GPUs to work with now we thought they primarily wanted like big heavy hitter GPUs for training though it turned out that what they really wanted them for primarily was inference so smaller GPUs. Yes. But this is one of those lessons learned things that we had to actually deploy the solution to find out. Uh now even still, but with that what it really turned out people wanted, they didn't want hardware at all. They just wanted to be able to put a credit card in, get access to an LLM, get, you know, feed context in, get tokens back. Uh but again, that was the that was kind of the the uh down the line learning that we had to bump into. My boss wrote a great piece about this. It's called We Were Wrong About GPUs. Uh there's a link there. Check it out. He's an amazing writer. So, back to 2024. Uh just kind of some context. You can think of a GPU in a lot of ways that if you're not careful about it, it's functionally equivalent to remote physical access to your host. Uh especially the data center grade cards. These things are basically fullon secondary systems. Uh they have their own firmwares, their own uh nonvolatile storage. They can do DMA willy-nilly all over your host. And uh so if you remember the Thunder Strike family of attacks from a few years ago where you know people would walk up to a MacBook, plug in a Thunderbolt cable and it would just unlock. Same threat model just now delivered over the internet. So uh you can compromise the host obviously attack other users you know move through the network laterally. Uh I some call it evil made as a service and it violates a lot of the gu if you're not careful it'll violate a lot of the guarantee that you get from being in like u you know ISO 271 data center having person traps all that jazz. So we were scared we you know we wanted to offer this thing but it's terrifying. So, and then of course there's all I mentioned the nonvolatile storage. There's uh side channels and persistent risk all over the place. Good times. So, we with a devil may care laugh in the face of danger uh decided to soldier on. Uh but there was an immediate problem. Firecracker doesn't support PCIe pass through. Now, we could try to build it. I mean, you know, it's open source. we have the the ability if we really wanted to, but that's not a that's a hell of an engineering lift. Uh so we hunted around for other solutions and salvation came in the form of the kind of boringly named cloud hypervisor which shares a lot of the same rust VMM core components that came out of firecracker but expanded the functionality available. So we were able to deploy that and we had we got comfortable with some of the other you know the expanded surface area in you know in expanded attack surface as well as just the challenge of now we're running a dual hypervisor fleet that took some work but we got it done. Um but okay so now we figured out how to at least pass through or like connect a VM to uh the card but okay now what how you know they're physically connectedish but how are we going to virtualize them? So there's two kind of obvious solutions and neither of them worked for us. The first is MIG which is an Nvidia uh solution that actually does the virtualization and GPU card partitioning on the hardware itself. So, we kind of use the terms interchangeable, but when we talk about a GPU, like the physical card you put in the box, there's actually m at least for the data center grade ones, there's multiple discrete GPUs on that card, and they all have their own memory. And that card uh uh of multiple GPUs can do the logic of grouping one or more of those GPUs together, splitting them apart, and it's all enforced at the hardware level. That's great. uh but you have to be on Red Hat and VMware. So we're still you we've gotten somewhat down the way with cloud hypervisor but we're still losing out. Okay, next option vGPU which another Nvidia solution higher up now it's software it's at the host driver level that uh handles the partitioning and isolation and there's broader hypervisor support now but still no cloud hypervisor uh but worse still and I've never heard of this before every time a vGPU would be provisioned which would happen every time a customer it was loaded onto a GPU new host, they Nvidia would charge a fee. The driver phones home to the mother ship every time a GPU is spun up. So that might work for some in you know organizations. Maybe they have long running tasks or they don't change things around much. But for our ephemeral compute solution, it it just couldn't work. And we're bringing systems up and down in response to HTTP requests. It there was no way to make that work with our billing model. Okay, so what that are we boned? No, we cheated. We said, how about we just don't virtualize at all? Just take the those GPUs on the card and just pass them through to the VMs. So using VFIO. Uh so as I mentioned, a single V uh Nvidia card can have multiple GPUs on it with independent memory. Great. Package each of those up as a virtual function. expose the virtual function to the guest VM. Let the IO MMU handle the safety. Now, for those of you who are maybe not as low to the metal, uh we had to get comfortable with this cuz it's not something we normally dealt with much, but uh VFIO uh is a uh kernel framework that lets us pass PCIe devices like GPUs directly into user space safely. uh it does this by handling handing off control uh only after verifying that the device is isolated in its own MMU group uh and it relies on the IM it the VFIO framework does do some controls checks and so forth but the real security boundary is enforced by the MMU which prevents the unauthorized memory access. So the IV I kind of alluded to what that is. Uh it remaps device virtual memory or OOVVA to uh physical memory or system memory on the host. Uh without it a GPU that's passed through to the guest could just issue DMA anywhere on the host system uh RAM other devices. Uh with it DMA is contained to what we explicitly allow. Uh it's the core isolation layer for uh hardware pass through. uh when the IMU enforces memory protections, it works on a MMU group. So you can bundle up devices within a group and let them talk to each other or isolate one device per group. It really is your kind of your mileage may vary situation what you goals you want to achieve. But again, if more than one device is in the same group, all bets are off. Also relevant to this is MMIO which memory mapped IO and it's how the CPU or guest VM controls the device itself. So not just writing to the frame frame buffer device memory but triggering you know firmware reads saying it's time to start operating. Uh so instead of writing to special IO ports it just reads and writes to memory system memory like or uh virtual memory like any other. Uh and those are that memory is backed by device registers. So like say a guest could write to a memory range and say that range means start processing a job loaded from in the fame buffer and work on it. Now the ranges of where these are find are called bars or uh base address uh registers and they define these regions. And finally so the bars tell the operating system uh where the CPU's address space of devices registers or memory live. Uh a device may have more than one. This is not just like a single map. Um in fact you can have up to six and the reason you have multiple ones and this is relevant for the security of the device is you know one region might expose the control registers might be a small you know 4K region another might expose actually the frame buffer memory of the of the GPU uh then and they also have certain prefetch guarantees that you can apply uh and you can and and different policies around how can those areas can be written to and accessed. So you you can protect these bars uh using ebpf which is we're a very big ebpf shop. Uh or you can shim the uh VFIO PCI driver and actually intercept calls and check against the bars and be and say that's okay or that's not. Uh these are some monitoring solutions that we dug into for the proc or the purpose of securing these cards because while MIG and VGPUs can enforce a more of these security guarantees out of the box because we weren't able to use those we had to go to a lower level and put our protections closer to the metal. So just always double check what devices are in your IO MMU group and look at what's actually bound to the VFIO. You don't want surprises. Just to give you a sense of what it looks like, uh you can see here that this is on a host. This is one of my test hosts. Uh if you do lspci KVN, you'll get a a whole mess of text including you can see the IO MMU group 14 in this case as well as these bars are laid out. And you'll note one is 16 megs and then one's 256 and another one's 32. You can kind of guess that 16 is probably more config and control whereas the larger ones are more for actual uh data throughput. Uh and if you want to hunt around, you can see the IO MMU groups. There's a bunch listed, but 14 is the one we're in. And so if you look at the path, that kind of light blue text, that's the uh PCIe BDF and uh bus device function. And so you can just look and say what what is that device? And you see in this case it is a uh Nvidia Tesla on the VM side because remember we're not the VM thinks this is just a card being passed through it. So you see it it doesn't see an OMMU at all. It sees physical slot and those are those same three bars but unsurprising at memory areas that are different than what's seen on the host. So, okay, so VFI and OMU keep the pass through devices from DMA to each other. But if only that were enough to call it a day. So, we have to take PCIe into account. Uh, and if you're of a certain age, you may recall installing like a PCI scuzzy card in your host like I did. Uh, well, the PCI of of old and modern PCI are very very different. that PCIe hides a ton of complexity which we can go into that but really the most important thing is old school PCI was a flat bus it was shared whereas PCIe is hierarchical there are actual switches there are serial links pointtooint and switches that route traffic up from a device to the root complex which is basically the interface to host memory more or less um now I said it's like switches that's networky they actually use packets. So TCIE communication signaling control is handled with these transaction layer packets. So you know they are addressed using these OVAs or physical addresses. Uh they can be routed through up down side to side through the switches up to the root complex or device to device. And when they're routed up to that root complex those IOMU checks are enforced. But what if they're not? So this is the risk of PCIe peer-to-peer attacks because the switches allow Divi, you know, direct device communication. And this can be great. I mean, especially in high uh high performance computing scenarios, you you you want to have your GPU talking to your storage. That's Nvidia um GPU direct storage. That's that's a feature, but that's not great in the situation where you've got multiple uh u privilege levels on a host, multiple parties on the host. Uh so you can tweak this behavior using something called access control services or ACS. So ACS is not always available. Uh and the ACS interplay is kind of complex and strange. Uh but and your again your mileage will vary but just to kind of give you a shot of uh what you might want to look for is these four rules. Uh source valid. ensuring and the plus and minus means what's enabled and disabled. Uh source valid, it ensures that a device cannot spoof a request ID. Straightforward enough. Uh request reader, it forces all requests to go up to the root complex where the MU can can uh uh validate the the uh the rules. Uh egress control, it controls which downstream devices can receive forwarded traffic. So they can't be they can be kind of isolated and direct trans just disables peer-to-peer entirely. Now there's a there's some overlap here between say source uh um request reader and direct trans but belt and suspenders you know have them both. Uh but one thing to work watch out for is not all systems have this. So even even server grade hosts may not have ACS. So if this something you're concerned about or you're curious about you and you're like where is this? That's why there's also ATS which is another thing to be afraid of which is essentially a cache mapping a cache lookup and storing local on the device itself where uh the device can request a resolution of a physical of an virtual address to a physical address and use that physical address in future TLPS. But if a device can write an address, a physical address into a TLP header, what's to stop it from putting in an arbitrary address? It can make things faster. It can be useful. That's why it exists. Most folks won't need it. Turn it off if you can. Uh and then envy link. This is the uh the physical wireless. You gang multiple uh GPUs together. Uh it bypass even the PCIe safety net. So this is a third consideration. Now, you'd think that this is something that you would know if you put installed it or not. Funny enough, at least one of our hosts, our remote hands had installed the cable in place. We didn't ask them to do it. We're not mad about it. They just saw it, you know, it's part of the hardware that was shipped and and if attacker had gotten to a sufficient level of privilege, they could have enabled the NVLink and then used it to DMA through that other card. Uh, also with a minute left, VB BIOS is a big blob, a big pile of firmware blobs on the card. Some are and they're loaded when the card comes up. Not all of them are signed, but some are. And the info ROM is metadata for the card. You know, logging, uh, temperature data. Either one of those can be modified. If an attacker can modify those, that's risk of persistence. Uh, you can dump the VB BIOS and verify the signatures, but that assumes that the card is actually returning the one that's actually running. But that's at least something that can give you some confidence in what BIOS has been tampered or not. Uh then there's the GSP which is what handles the boot logic of the GPU on the card. And what's wild is the driver for the for this is loaded not from the host but it's from the guest. So the user controlled guests are are passing the driver down. Now this driver is signed but it's e it's parsed. It's an ELF file. There's parsing volume. there's you know there's it downgrade challenges um it's scary so uh our consult bank breakout they just showed that certain parts of the GPU stack are scary uh and uh helped us understand how to maintain uh ACS and ATS and they recommended continue to fuzzing and let's see yeah scan it monitor your bars and concern external route of trust and fuzz the driver and so now what they're uh Oh, shout out to our consultants. A trade is in tetro. Worth every penny. They're they're great. Uh, do you need GPUs? Maybe not. Uh, and if so, try to get dedicated hardware. And thanks. I see we've got questions now. Do we have uh we didn't get any questions in the room. Do we have any questions in this room or the thread? Yeah. Um, so going back to your solution for assigning individual GPUs to incoming requests, uh, what considerations were made regarding load balancing um, or resource allocation to prevent overburdening these GPUs? Sure. So, because we're not we're not truly virtualizing GPU, what you get is what you get. Uh, we would say that we could get, you know, an A um, an A10, an L40S. Um, but they had the the the use of that card and that was it. So if they overstressed it, upgrade. All right. I should have positioned myself a little bit better. I guess uh similar question but in the reverse. Uh since the GPU only has a certain number of cards and you're renting uh like those GPUs, did you have any problems with bin packing? Like uh I'd imagine it became like a lot less efficient because you does that make sense? Yeah. Yeah, it does. So you we ran into just very strange challenges around where we would put the cards in what regions that bumped up against like customs issues and tariffs, all that jazz. uh we slowed we we really slowed down the deployment of the cards for uh to uh trusted customers so we could kind of get a sense of where we needed them. So we never really ran up against like a sudden surge of usage in a region where we we were worried about it. Uh so by slow rolling we were able to be more efficient about deployment and use. So, as a customer, um, if you're using Fly or somebody else's GPUs, are there ways to get asurances? Do you publish like these are the secure configurations that we use? And do the other providers give that level of of visibility? Yeah, I I kind of had a speedrun through past part of that. Um, the hyperscalers, they build their own stuff. They really have it tied in good. Uh, we're at that kind of weird middle point. I would say if you know if you can ask them they'll usually tell you um what they're I mean if they're if they're using AMD or Intel that might be a place that I'd kind of look a skew or a scans at them because there's been more of a history of volence there. Um the way to mitigate that is just say can I get dedicated hardware? Um cuz sometimes they say here you go and if it works pricing wise. Um but yeah I don't think they publish necessarily how they configure their host. a lot more transparent about this stuff than most, but you probably ask if you spend enough with them. All right, thank you very much. And again, we'd like to thank Hannibite, our sponsor. Please don't forget to scan those QR codes for your session feedback.

# Securing organizations ML & LLMops deployments : A platform architects journey onboarding LLM &...

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=oxTsbP-iv7Q)

- **Author**: Sai Guranjan and Kyler Middleton
- **Talk Type**: Cloud Security Architecture

## Summary

This talk explores the platform engineering principles essential for securely deploying AI, Machine Learning, and LLM applications within enterprise environments. The speakers, a developer and a cloud architect, share their real-world journey of implementing compliant, secure, and cost-effective AI solutions on both AWS and Azure. They emphasize that default cloud service configurations are not secure and that engineering teams must proactively implement security controls, manage data access, and "shift left" to integrate security from the start.

## Key Points

- **AI is Inevitable**: AI is being rapidly adopted in all organizations, making it crucial to understand and secure the data it accesses.
- **Defaults Are Not Secure**: Cloud providers are shipping AI services quickly, and the default settings prioritize cost and ease of use over security. Teams must manually configure security features.
- **Shift Left Security**: Security needs to be integrated into the AI development lifecycle from the beginning to prevent insecure implementations, just like in DevOps and DevSecOps.
- **Identity is Critical**: Understanding and securing identity and authentication, particularly with protocols like OAuth2, is one of the most critical and weakest points in AI security. Bots can inherit user permissions, creating significant risk if not managed.
- **AI Engineering as Data Engineering**: A significant part of AI engineering is managing data: knowing where it is, how it's protected, how it moves, and who can access it.
- **AWS Bedrock Gaps**: While powerful, AWS Bedrock has security gaps, such as a lack of resource-level policies to protect knowledge bases or control model usage, and guardrails are optional rather than enforceable.
- **Azure Policy for Governance**: Azure provides robust governance through Azure Policy, allowing organizations to block insecure deployments, untrusted models, and enforce network security configurations like private endpoints.

## Technical Details

### AWS Architecture & Services

- **Core Architecture**:
    - An initial **Lambda receiver** immediately responds to webhooks (HTTP 200) to avoid timeouts.
    - A second **Lambda worker** is invoked asynchronously to handle the complex, multi-step AI processing.
- **Processing Flow**:
    1.  **Knowledge Base Query**: The worker queries a **Bedrock Knowledge Base** (backed by OpenSearch or Aurora with PG Vector) to retrieve relevant data chunks (vectors) based on the user's request.
    2.  **Reranking**: A **Bedrock Reranker** model is used to analyze the retrieved chunks and the original query to select the most relevant ones, dramatically improving response fidelity and reducing costs.
    3.  **Foundation Model Call**: The refined context (user prompt + reranked vectors) is sent to a **Bedrock Foundational Model** (e.g., Claude Sonnet) to generate the final response.
    4.  **Guardrails**: **Bedrock Guardrails** police the input and output tokens to filter harmful content and block denied topics (e.g., insider trading).
- **Key AWS Tools**:
    - **Converse API**: A meta-API that allows applications to use a single API format to interact with various foundational models and supports document types (PDF, DOCX) that models don't natively handle via Bedrock.
    - **Monitoring**:
        - **CloudTrail**: Logs all API calls for auditing.
        - **CloudWatch**: Records metrics and can log entire conversations for analysis (though all conversations for a region/account go to a single log group).
- **Mentioned Security Gaps in AWS**:
    - **No Resource Policies**: Bedrock resources (like Knowledge Bases and Models) do not support resource-based policies, making it difficult to restrict access directly.
    - **Optional Guardrails**: Developers can choose not to specify a guardrail in their API call, bypassing the control.
    - **No AWS Config Rules**: AWS Config cannot be used to monitor the configuration of Bedrock resources.
    - **Limited Web Crawler**: The data source crawler for knowledge bases is described as "beta" and lacks support for authenticated websites.

### Azure Architecture & Services

- **Core Architecture**:
    - **Network Security**: Emphasizes creating a **Microsoft Managed Network** with private endpoints for all services to ensure no public exposure.
    - **Private Connections**: Use of **Private Endpoints** and **Private Link** to securely connect Azure ML/AI Hub to data sources (Storage Accounts, Key Vaults, Databases, Snowflake) across tenants and subscriptions.
    - **Egress Control**: An **Azure Firewall** is used to govern all outbound traffic from the AI models.
    - **Ingress Protection**: A **Front Door WAF** is used to protect the application's API endpoints.
- **Key Azure Tools**:
    - **Azure AI Hub / ML Workspace**: The central service for deploying and managing models.
    - **Microsoft Entra ID**: Used for robust authentication and authorization for both users (data scientists) and service identities (applications).
    - **Azure Key Vault**: Used to store all secrets, keys, and certificates.
    - **Azure Monitor**: Provides a robust logging framework via diagnostic settings and activity logs.
- **Governance and Security**:
    - **Azure Policy**: Heavily used to enforce security. Policies can outright block the deployment of insecure resources, prevent the use of untrusted models from the marketplace, and enforce network rules.
    - **Content Safety**: The AI workspace has built-in **Content Filters** and supports **custom blocklists** to control what the bot is allowed to discuss and respond with.
    - **Deployment Methods**: When deploying models from the marketplace, it's critical to select the correct deployment option to prevent data from being processed in another geographic location.

## Full Transcript

So, welcome everybody. I'd like to take a moment to thank our sponsors, but especially our silver sponsor, Sonorai. And if you could take a moment to fill out our um conference survey, you'll see uh throughout the hallways some QR codes prominently displayed. If you could just take a moment to scan those and fill those out, conference organizers would really appreciate it. But without further ado, we have Sai Guranjan and Kyler Middleton on securing organizations ML and LLM ops deployments. A platform architect's journey onboarding LLM and L ML ops tools and securing multicloud data access. Take it away. Thank you. That was beautifully done. Hey everyone, I've got my Britney Spears mic. I can hear that it's working. Hey everyone, we are talking about securing organizations machine learning and LLM ops deployments, a platform architect's journey. Really, all of that means we are going to be talking about the platform engineering precepts that underpin modern AI deployments. We're going to be talking about AWS and Azure real life deployments that are compliant, secure, and cost-effective. And it's going to be fun. I hope. All right, AI is coming. Let's see if I can hit this. This worked when we practiced. That's the intro to the Law and Order theme. AI is coming. Q ominous music. Okay, let's turn that off. Cool. So, we're going to talk about it because it's coming. It's coming to all of your networks. Um, for instance, how many folks have implemented AI systems internal to your company? Do you have AI things that you're using today? I see maybe half the room, maybe a little bit more. What about into your application? Customerf facing, client facing, maybe a third. It's coming. Absolutely. Yep. Um, all right. Well, let's talk about us because this stuff is coming. Um, I am the principal developer, uh, internal AI solutions. That's a madeup title. I'm something software engineering for real. And I like to create problems. I'm I'm one of the AI developers within Virodine, which is in the healthcare space in the United States. And this is I'm Sai. I'm I'm the lead architect on the cloud platform team in Van. Likes to solve problems securely. So, generally, I'm kind of creating things that are maybe insecure, but work. and then site helps us make sure they work and they're secure. Um, where is your data and who can access it? Uh, a lot of AI engineering is answering this question. Uh, where's your data stored? It's probably across multiple clouds. Uh, if you're an enterprise, uh, it's probably stored within both like SAS native tools as well as maybe data warehouses, uh, thirdparty ones like Snowflake. And is your data classified? uh are you able to find out who has access to it? Because you need to do that. Your bots want access to all of it. And bots act as a funnel for data. So anything that a bot can access that a user can access that bot, that user can now transitively access that data. So you need to be aware of that. Be aware of what data you're feeding to your bots because they are going to give it to anyone that asks. They're very trusting employees. Uh what's happening glo globally. So let's talk about just what brings us to this point, the context that brings us all here. So first of all, data is propagating like we just talked about. It's everywhere and we need all of it and your bots want to access all of it. And a lot of it is sensitive in a way that is not particularly compliant and compatible with generative AI. Um if you feed in anything sensitive and give it a stern system prompt that says don't tell anyone, it you know probably won't but is probably good enough for your organization and your data classification. That's up to you to decide. AI tooling is coming. So, Agentic bots are now using MCP, potentially like remote MCP. Maybe they're using OOTH. Probably they're passing it through directly to use things and do stuff. There's also agents triggering agents. The A2A protocol is now under the Linux Foundation and we're all going to be implementing it in the next year probably. Um, machine identities in the bottom left, the the green bots are now acting like users on your network. They're accessing your APIs just like your users do. They're accessing your data. They're processing it. They're potentially making changes. We'll see how much that that perks up in the next year. And they are users that do anything that anyone asks them to do, which is probably a little bit problematic for most most data classifications and most APIs. And a lot of your developers are using AI to generate code. Do you care? Do you want to classify that code separately? Do you want to test it more thoroughly? I don't know. We're going to talk about that. Is AI engineering just data engineering in a funny hat? Yeah, pretty much. Um, you want to know where your data is. You want to know how it's protected and how it should be protected. You want to understand where it needs to move to. And you also need a strong maybe DevOps need because you need to understand automation. How often is your data refreshed? How are you moving it across the network? How are you protecting it at rest? Um, you also need a strong identity and authentication engineering component. In my mind, this is the weakest part for most of us, right? Like, we know how to write Python and Bash and PowerShell, but do you understand what Ooth 2 claims are and how they're secured and how they're validated? And and I certainly haven't until the past month or so, and I'm certainly not an expert yet. But if there's one area where you want to know like should I focus my time on anything in this talk, it's that because we need to know that. We need to understand how to make sure bots can prove who they are. So, that's coming. What are we talking about with AI? AI means a lot of different things. In the same way that DevOps means nothing and everything, AI means nothing and everything. We've been doing machine learning for like 30 years. AI is not something that launched when chat GPT went live. This is something that we're sort of making better and we're implementing and improving over time. and is AI when like co-pilot or Intelligj autocompletes a line of code maybe I I think of AI more as this agentic or this generative AI summarization there's chat bots on websites there's AI like as soon as something gets launched into Adobe Reader like it's just in your network there's no way to keep out Adobe Reader in an enterprise it's just coming for you and uh Adobe Reader now has generative AI to summarize PDFs So, is that something you need to secure and worry about? Is that something that like is going to take advantage of MCP on your network in the future? God, I hope not, but maybe. Uh, so it's something we just kind of need to keep a pulse on to see what is coming and what we need to worry about. I don't actually know what AI is as much as I talked just now. So, I asked ChachiPT and it says AI is the simulation of human intelligence, my machines, da da da da. And even the robot itself did not claim to actually be intelligent. It's simulating intelligence. It's predicting tokens and it'll make some really poor choices. One of the things that we are going to need to do collectively in this room is educate others around us that don't understand this as well. AI is going to lie to you and it's going to lie to you very convincingly. It is an excellent liar and it will continue to make excuses. If anyone's worked with Chad GPT or programmatic assistance, it says, "Oh, I see the issue." And it changes something and it still doesn't work and it says, "Oh, I see the issue." And you loop that way for half an hour until you realize it doesn't know what it's saying. It just really wants to say it. It really from its heart wants you to believe that it knows what it's doing. I asked it to make a picture to make us all feel better about that terrible realization and it put a brain in a metal metal body that's smiling at us which is just deeply horrifying to me. So, thank you Chad GPT. Um, MCP has matured really rapidly. I want to like applaud the MCP team and it's just an open source project that's coming. It permits uh agentic bots to interact programmatically with your tooling and your APIs and stuff like that. And what it used to do is the confused deputy problem. Let's hands up. Has anyone heard of the confused deputy problem? Totally. We It's in a lot of our security textbooks and programming textbooks. It's pretty hard to define exactly. One of the examples that we use a lot is like a Terraform pipeline in GitHub. It has a lot of permissions because it needs to crud stuff. It needs to create, read, update, delete. And if you are able to use, you're able to leverage that IM role to do other stuff like exfiltrating all of your secrets, that's a confused deputy problem. You have a bot or an automation that has a permission set and you're leveraging it in a bad way to do other stuff. So how MCP used to work is you gave it a permission and everyone that talked to it got that permission and that's, you know, terrifying. That's a terrible design, but it was a nent technology. It is evolving and it's learning to pass forward tokens. So, we're using OOTH2 tokens. When you when a bot or you as a user interacts with this uh MCP thing, it proxies your connection, but it also prompts you for a token and it says prove who you are. Prove your permissions. And that's great, right? Because you won't be able to get something that you couldn't otherwise. If you can't access the data yourself and you give someone a token that says I'm exactly the same person, the same Kyler, they won't be able to access anything more than you. That's great. Uh, it's also probably terrible. Like we're going to see some failures because I have administrative rights to a lot of systems. And if I interface with a bot and it says, "Give me a token that proves I'm you and I get all your rights." Well, now this bot has administrative rights and that's probably a bad idea. So probably as we build these MCP remote servers, we need to implement a bunch of functionality. We need to validate that these tokens are real and came from our network and prove that folks are internal. We need to prove what rights they actually have to our data or our APIs. And we also need to prove that they uh don't have all the same rights as the admins that triggered them because then they would be able to do terrible things. So probably we're pruning rights on the way through. I'm not sure if that's MCP or that's OOTH token issuance. That's something we're all going to collectively need to get better at over the next year. AI integration is coming. Are you ready? So, uh, even if you haven't touched it yet, I think about half of folks were saying we've implemented AI things internal to our company. I think every single hand in the room is going to be up over the next year. AI is amazing at certain things. At summarizing data, it's fantastic at uh, making up stories, at helping you write new ticket prompts based on nothing, like copy this structure, do this thing. It's amazing. It's incredible. So, we're going to see it in lots of use cases. for where we try to misuse it is where things are going to get messy and we're going to need to clean it up. It's terrible at doing uh structured things. So, if you have it generate JSON, it's going to fail all the time because it forgets about stuff. You know, it's a junior that forgets to put a comma and it's wrong and it breaks stuff. So, like don't have it generating structured outputs because it's terrible at that. It's really good at pirate ballots. It's really bad at JSON. So, like something to keep in mind. uh average platform engineering team day if you are not shifted left to when your folks are developing AI applications is talking to them and saying oh you added a new AI feature great turn it off please don't implement it insecurely so just like with other secure stuff we need to shift left right we got to shift left because if we're implementing AI without access controls or we're ingesting the entire payroll system including everyone's pay into the bot it's going to go wrong it's going to go really really wrong so make sure that you're integrated as far left as you can just in the same way that DevOps and uh DevSec has, you know, mapped out for many years. Let's talk about AWS. Uh AWS is a littleknown cloud provider. Raise your hand. I'm just kidding. I know you all know what AWS is. Um AWS has uh Bedrock. This I cribbed a bunch of these slides from a reinforced talk from a couple weeks ago. Uh bedrock is scopes three and four. So, it's using pre-trained models and fine-tuned models, and you're not generating your own model using Bedrock. You're just generally able to like use AI, including in a serverless fashion where you just pay for tokens. That's how you're probably going to be experimenting. Uh, that's how we have built stuff primarily. There's also SageMaker. I haven't used this yet, but it's for like pre-training models, but also generating your own models yourself and running them. Uh, when I priced this out six months ago, running your own model that you trained yourself was around $30,000 a month. and that's beyond my PCAR budget, so I haven't gotten a chance to play with it quite yet. Um AWS Bedrock, you have serverless models. Those are foundational models. That's an AWS terminology. That means it's third party models and also firstparty ones from AWS that you're able to call and just pay for tokens. Tokens are really cheap. A million tokens for three bucks or something like that. So you're able to just go and play with it without paying a lot of money, which is pretty fantastic. Um there's also knowledge basis that was in the last talk that was an excellent talk uh which is ingesting unstructured data using rag methodology read and generate which is using like probably a foundational model that understands structured data like charts and uh word documents and diagrams and stuff like that and converting it to text and then using an embedding model to put it into vectors into your vector database because AI speak vector it's able to map linguistically or um just directly your queries against those knowledge bases. A common one is like open search and that's about $40 a day or Aurora that's less than a dollar a day. Prefer that because it's way cheaper. Um also guardrails which again the last talk was all about was excellent. Um guardrails are a way to police the tokens in to your model, the request in the text and also police the text out. And that's really important for two directions. We're going to talk about that in a second. Um, here's an example of Bedrock guardrails. So, you can enable like harmful categories filtering. So, if you don't want your bot to like insult people or generate hateful content, this is probably important for you. Even if someone asks the AI and it of course will say yes, because all of them do say yes, of course, I'll generate some hateful content or sexual misconduct. Um, please don't. Bedrock guardrails are there to help you. You don't want to expose that stuff to your employees or to your external folks. It's turtles all the way down. Um there is AI for your AI in guardrails. You can write topics that it should not talk about. And this example is an actual real life one from work because your work assistant shouldn't be an insider trading assistant. Um we found we trained our model on uh we we ingested all the data into a knowledge base more so and then when that model was asked about like hey when should I purchase stock for this publicly traded company it was scary accurate of like when you should actually sell your stock which is terrifying that's called insider trading please don't do that um and we found that it was ingesting all of our product launch dates it was ingesting all of that information and it knew when there were delays it knew when stuff was published and that's cool. That's really really cool but also it's it's illegal. So that was one of the topics that we turned on early on and that's worked pretty well. AI for your AI is there. Um this is the primary architecture that we're using today in AWS. You can see there's a whole lot of bedrock going on but let's walk through the steps. So over on the left side there's a system input and that's however you generate a request to your AI stuff. Um, we have uh several chat bots that are in play that use generative AI with this architecture, but other applications are using it as well. And you can see there's a lambda receiver that sort of receives the request and responds immediately. A lot of web hooks have a timeout that's just a number of seconds and bedrock takes longer than that probably. You're going to have a couple conversations. So the receiver says, "Hey, web hook, cool, we got it. HTTP200." And it summons from Azithoth, the other Lambda worker. uh that's going to do all of the bedrock stuff. You know where Cthulhu is from? You guys get it. And uh that lambda worker is going to have several different bedrock conversations. So here, first of all, it is going to talk to the knowledge base, which is usually backed by open search or Aurora, and it's going to say, here's here's the request from the user. Here's the system prompt. Here's all of the information that we need. Please give us some vectors. And you can ask for a certain number with the API from 1 to 100. And generally we ask for something like 50 or 75, like a whole bunch of chunks of data that look like they're related according to open search, according to PG vector. And then we ask the reranker, which actually reads your request and all the chunks and can pick out which ones are relevant to winnow it down. Because if you feed 50 or 75, you know, text chunks to a foundational model, it it chokes or it gets really expensive. But occasionally, we've seen it just go absolutely crazy and uh give you back nonsense. So, re-ranker works super well. It takes about a quarter of a second. I have no idea what the magic gremlins are doing there, but they um improve the fidelity of the responses with almost no time and almost no cost, like dramatically. So, definitely rerank. Uh I have heard through the insider group that the reranking will be a checkbox pretty soon for your API request to knowledge bases. You won't have to call a separate model. You just say, "Yep, rerank it, please." And then once those two things are done, we're along the bottom with the bedrock guard rails and the bedrock uh foundational model in the bottom right. And we take the entire computed uh context from the user, the request, the system prompt, and the vectors that were passed through the reranker filter and came up, you know, good enough and feed it to a uh foundational model and say, give me a response that I can send back to the user. The guardrails police the tokens in, they police the tokens out. And if you pass both directions, we feed something back to the user. That's a ton of things. That's a lot of complexity. But this all happens in about 3 to 6 seconds, depending on how complicated the thing is, which is just incredible. Um, that won't work for every use case. If you're waiting, you know, you can only wait a second, that's not going to be good enough. You might want to train a model to do something like that. But for this very cheap thing, this works very well. Um, we're with one of our uh generative AI applications. It's an internal chatbot that lets you chat with our internal stuff. We're responding to about 150 requests a day and it costs something like a dollar a day. So, it's a dramatically cheap way to implement something and to get your exposure to AI. Um, foundational models don't support documents at all. Uh, so even though when you go to the documentation for like a claude sonnet or something and they say we support PDF, there's a little tiny asterisk in there and that those tiny asterisks they always get you. And this one says we support PDFs if you use our services directly. If you're using Bedrock, nope. No documentation, no PDFs, no docs, no images, no nothing. And that really cripples your model depending on your use case. But if you're like accepting free form stuff, free form content from your users, that's not great. That's not that's really going to limit you. So the Converse API is a meta API from AWS. I'm a huge fan of this. I want to cheer for it. Um it is an API that you can target with your applications and it will transform your request on the way. it'll proxy it over to the model and it'll reform the API to whatever that model is expecting because all the models expect different stuff in their APIs, right? So, this lets us really easily upgrade to a different model, try stuff out. It's super easy. You don't have to change your applications. You just change the one string of what you're requesting and the Converse API does the rest. Oh, and also it supports all the document types that your users will use. So, you can feed in PDFs, documents, Excel, PowerPoint, all sorts of stuff. when you call bedrock, who's keeping track? So, first of all, the IM is validated like with everything in AWS, IM is involved. Uh the principal IM policy is of course validated to make sure that you can call this model. This model is kind of a misnomer. You are calling a um meta resource that exists in Bedrock, but it's not a real resource in the same way that like an S3 bucket is a resource and it's singular. um those resources, especially the models, but everything in AWS is not a real resource. There are no resource policies. That's why I have this crossed out here. So, if you want to say protect your knowledge base with all of your company's most sensitive data, you can't. And that's not great. It's coming. What I hear from AWS is it's coming. You can instead audit your hundreds or thousands of principles and make sure that they can't do bedrock stuff. Not great, but it's possible. you can secure it. Um, cloud trail in the center is logging. The robot's waking up. Write that down. Who's calling a certain verb against a certain noun? You can audit that to see who is doing what. Really, you should have a tool be analyzing this stuff and keeping track. And if someone starts suddenly just absolutely hammering your serverless bedrock and charging you a ton of money, you need to notice because it's difficult to protect them. There's no resource pol policy to protect these models. So, watch cloud trail. And then Cloudatch can give you some statistics uh and also record the conversations. So you can build some dashboards that say like how many calls are we getting for these particular models over time. That's excellent. We've built some of these internally. But it can also record the conversations. Recording is kind of weird compared to to Azure. So I was going to talk a lot about this when we get to Azure. You can't record all these conversations for just a particular deployment like a particular agent or something like that. You turn it on for a region in an account and it for all the AI requests against all the models get put into one log group and that's fine. It's not great but it's fine. It's just different to it's difficult to disambiguate all those different conversations. It can be a real challenge to do so. Um securing AWS Bedrock you can just turn on AWS config. Oh no no you can't. There is no resources in Bedrock. So there is no config rules. So, nope. Uh, there are some gaps. I had this on my reinforced talk uh two weeks ago and they made me delete it. Uh, so I made sure to bring it here for you all. Um, there's no bedrock resource policies like we talked about. Those are coming. We hope to protect your stuff like you know your knowledge bases with all of your data. Um, or if you want to say, you know, only this principle can use this expensive model. That's not a thing yet. There's no way to protect expensive stuff from utilization within the account unless you're using principal IM policies. The webcwler data source that can shove data into your knowledge bases is pretty beta. I am it will surely come, but I'm just hoping that it comes sooner. There's no web authentication at all. So, if any data is behind a login, even basic login with a username and password, you can't get there unfortunately yet, much less SSO. Um, you could probably build a reverse proxy uh that would authenticate for it. That's a terrible security design. Like don't blame me if your security team comes after you, but you know it would work. It also doesn't support Aurora database yet. So you have to use open search which is pretty expensive. It's about 40 times more expensive than Aurora in my experience. And you cannot enforce guardrails. So in Azure Sai's going to talk about this. When you deploy a model into a deployment, you specify a guardrail and that's just hardcoded. If you use that AI, you use those guardrails. Um, in Bedrock, it's much more fast and loose. Um, on your API call, you specify a guardrail, and if you specify it, it's used, but if you don't specify it, it's just not used. So, your developers can just turn it off. Um, which is not great. We would prefer a developers have to use the security mechanisms that we have implemented for them, but they're pretty um, optional in this model. And let's hand off to Azure. Thank you, Kyler. So, so far we heard a lot about AI usage, model usage, security pattern on for for for my talk, I'll just mainly focus on Azure services and Azure enabled AI services and and and and before we go to kind of resource level configuration, we just talk about the shared responsibility model that Microsoft publishes in their documentation. And this is very similar to what we saw 10 years ago with cloud-based models as well, like you know, how much security the customer is supposed to own and what is the platform ownership and so on and so forth. And this is just from from their documentation and in our case as consumers of Azure AI we equally own responsibility of the security of the platform and also um and and also how the models are hosted and all of those things that the the AI the the AI apps actually use and in the following slides I'll touch upon various controls available to us to secure the p kind of secure AI patterns in Azure. Firstly we start off with resource implementation. Um this is a high level design document or design diagram of how um Azure ML or Azure AI services or Azure AI foundry AI hub or all of those services actually deployed and I'll actually go through various layers of the deployment. Uh firstly we'll start off with network deployments. Um again most of these are not by default. So that's the reason why we're calling it out when you kind of go go to deploy these is not the default settings that you get on the platform. So the idea is that once you start deploying the resource you always want to select a Microsoft manage network so that all of your ingress egress controls are applied and we can actually govern you know what has access to the service and also what what what services it's actually going to talk out to to the internet or to the Azure portal or anything else. Secondly you have a lot of support services that need to be deployed like ACR storage accounts key vaults databases and other stuff that need to be deployed. So all of those things again by default are not secure or not privatized. So the idea is that when you deploy them you know you have to ensure that you get private endpoint setup private link setup or to some degree service endpoint setup so that you know only ML services and authorized users can access them and it's not just accessible to anybody that has you know uh kind of network access to the system and other than that overall um the the bigger picture is that once you zoom out we need to know data we need to have logs of everything so we um the best part is to integrate Azure monitor that has diagnostic settings activity logs or insights that provide a kind of robust logging framework for all of these services and activities that they perform. So if you have an ML uh workspace or or like a workbook trying to query an ACR and trying to get some data from storage accounts all of those things that track to some degree and actually it'll help you troubleshoot issues also it'll help you to identify what's going on within within that within the platform itself. And one more important question is now that we privatized everything right we have our data privatized we have our sources privatized and everything else but when when you deploy the model if the correct options are unselected the data could be shipped off to some other geographic location for processing purposes and that's something that you have to pay a lot of attention to when someone is using um like an available model from the marketplace they have different deployment standards uh so deployment methods and one of them is global standard or standard and stuff like that and you have to select the se the correct option if not then Microsoft themselves call it out that you know the data could be going outside the geography and it's not the best intention especially if you're dealing with sensitive data and so on. And then finally uh you want to apply uh key volts for all your security and like kind of secret contents. If you have certificates or keys and anything that you're kind of accessing again based on deployment patterns you want to use key volt and have workbooks actually access the key vault to get content out of it rather than just hard coding it into the values. And like I was mentioning about all of these sound like very obvious things to do on the platform when you deploy it. But again when you give app dev teams access to start deploying stuff this is not the defaults and they kind of skip over it either use terapform bicep arm or even if you just click ops the entire deployment you know there is chances that the defaults are selected and the defaults are more cost effective than actually security effective. So the Microsoft prioritizes actually cost in this scenario than than than security. Um the next thing we want to talk about is data access patterns. So now we have a lot of data sources that we want the models to either fine-tune or train on or just access for various other purposes for the application to consume. But now everything is privatized on the deployment side. How do we get to the actual data source? Um and and predominantly I'm focusing on Azure based services and maybe Azure hosted partner services like snowflake and other services as well. The idea is that from Azure ML um either from Azure ML or Azure AI hub, we want to establish private connections, private endpoint, private service and and and sorry uh private link service endpoint connections to either a cross tenant, cross subscription or even a partner application so that your data doesn't travel the public internet and it is to some degree has private control and you have an option to disconnect the connection or maybe break a connection in case something goes wrong. uh majority of majority of the past services that I'm talking about over here support Microsoft entrabased. So the ML workspace has identity so you want to enable that and then use that to kind of authenticate against a SQL pass database or or like a storage account or maybe even snowflake if if possible like you know there are options that you actually can use to actually authenticate and not have to use keys or standard like um kind of static credentials to access the services. And then lastly we have data sources protected. And now we want to talk about data like the access to the ML workspace itself. So we have data scientists or a researcher uh access to them to this to to run their workbooks and and maybe configure the workspace and just deploy models and complete the entire application deployment. Primary would be using VPNs and then get kind of establish a private tunnel to the AI workspace and then from there um you know access the workbooks and then and then do their configuration and so on. this at a very high level for data access and and and the deployment practices. So from here I'll just talk about a very high level picture of how an AI application would be designed on Azure and various at at various layers we can apply security and then we'll focus on some some of them are application related which might which might be something we already are doing a lot these days and then some of them are als and and we have a good number of them using Azure policies and and then you know content controls on the AI workspace so that you know you actually have the correct deployments actually going on. Um so this this is a simple app. Um so we have like a web application, a function app or a Kubernetes based app something very very simple um that users are actually accessing and from there the app has access to the models that are hosted within your AI services again ML workspace or anything like that and then they then reach out to either OpenAI or search or anything else and then get get the responses back to the user requests. Um yeah so firstly at the edge we want to protect the application by by by applying ingress egress control. So we have like a front door vap or some kind of vap solution so that your your endpoint is protected like an API endpoint is protected and then you have an Azure firewall that gives you access to govern all outbound traffic. So you don't want your models making outbound calls randomly and then kind of communicating to somebody that we don't want them to communicate to. So that's the first layer. The second uh yeah this is so this is a screenshot of how the network settings would be on the AI hub when you when you get to deploying it and and here is like a screenshot of like firewall rules very granular firewall rules you can see it it's like it has like access to like the pi or you know all of the stores and stuff so you can download Python um kind of the the libraries and so on and so forth. So and then the second layer you know you have at the application layer uh the idea is that here you want to use some kind of intra based off to authorize and authenticate with users and then can have some kind of authentication with the models as well and that's where we basically want to do uh the layer of here and then if you the the this slide we talk about the roles available which can be assigned to either the application the data scientist or anybody else that can actually be uh accessing your backend work workbooks and models and then you have some kind of security around there rather than again using static credentials. And then the finally the last piece would be is Azure policies and controls. This is I think something um uh we have actually implemented a lot within our tenants is that they actually have policies that just outright block stuff from being deni from from being deployed. We can we can communicate, we can tell, we can document all that's allowed and not allowed, but then we're going to have some app dev who doesn't like who who didn't read or something else or who missed it. And again the defaults are not secure. So we have policies that will just block any insecure deployments either it being the model deployment or maybe accessing like an insecure model like suppose there's there's untrusted model that we don't want to use as an organization that's using policies you can just block them from being deployed and then you also have like controls within the AI uh workspace itself that you can actually have content filters and like I was mentioning you actually can specify what it it it should accept and what it can respond back and so on and so forth. Um here's some highle screenshots of policies dep deployments. Um some of them are in preview from Microsoft. Some of them you know we have actually kind of tweaked for our own use kind of use. And then they basically provide you options to just block uh registries or marketplace vendors from publishing their models and from us when accessing them. Um and then finally the last screenshot I have is just content safety. when you actually within the AI um ML workspace itself you have options to actually enable content filtering and you also have option to do custom block list which is I think in preview still and where you can actually define what is allowed and not allowed for the bot to actually respond back on it's at a very high level um overview and then here's some different documents on from where I got all of this content and a lot of this is actually available on the portal but you have to actually dig through a lot of details uh they prioritize a lot of like application dev work then then then resource configuration So this is just more of how to secure a resource implementation within Azure itself. And Kyler, back to you. Yeah, we're going to close up here. The we have a couple of just takeaways for you. One of the primary themes of what Sai went through is that you can build your stuff in Azure, but the defaults are not secure. That's not just Azure. That's absolutely every provider today. You have FOMO that your companies are going to miss out or your yourself will miss out on AI. These cloud providers have the exact same thing. It's just ramped up. So they're not waiting until it's secure and ready and stable. They're shipping it as soon as it's ready. So make sure that you are turning on the security features that are available. They won't be turned on for you. Um AI services are both securable and cost-effective. You're going to have to do the work just with any just like with any architecture, but you can absolutely do it. And your developers are building AI services now. If you're not aware of that, you need to become aware of it because it is happening absolutely in your organization today. All right, that's it for us. All right, do we have any questions from the crowd? These are not malicious QR codes, but you know, trust me and find out. No, thank you so much for the presentation. And I have just a quick question specifically for Azure uh about your security control. I was curious to know if you have already tried you know the Azure EI red teaming agent. Uh no we have not tried that. Uh we very early in that option. We not tried the the the agent that you're talking about you know and maybe pirate you know to run simulation about malicious content. Uh not yet. Again we're not there yet from Azure point of view. uh it's pretty early and uh we have teams right now doing some research and and identifying like secure patterns for deployments but we haven't tried those two agents yet. Yeah. Okay. Perfect. Thank you so much. We're in healthcare in the United States so it's one of the regulated industries like like fintech and so we're pretty conservative in terms of new adoption of stuff. So we're we're getting there but we're involving like lots of legal teams and infosc and stuff like that. So we're a little slow sometimes. Thanks so much.

# whoAMI: Discovering and exploiting a large-scale AMI name confusion attack

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=2mxMW6dw-Fs)

- **Author**: Seth Art
- **Talk Type**: Security

## Summary

Seth Art reveals a critical name confusion vulnerability in AWS AMI (Amazon Machine Image) deployment processes that could allow attackers to gain remote code execution in thousands of AWS accounts. The attack exploits missing owner validation in automated EC2 deployments, particularly in Terraform configurations, where attackers can publish malicious AMIs that victims unknowingly deploy instead of legitimate ones.

## Key Points

- Name confusion attacks occur when attackers deliver their resource instead of the legitimate one being requested by name
- The vulnerability stems from optional "owners" attribute in AMI searches - when omitted, attackers can publish AMIs with matching naming patterns
- Attack provides remote code execution and access to IAM credentials in victim accounts
- 2-4% of DataDog's monitored customers showed vulnerability to this pattern monthly
- AWS internal non-production systems were found vulnerable and subsequently fixed
- Terraform made the owners attribute optional in 2022 after it was required since 2018 due to CVE-2018-15869
- Multiple programming languages beyond Terraform are vulnerable (Go, Python, shell scripts)
- Attack can be delayed through launch templates, autoscaling groups, and CloudFormation stacks

## Technical Details

**Architecture Components:**
- AWS AMI catalog and EC2 describe-images API
- Terraform data source aws_ami
- Command and control infrastructure (Sliver framework demonstrated)
- CloudTrail logging for detection

**Implementation/Methodology:**
- Attacker creates malicious AMI with doppelganger naming pattern
- Victim's automated deployment searches for AMI without owner validation
- Malicious AMI appears more recent, gets selected over legitimate ones
- EC2 instance launches with backdoor, phones home to attacker
- Attacker gains shell access and IAM permissions of the instance

**Key Findings/Results:**
- Thousands of vulnerable code instances found on GitHub/SourceGraph
- AWS internal systems were exploiting vulnerable AMIs 70,000 times
- CirrusCI documentation contained vulnerable pattern, fixed within 12 hours
- Created Cloud Image Investigator tool for AMI verification and analysis

**Tools and Technologies:**
- AWS Allowed AMIs feature for prevention
- Declarative policies for EC2 at organization level
- CloudTrail correlation rules for detection
- Cloud Image Investigator (investigator.cloud) for AMI analysis
- Semgrep rules for code scanning

---

## Full Transcript
Our first talk is uh who am I discovering and exploiting a large scale ammy named confusion attack or am I you'll figure it out. All right, welcome Seth. Thank you. Hello everybody and welcome. My name is Seth Art and today I'm here with a public service announcement. If you use AWS and you have automated the deployment of EC2 instances in some way in your environment, most likely probably Terraform, you might be vulnerable to a name confusion attack that would give the attacker remote code execution into your AWS account. So what is a name confusion attack? Simply put, it's when you are requesting a resource by name and the attacker has figured out a way. Okay, so what is a name confusion attack? It is when the attacker you are requesting a resource by name and the attacker has figured out a way to deliver their resource to you instead of the one that you were searching for. So the most quintessential name confusion attack is typo squatting on a domain or a URL something like that. Now today we're going to be talking about a name confusion attack that target is targets AMIs or amies. And what are AMIs and where do we find them in the AWS console? you'd find them in the in the AMI catalog. And so here I'm searching for Amazon Linux 2023. And you can see that in the community section there's a warning. It says anybody can publish an AMI and they would be in the results. So use caution. So what tools do we have to use caution? Well, there's this verified provider stamp here that's green. And the way that works as you see the arrows that's pointing to owner alias. That's an attribute that comes back when you do a describe images call. And if the owner alias is Amazon on the console they put verified provider. And what this means is this happens at the account level. So when AMI is not verified the account serving the AMI is verified. And according to my research that's about 200 250 accounts. Now what happens when you find an AMI that's interesting that doesn't have that verified stamp. So here we have the owner alias is null meaning it hasn't gone through the verification process. And so the we'll come back to this at the end of the presentation on maybe how you can have some tools that might help you better answer this question. What is this AMI? Is this an official Buildkite account or is this just some random person publishing a Buildkite AMI? So why am I here talking about AMIs with all you? So I joined Data Dog a year ago to do cloud security research and advocacy. But before that I had been a penetration tester for 15 years. The last four of which I started the cloud penetration testing practice at Bishop Fox and two years ago at forward cloudsack I debuted cloud foxible. This is a gamified learning environment to teach the art of cloud penetration testing. And the way this works is it's a GitHub repository that's open source. You clone it. You configure your AWS profile. You install Terraform if you don't have it installed already. And you do a Terraform apply and it spins up all these resources, these challenges in your AWS account. Now I have to always give the disclaimer do not deploy this into a production account or anything that you use on the daily. This should be deployed to a test playground account. But in cloud flexible I create multiple EC2 instances like probably many of you. And how do I instantiate an EC2 instance using Terraform? I use the resource type AWS resource. And then how do I pick which base image to use? Because that's at the core what an AMI is. It's a base operating system image used to create EC2 instances. I specify the AMI. Now, you can hardcode an AMI ID here, but there's a couple of reasons why that's not always the best practice. One of them is portability. AMI IDs are region specific. So, if I did do that here, every AMI or CODFoxible would only work in one region. So there's this cool data source called AWS AMI that does a describe images call on the fly when you do a Terraform plan or Terraform apply that grabs the AMI that matches your search criteria. And so I in Cloud Foxible am searching for Amazon Linux 2. You could see the rest of it and there's a star in there because I always want to get the most recent one. That's usually where the date is where the star is. So this allows me to only pick the most recent if more than one result is given back. And then here is where I'm saying I only want the ones from those verified providers. So now we have arrived at when I realized there might be a thread to pull on here. What if somebody forgot the owner's attribute and made this query instead? Is it optional or is it required? Because if it is optional and somebody forgets it, what happens if I publish an AMI that meets this naming pattern today? it's going to be more recent by definition if I just published it than any of the official ones. Would a victim account slurp up my AMI instead of the one they were looking for? So, I went to the documentation. Is this thing required or optional? And as of 2024 when I started doing this research, it was optional. So, this is the point where I knew I had a theoretical attack in place, but that's all I knew. I was new to this AMI uh stuff. So let's see what the attack looks like when I tested it in my own account to prove the theory or two two accounts that I control. So what I'm going to show you first is the vulnerable deployment before an attack has ever been executed. So highlighted in yellow, it's missing the owner attribute. It's vulnerable and they've just done a Terraform apply the victim, but no attack has been executed. So they're going to get the AMI that they were looking for. And right here, the describe images happen behind the scenes. They're going to grab that AMI. And so what we're going to do here is we're just going to look, okay, yes, we're going to confirm manually that that AMI is from a verified account. So now let's see what happens when the attacker executes the attack. Now I have here the attacker creates malicious public AMI with a doppelganger name, but I'm only going to share it with one account for this. So what the attacker is doing is they're picking a name filter that they want to exploit. It could be the Amazon Linux name filter. In this case, it's the YUbuntu. And you can see they named their malicious AMI C2. That's for command and control. Because this AMI that I just created has a C2 in it. I'm only sharing it with one specific account. I wouldn't want to make public an AMI that has a call back uh to it. And this is the account. It starts with 864 that is serving the AMI. So this is the attacker's account. And so just remember that 864 now the attack has been staged and then next time the victim they don't have to make any changes. Next time they hit terraform apply to that vulnerable code they're going to do the describe images again and they are going to get the attacker's AMI. So let's take a look at what that looks like. So right here the AMI is different and we're going to do a describe images call again. And if this thing worked, this AMI ID is going to be associated with the attacker's account. And look, it is. So at this point, I knew in two accounts that I control right in my victim account, I pulled the AMI from the attacker account. Now, because it has a command and control callback built into it that's going to execute at startup, as soon as the user hits, you know, approve or says yes to the Terraform reply, that instance is going to get created. It's going to phone home back to the attacker's infrastructure and the attacker can have remote code execution on that instance in the victim account. So I'm using sliver. You could use metas-ploit or any command and control framework for this. And so now the attacker is interacting with the session. They just got the call back from the victim's account. They are then going to spawn a shell. And at this point they can do they can do post exploitation. So here we have now so as many of you know this is why the principle of lease privilege comes is so important in the cloud because any IM permissions that EC2 this EC2 instance has the attacker now has access to that. So the attacker first just checks do I have any is there an instance profile attached to this instance? Yes there is. Do I have permission to list all the buckets? Oh, look at that. I do. Do I have permission to list the objects in the bucket? The one called super secret stuff bucket? Yep. And cross your fingers. Does the attacker have the ability to get the object out of the bucket? Spoiler, they do. And so here in my demo, there's where the sensitive data lives. So the attacker hopefully this has showed you what this looks like from the victim side and the attacker side and how the attacker could gain remote code execution and IM credentials. So let's take a step back because believe it or not this is not the first name confusion attack targeting AMIs. So I had been thinking about targeting uh infrastructures code and code in general but I believe the first attack the honor goes to Harunir and Marco Slavierro and I actually found this out reading Scott Piper's blog that he wrote in 2021. So they weren't they were targeting users of the AWS console back in 2009. What they realized, and this is pretty cool, is that when you search the community AMIs, they're ordered by AMI ID in ascending order. So, they had a hunch. Well, first of all, AMI IDs used to be a lot shorter then, and there's probably a lot fewer public AMIs. They had a hunch. If I can create an AMI ID with a low enough ID, it might show up in those first results. So they created a script that created the same image twice, got two AMI IDs, compared which one was lower, threw out the higher one, and then repeat, repeat, repeat. They ran that in a loop, and by 2 hours, they were targeting people using Fedora Core 11, they had a top five result for Fedora Core 11. Within an hour, a couple of hours of that in their talk, they say people started using their AMI. Now, let's fast forward to 2018 because not only is this not the first name confusion attack, this is not the first time that this attack has been executed. In 2018, Hashi Corp's security team made this issue on the packer repository. So, packer is a way that you can create images. You can, one of the ways you can do that is you can give it an AMI ID as the source. You can add your customization and you could build, you know, a new AMI ID. And they realize that if you don't have the owner like I realized, but this was, you know, 6 years before I realized it, that you could get in trouble. You can get an AMID that you don't intend to get. I don't think it was a coincidence that they created this issue because the next day somebody said, "Oh yeah, I was looking for a Ubuntu AMI and I got a Monero minor instead." Scott Piper also, every time I've ever done any research in cloud security, I realized that Scott Piper had done the same thing six years ago. I'm sure that's happened to others in this room. Uh Scott Piper investigated one of these as well in 2018. So I've been talking about this CVE, but this story has lots of twists and turns. The CVE is not associated with Packer. The the Hashior team created the CVE for the AWS CLI and got it granted by the authoring uh body saying that the AWS CLI doesn't require owners and it shouldn't. But AWS was like, "Listen, like there's lots of reasons why somebody might want to do an EC2 describe images without owners. I've done that hundreds of times over the past year in this research, right? I just want to list all the AMIs in the catalog." So, they essentially treat this as a won't fix even though the CVE to this day is still associated with the AWS CLI. But the good news is that the data source AMI, the Terraform side, they also made the owner required. So since 2018 or as of 2018, both Packer and Terraform had the owner required. So if any of you are wondering why I said it's optional in 2024, but it was required in 2018, what the heck happened? Well, in 2020, somebody politely said, "Could we have it optional?" The story's not over. They had a really good reason. They said, "Sometimes I have the image ID and I don't want to have to then search for the account associated with the image ID. Don't make it required. It's an extra step." The Terraform team stood strong. They said, "No, there's a CVE. That's why we have it this way." And the person who made this request is like, "I didn't know about it. Makes total sense." Unfortunately, in 2022, somebody asked for the same exact thing. And this time, we weren't so lucky. Open source did not stand strong. they probably were lacking the context of a CVE that was four years old at this point and made it optional. So between 2022 and 2024, this attack that was possible before 2018 is now possible again. And that's when I stumbled across it. So that's the history lesson. So now I know it's theoretically possible to be vulnerable to this in Terraform. So I wanted to start seeing well has anybody made this mistake in the last two years? So I went to GitHub and source graph and started doing some searches with the vulnerable criteria that we've been talking about. Now I was thinking as a researcher for prevalence. How many people are actually vulnerable to this? What I realized was that this is the same art of detection engineering, right? I want to filter out false positives and I want to think about am I missing any false negatives. So let me take you through that journey. So what I realized that I found at 1.5,000 files before, but a lot of them were false positives because if you use name owner alias, that's functionally equivalent to owner equals Amazon. So get rid of all of those. And of course, anybody who's hard- coding a owner ID, they're not going to be vulnerable to this attack. So you got to remove those results. So I have 680 left. Remember that every file that I find in a public GitHub repository means that there's probably even more in private repositories all across the world like in all of our organizations. Now, this is where I finally had my second realization. I had just been so focused on Terraform because Cloud Flexible is Terraform and that's where my brain went. But you can be vulnerable to this pattern in any language. Oh, sorry, I skipped ahead. This is another way that you can be vulnerable to this in Terraform. You can use the name Reax. So this is functionally equivalent to the name filter. Um but you can also be vulnerable this to to this in other programming languages. Here is go a go example that is vulnerable to the who am I attack Python shell scripts. So at this point I finally feel like I am comfortable. I know I have the vulnerability summary mapped out. If you are doing the insecure search and then you're using that IM AMI ID to eventually eventually create an EC2 instance, you are vulnerable to this. I say eventually because it's the most common use case is you grab the AMI ID, you immediately create an EC2 instance. But there's all these delayed ways that you can also be vulnerable to this, right? You grab that AMI ID, you put it into a launch template or launch configuration. Once that executes in the autoscaling group, then the attacker's AMI is going to be launched. Same thing. Look at this example I found for a cloud for map. So, this is on a a well-known vendor website. They just it was an example, but it says this is how you can grab AMI IDs to populate a map that your cloudformation stack will use. Well, depending on if the attacker was exploiting this at the point where you grab those AMI ids, it might be, you know, a day, a week, a month later where you execute the attacker's AMI. So, how can we detect this? Little suspense. So, the two ways that I think are the best ways to detect this are number one in code. So, just like I was doing those GitHub searches, right? Here's an example SEM grep rule how to detect uh vulnerable Terraform files. And you can apply this to any programming language. But as all of you know, especially those in security, there's always going to be people doing things outside of the repositories that you have access to, right? Maybe they haven't checked their code into GitHub yet. So the source of truth for this, the best way to find vulnerability to this attack is using cloud trail. And particularly I found using correlation like in a sim. So I found that if I look for EC2 describe images events followed by within a certain time period, I think 10 minutes is the sweet spot for me, a run instances event, you can be pretty confident that they that and if it's both executed by the same principle, you can be pretty confident it's at least worth having a conversation with the owner of that principal ID. Okay, so what did I find? I'm going to share three findings with you because there's only so much time. The first is that when we deployed that cloudsim rule to our customers, we found in any given month between 2 and 4% of our customers were vulnerable to that pattern. Now, if you extrapolate that beyond our customers worldwide, that's a lot of AWS accounts that are likely vulnerable right now, probably at least one of you in this room. The second one is a fun story where we were trying to see if we could find anything vulnerable within the scope of AWS's vulnerability disclosure program. So we worked with them, coordinated with them. Interestingly, what we were trying to think if something like this like the launch instance wizard was vulnerable, but we that is this is not vulnerable, but this is where the story starts. We found something else that was vulnerable. So we thought this thing is doing a describe images behind the scene. That's why it's populating AMI IDs. So when you click Amazon Linux or Mac OS or whatever, you're going to get the right ID. So, we worked with AWS and we published an AMI with a very specific name and a very specific description that had a lot of info on how to contact us. And unlike my C2 binaries, these are bit for-bit clones. So, you're getting exactly what you're looking for, but it it would be coming from the attacker's account to prove the scenario. Unfortunately, it was never used in this. So, this is not vulnerable. But how are we going to find things in AWS vulnerability disclosure program that are in scope that are vulnerable? We looked for shared snapshot volume created events. This is what you see on the AMI, the account that's serving the AMI. And whenever the type is AWS service, that means it's an AWS backend service that's doing something either on your behalf or something internally. So we all we had to do was look for these events and if we had anything that matched this criteria we could submit it and tell the vulnerability disclosure team that we found something and we found that 70,000 times uh this event was seen in our lives 70,000 times. So, we shared this with AWS. They tracked it down to two internal non-production systems on their side that were using our AMIs and they fixed it and they looked more broadly to see if there was anything else that could have been vulnerable and we remove the AMIs and I think we're all better as a result of the research in coordination with the vulnerability disclosure team. The third finding, this one, remember I was doing GitHub searches? Well, I also did some Google searches meet with the same vulnerable vulnerable criteria and I found lots of different interesting things, but this is just the one that I want to share today. So, Sirrusci is a continuous integration platform you can use similar to how you could use GitHub actions and like GitHub actions you can ro you can use self-hosted runners. So this is what it looks like in their documentation how you would use a CI self-hosted runner using an EC2 instance. And you can see that name filter and this is what caught my Google search. They're saying we're doing the analog of this command. Now one of two things is happening here. Either they forgot to put the owners in the documentation and they really are using the owners to filter only the good stuff or they're not. So I think it's time to do a little test. So I created an Assiraci account. I uh configured it with the ability to assume a role into my account. I configured using the naming pattern that's in that documentation. And I noticed that a lot of their customers were using they have this ability to use encrypted secrets. So you create the secret on the CCI side. It'll encrypt it. You you put the encrypted value here and then in the task itself, it'll decrypt and run. So I was like, well, that's a fun challenge. Let's see if I can get my that that self-hosted runner to call back to me, the attacker, and read these secrets. And so that's what I did. So here is a a repository I created for this for this purpose. I'm going to commit my change kicking off the CI task. So you're going to see the CI task is now running. So on the back end, CCI assumed a roll into my account and created an EC2 instance. And of course, they grabbed my C2 instance. And again, I shared this with just my two accounts for this test. So now, like a good little malicious AMI, it phoned home to me. I have the shell. And now this time, I'm going to not see look for IM permissions. I'm going to see if I can get those encrypted secrets. And to do this, I used a trick that I learned when I was doing Kubernetes breakout research where I'm going to print the environment variables of every single process because some of the processes don't have any environment variables. But what I'm looking for is the process that has the CI task running. And so that's what we see here. I finally found it. And there, success. Here's the AWS secret access key decrypted. Next, we'll see the access key ID decrypted. And then the Docker password. And again, I use these examples because their customers were using this. So I could have in theory potentially compromised one of their customers and back door a binary that was, you know, a build a build artifact. Sirci was so amazing to work with. They fixed this within 12 hours of me letting them know like they knew exactly like the implications what I was talking about. No questions, just a thanks so much. We're on it. Notified their customers and yeah, fixed. So awesome job to them. So how can we all fix this? What's the best tool in our toolbox? Lucky for us, just last year AWS released the the allowed AMIs feature. So this allows you to create an allow list of trusted AWS accounts. If you're not on this list and it's in enabled mode, the the who am I attack for instance, it would just be thwarted in its tracks. It's not in the allow list. The best part, you can use the owner alias. You can use Amazon AWS marketplace. So that's you know you're getting 200 account IDs for one there. Now what we're looking at here is how you would deploy this per region per account. So I don't really recommend doing that. What you should do is use declarative policies for EC2 at the organization level. And Tyler Petty has an awesome blog post talking exactly what this process looks like and what it looks like before you apply these and how this effectively prevents the attack. So worth noting uh this page as well. But in summary, definitely use allowed AMIs if you're not. If you leave with nothing else, I would love for you to leave with that bit of knowledge. So here's what the declarative policy for EC2 looks like. And if any of you are familiar with SCPs, it's similar. You declare the policy and then you apply it to one account, an organizational unit or your entire organization. And you can run it in audit mode first and then eventually switch it over to blocking mode. I also wanted to create a tool that would retroactively look at all of the EC2 instances you had running and kind of find the AMI that was used for that EC2 instance and sort those into buckets for you. Right? The green buckets are AMIs that came from your account or other verified accounts. The red would be public, unverified, and unknown. Those are worth investigating. And by the way, the known and unknown, I'm using the forward cloud sec known vendors list to determine whether it's a known quantity. Maybe it hasn't gone through verification, but if forward clouds knows it is a trusted vendor, you know, that's where it goes in that yellow area. And then the last thing I'm happy to share on this side is that we worked with the AWS hashy corps AWS provider team and told them listen this used to be required now it's optional what can we do and to this day it's still optional but they have a more elegant fix they simply say if you're using most recent true and you are not using filtering by owner owner ID or owner alias they're going to warn you as of version 5.77 and I think around 2 weeks ago version six of the provider was released and it's a now a blocking uh error. So that's awesome. Okay, now I get to share my passion project that I've been working on lately. And to introduce this, I want to go all the way back to this first slide. It's always it's been bugging me, right? Since I've been thinking about AMIs, there's no easy way to know if this AMI is a legit one from Buildkite or not. So I wanted to build something kind of like virus total or URL scan for cloud images, right? In those tools, if you have a malicious binary or a malic or a or URL that you think might be malicious, you can upload it to virus total or URL scan and they'll they'll tell you they'll give you some indicators of is this benign, suspicious or malicious. And so I'd like to introduce this is a soft launch of the cloud image investigator which is public. Um and this is essentially allowing you to answer some of those questions. So let's go with question number one. That account ID associated with that build count uh build kite image. Look at that. It has 1900 AMIs released over the course of two years. They all have a similar naming pattern. I feel pretty good that this is the Build Kite account, right? Oh, you could also look up on Daniel's AWSI if you just click that if you want more information about this account. Now, let's take another example. This is an account that just published 34 AMIs last week, and I've never seen it since I started looking in January. Now, this is not malicious, but it just shows you the power of having this tool to be able to do some of this research. This is a list of the AMIs, the 34 AMIs. And let's say but what I have oh and so let's look at the metadata that I have for some of these AMIs. So what the thing that I really like is that you can see oh this AMI that we're looking at is still public. It was first seen on this date last seen on this date. But notice the bottom right corner the parent lineage. What's that about? I don't know if anybody that wasn't so heads down in AMIs like me caught this, but in the last year a AWS or sometime in 2024, AWS started adding two new elements to the results of the describe images call source image ID and source image region. So this gives us the ability to map parent child relationships between AMIs. They've known this for a while, but now they're exposing it to us. How cool is that? So now in the cloud image investigator, I can trace that lineage as far back as it goes. So the first two AMIs come from this same account that we're investigating, but then ultimately it comes from a verified AWS account and it's sourced from Ubuntu, right? I didn't even know it was sourced from Ubuntu until just now. But I wasn't done. I really wanted that virus total like pull back the curtain, poke inside a little bit. And so lucky for me, I learned about this research that Edward and Mate released at Defcon 32 last year. They cloned thousands of AMIs, scanned their file systems for secrets, and they found a ton of secrets. I definitely recommend this talk. It's really cool. Uh, by the way, the reason is because the EB the snapshots that are associated with the AMIs, they don't show up if you just like des like describe snapshots, public snapshots. They're in this like quasi public state. They're only available once you clone the AMI and that's why nobody had done this research before. So that was notable what they did. But the coolest thing for me is that they open sourced the tool that they used to do this analysis. So they create they I decided oh spoiler uh so they were looking for secrets in all these volumes but I wanted more of that virus total. I just changed the code that was looking for secrets and changed it to hash every file on the volume. throw all that into a database and now I can tell you exactly which files have changed between the child and the parent. So here we go. So these are if we're looking from the child perspective, these are either files that are new hashes, you know, maybe it's been modification of a file or it's a brand new file. Also, since I'm hashing so many images, I can tell you for this AMI we're looking at, these are the files, the file hashes that I've never seen in any other AMI. So, another kind of thread that you can pull on. So, here it is, investigator.cloud. Uh, I encourage you to take a look, poke around. I would love feedback. I have some ideas on what I want to do next with this project, but I would love any suggestions or feedback and see if my ideas align with what you are all thinking. I did find one cool thing that I disclosed to the AWS vulnerability disclosure program. It's nothing crazy, but I found that they were making AMIs public for like 10-minute periods and then they'd go private. I kept on seeing the same AMI with the same name, same description from different AWS accounts and then it would go private. So then I caught one of those in that 10-minute window, started poking around the file system manually and I found a couple of internal binaries. They referenced internal build systems. I disclosed this on hacker one that uh through the v vulnerability disclosure program and they fixed it. Again, nothing really sensitive here, but it just shows you that this might powerful tool for security researchers as well. So, in summary, the public service announcement is this attack can be successfully executed today. If an attacker were to launch this attack today, they would gain remote code execution into thousands of AWS accounts. But you all have the tools to prevent that. Go home and please let check to see if your organization is using this new allowed AMIs feature. It is the best bang for the buck. And defense and depth, right? Always deploy any kind of detections that you can to detect this in addition to the prevention mechanisms. And speaking of defense in depth, I don't think I've ever given a talk without talking about using the principle of least privilege for IM. And that applies here. We often think, well, it's going to be the EC2 instance that's public that's going to be compromised. So, we need to look at that one more carefully and not give that one too many permissions. But hopefully, this research breaks down that assumption and now you can realize any of your EC2 instances, any of your workloads could be compromised at any point. So, just use the principle of lease privilege. And the best way to do that I have found is through infrastructure as code. Thank you very much. All right. So, first question we have on Slack. Um, if you have Oh, questions in the room. Cool. Let me do one question on the room and I'll walk the mic over to you. Uh so the question is so you'll recall that uh Terraform reverted the change. Uh and so the question from Jeremy Croc is uh how do we as security practitioners carry forward the context on decisions we've made uh in interface design or anywhere else so that we don't do that ourselves like how do we remind ourselves that this was a vulnerability that we chose not to do. So we uh yeah that's that's a great question. I mean and that that is that that's the hardest thing that keeping that in the forefront. Uh from from my perspective, somebody who's been a penetration tester for 15 years, I would say the answer is always defense and depth controls, right? You always have to assume you always have to assume the worst. So you want to be auditing the Terraform code itself. You want to be auditing, you know, have your cloudnative controls in place. But yeah, there's I don't really have a good answer. That's a great question. Um, yeah. I I have a leading question which is just for fun. Um, now that it's blocked in Terraform again, how long do you think until someone asks politely for it to be unblocked? And we'll be back here. And for uh for a real question, uh, how much responsibility do you think tools like Terraform have to differentiate themselves from the CLI and APIs that the cloud providers publish to keep us safe? How much is it Terraform's job versus the cloud provider's job? Yeah, that those are good questions. Uh the the first answer is hopefully by publishing this research, it's going to be in people's with a catchy name. By the way, it does help it stay in people's minds more. Uh for the second question, it is really tricky. Actually, it reminds me of something I was going to talk about when you're when you're talking about name confusion attack, right? It usually means one of two things. either the place that you're looking for the resource. So the AMI catalog in this place is allows an attacker to launch the attack. The controls don't exist on the the repository, the registry, the catalog or sometimes they do exist but you just haven't implemented them, right? And so and sometimes it's a mix of both like this same type of attack name attack supply chain attacks in general they apply to everything, right? They apply to containers. They apply to models on hugging face. They apply to everything. So the responsibility has to happen in as many places as we can. You want the you want to put the responsibility on the the registry repository catalog owner, but you also want the tooling to do as as much as it can do. Uh yeah. Uh awesome job as always. Um I just question with AWS. One of the things I think is very cool about AWS is that they use a lot of the same infrastructure that we do to be build all their own systems. But obviously in this case here like when that those internal systems clone you know grabbed your AMIs or when those load balancer AMIs are being made public for 10 minutes you know do where do you feel that trade-off is like does it do you feel like it's better ignoring AWS for a second but just in the industry do you feel like it's better for organizations to use the same infrastructure that they give their customers or do you think it's better they kind of use like more of a closed backend system I think it's better if they use use their own technology. I think it's always better when you have people using the stuff you're building because that's how you're going to find as many bugs as possible. Um, so yeah, I I highly advise that pro that approach. Thanks. I have a comment rather than a question. I think the fact that Terraform has fixed the issue in their in the provider doesn't mean that that fix is automatically applied with the Terraform log files, chances are that provider is not going to get upgraded at least automatically. That is a good comment, right? uh it might be quite a while until people are using the new provider uh the new version and that Terraform fix only applies to the Terraform cases which of course are the most but there's all that Go code and all that Python code that is doing this and I saw lots of J I was surprised when I was looking at the user agents for the all of the describe images call I was really surprised at how many of them were like custom binaries that that are written you know the user agents were Java or you know things like Seth, have you seen any of the um name confusion attacks make their way into Guard Duty in terms of picking up any of these? Oh, I love that you asked that question. So, not in Guard Duty, but Codeguru, what's the AWS service that does the Yeah. So, go C code guru has had these uh signatures just missing owners basically for a while. It's it's hard work like now that I you know work at a company that creates detections, it's hard like so the code grew signatures like some of the languages have this signature and others don't. You know, like Java might have it but Python doesn't or vice versa. Um but I haven't seen it in Guard Duty. It would be cool to see kind of a a a sim type correlation retroactive type finding like that. But yeah, they've clearly had been thinking about this since the 2008 or 2018 time frame. Similarly, I think there's a way to use AWS config to check for this. Um, they definitely have a config that looks for specific owners on AMI and you could probably write your own to make it more expansive. Awesome. No, thanks for sharing. I I didn't know that. We got time for one more. How did the AWS AMI? Have you looked at marketplace images, third party AMI? Because they don't consistently use the proper tags. Any suggestions? Yeah, I'm glad these customers are so great at touching all the stuff that I had to cut from the the main talk. Uh, so I didn't look at the uh marketplace AMIs because I don't want to have to go through that like verification process or or you know Yeah. And because everything is technically verified, but you bring up something I originally was going to talk about. So Ian McKay after I released the who am I research on our blog, he did saw that in the marketplace side for Azure, they were vulnerable to a name confusion attack as well. So he actually registered a kind of like a marketplace Azure account created an AMI with like called get targeting GitHub enterprise and so on his onecloud please blog uh he walks through that journey. It's it's quite cool. So thanks for bringing that up. Cool. All right. Uh I'm sure Seth will stick around for for questions but uh thank you. Thank you very much.
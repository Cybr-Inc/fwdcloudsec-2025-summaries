# The Good, the Bad, and the Ugly: Hacking 3 CSPs with 1 Vulnerability

**Video Link**: [Watch on YouTube](https://www.youtube.com/watch?v=TkDsnzlPJAg)

- **Author**: Andress Riano (co-authored with Eli Hai Sasson)
- **Talk Type**: Cloud Security / Vulnerability Research

## Summary

Andress Riano from Wiz discusses the discovery and exploitation of a critical container escape vulnerability in the NVIDIA Container Toolkit. This single vulnerability was used to test the security postures of over ten cloud service providers, with the talk focusing on three representative case studies: Azure, Replicate, and Digital Ocean. The research highlights the vast differences in security architecture and incident response across various cloud platforms, ranging from complete tenant isolation to full service compromise.

## Key Points

- A critical container escape vulnerability was found in the NVIDIA Container Toolkit, a common component for running AI/ML workloads in containers.
- The vulnerability allowed mounting the host's entire file system into a container, requiring only control over the container image to exploit.
- The same zero-day vulnerability produced drastically different outcomes across three major cloud providers, demonstrating varying levels of security maturity.
- **Azure Container Instances**: Showcased a well-designed, secure architecture with strong tenant isolation. No cross-tenant access was achieved.
- **Replicate**: Showed signs of multi-tenancy and exposed a Redis task queue, which could have led to cross-tenant data access. Their security team detected the research activity and responded quickly.
- **Digital Ocean (PaperSpace)**: The vulnerability led to a full service compromise, allowing the researchers to access secrets, source code, and models from all other tenants on the platform.
- Key security controls that could have mitigated the impact include using Kubernetes User Namespaces, implementing strict network policies to block pod access to the Kube API, and adhering to the principle of least privilege for Kubelet credentials.

## Technical Details

- **Vulnerability**: A container escape in the **NVIDIA Container Toolkit** that allows mounting the host file system into the container.
- **Exploitation Requirement**: The ability to control the container image (e.g., provide a custom Dockerfile or a pre-built image).
- **Goal**: To achieve cross-tenant access by escaping the container and moving laterally within the host or cluster.

---

### Case Study 1: Azure Container Instances (The Good)

- **Service**: Azure Container Instances (ACI) with GPU access (in preview).
- **Initial Findings**:
    - Host contained only ~20 container images, suggesting a low degree of multi-tenancy.
    - Found **Kubelet credentials** on the host file system.
    - Found a `docker.sock`, allowing interaction with the Docker daemon on the host.
    - Processes from other tenants were not found in the host's process list.
- **Attack Path**:
    1. Escaped to the host file system.
    2. Attempted to connect to the Kubernetes API server using the found Kubelet credentials but failed due to network limitations.
    3. Created a new container on the host using the `docker.sock` and placed it in the **host's network namespace**.
    4. From this new container, successfully connected to the Kube API.
- **Result**:
    - The `get nodes` command only returned a single node.
    - The environment appeared to be purpose-built and isolated for the single tenant.
    - **Conclusion**: No cross-tenant access was possible. Microsoft's design demonstrated excellent isolation.

---

### Case Study 2: Replicate (The Bad)

- **Service**: Replicate, an AI model hosting and inference platform.
- **Exploit Adaptation**: The exploit had to be packaged as a **"cog"**, Replicate's container format for machine learning models.
- **Initial Findings**:
    - Host contained **more than 500 container images**, a strong indicator of a multi-tenant environment.
    - Found Kubelet credentials.
    - Connection to the Kube API from the compromised container was successful (no network restrictions).
    - Found processes belonging to other tenants running on the same host.
- **Key Discovery**:
    - Identified a process with parameters revealing a **Redis URL, including the password**.
    - This Redis instance was a task queue for handling user prompts and model predictions for both public and private models.
    - Access to this queue would allow reading, modifying, or deleting prompts and results for other tenants.
- **Result**:
    - The research activity was detected by Replicate's security team.
    - The researchers' account was disabled, and they were contacted via email within 5 minutes of detection.
    - **Conclusion**: Cross-tenant access was possible, but the provider had effective detection and response capabilities.

---

### Case Study 3: Digital Ocean / PaperSpace (The Ugly)

- **Service**: PaperSpace Deployments.
- **Initial Findings**:
    - Host contained **more than 500 container images** (high multi-tenancy).
    - Found Kubelet credentials.
    - Connection to the Kube API was successful.
    - Host processes indicated workloads from multiple tenants.
- **Attack Path & Result**:
    1. Used the Kubelet credentials to query the Kubernetes API.
    2. The `get nodes` command returned a cluster with **700+ nodes**.
    3. Was able to `get pod` information for any pod in the cluster, including pods from other tenants.
    4. From the pod information, they could retrieve the **image name** and the name of the **image pull secret**.
    5. With the overly permissive Kubelet credentials, they could also retrieve the **value of the image pull secret**.
    - **Conclusion**: **Full service compromise**. The vulnerability allowed them to compromise other tenants' models, source code, and secrets (like private registry credentials, environment variables, etc.).

---

### Recommended Mitigations

- **User Namespaces**: A Kubernetes feature that isolates pod user IDs from host user IDs, reducing the impact of container escapes.
- **Network Policies**: Restrict network access from application pods to the Kubernetes API server. Only components that need it (like sidecars) should have access.
- **Principle of Least Privilege**: Ensure that identities within the cluster, especially the Kubelet, have the minimum permissions necessary to function. The permissions on PaperSpace's Kubelet were excessive.

## Full Transcript

Good morning everyone and welcome back to day two of Forward Cloudsack. Uh before we begin, just a couple of quick administrative notes. Um first, just a reminder, please take out your cell phones and make sure they're on silent out of respect for the amazing latest speakers we have for you today. Uh second, we'd like to thank as always our sponsors who make all of this possible. In particular, we'd like to thank our gold sponsor, Tam Noon. So our first talk of the day we have the good, the bad and the ugly hacking three CSPs with one vulnerability. Uh this talk was put together by Hai bin Sassin and Andress Riano. Unfortunately Hai wasn't able to make it uh here today but sure Andress is going to have a great talk filling in for him. So please welcome Andreas to the stage. Thank you. Thank you very much. Today we're going to be uh discussing how we hacked into three different cloud service providers using a single critical container escape vulnerability. During our research, we identified this vulnerability in con in Nvidia container toolkit uh in Nvidia container toolkit and we used it to um hack into more than 10 different cloud service providers. Today we are just going to be discussing three out of those 10. As uh my presenter just said, uh my name is Andreas and my co-speaker is Eli. Sadly, he was unable to made it, but we are both security researchers at WIS and we specialize on cloud security research. We identify vulnerabilities in cloud service providers and also in open-source software such as Kubernetes and in this case Nvidia container toolkit. NVIDIA container toolkit is basically a container runtime that's going to allow you to run AI inference and AI training from within containers. Nvidia container toolkit is responsible for configuring your containers in such a way that those processes can access the Nvidia GPUs. You might not have heard about this software component, but it's actually very very common. If you are running an AI service on the cloud, 90% sure you're running NVIDIA container toolkit. During our research, we found a critical container escape vulnerability. This vulnerability allows us to mount the host file system inside the container. This means that from within the container, we can read and write files from the host file system and we can potentially interact with unique sockets which might also be in this host file system. The only requirement that you have in order to exploit this specific vulnerability was to control the container image. You had to either control the Docker file that's going to be provided to the service or provide an image that you build with a specific Docker file. This was the dream vulnerability for us. We knew that uh Nvidia container toolkit was used in almost every cloud service provider and uh the exploit itself was very reliable. We knew that there was not no way for this exploit to crash any of those services if it failed in any way. So we really believed that we had a chance to answer this question. How does each vendor handle a critical zero day vulnerability? From previous research, we knew that every service is built in a completely different way. But in this case, we had the same zero day affecting a ton of services. So we started our research and uh found between 10 and 15 uh different targets. We exploited the vulnerability and today I'm going to be discussing just three of those case studies. These case studies are representative of what we found in all the research. First let's start with Ashure container instances. Ashure container instances is a service provided by Ashure in which you can provide your uh image and they are going to run that on their network hardware and so on. You don't need to worry about uh updating packages in the operating system. You don't need to worry about anything. They just run your image. Azure container instances has uh GPU access by in preview mode. So we got access to that and uh it was pretty easy to get the exploit to run almost no modification in our initial proof of concept. After the successful exploitation we started a quick analysis of the host file system. What we found was that uh the host had around 20 container images. By inspecting the overlay FS, we were able to find that information in the host file system. We are interested in knowing why and how many container images exist in this host because in uh our research we aim to do cross tenant access and uh this means that we want to prove access to information from other tenants. in this case and because there were only 20 container images, it doesn't look very promising for cross tenant access. Next on the host file system, we found the cubelet credentials. This proved one of the things that we wanted, which is that the host was a cornet cluster. Then using those credentials we tried to gain access to the cube API but due to networking limitations and the security controls that was impossible. Finally in this initial analysis we went through the host processes using the proc file system. You can read all the processes from the host and uh we were able to just attribute those processes to different services running on the host. We were not finding processes from other tenants. Once again, well done Microsoft on the design and isolation here uh in the design of Ashure container instances. But we really wanted to connect to the cube API. So we had access to the host file system and because there was um docker sock we could interact with that in order to create new containers. So we used the docker cli in order to create a new container in this uh host and uh we used a special trick. We created the container in the same network name space as the host. We did that because we know that from within the host it's possible to reach the cube API. So the cubelet can actually reach the c the cube API. So we place ourselves in that same position in the network. That was successful. We were able to connect to the cube API and using our uh credentials from the host we authenticated uh successfully. Sadly the get notes uh command only returned one node. This from the researcher's perspective is uh something that we were not expecting. We have high hopes for finding hundreds or thousands of nodes but that was not the case. Um this is very much aligned with what we saw before. So this doesn't seem to be um a service in which we are going to be able to get cross tenet access. We continued our research. We scanned the the network. We do we did DNS resolution for the internal IP address name space captured traffic but after 3 five days we just decided to uh label this uh use case this uh case study as no cross tenant access. This environment was built for us. It was not shared with other tenants and uh really well done Microsoft. This is what we expect from a well-designed system. Next case study is going to be replicate. Replicate is an AI software as a service in which you can upload your models and the community can consume them in order to do AI inference. So they can do text, audio, video and so on. Multiple companies such as Salesforce, OpenAI and Bidance are using Replicate in order to share their models. When you want to share a model on replicate, you need to provide a cog. A cog is basically a container for machine learning, but behind the scenes, it's just a Docker file with a specific entry point and a few customizations. Nothing nothing really fancy. But we had to adapt our exploit in order to be delivered as a cog. This required two four hours of uh research effort. After the exploitation, what we found was more than 500 container images. This is interesting for us and uh as researchers because it's a strong indication that many other tenants are running their workloads on this host. We were also able to identify the cubelet credentials. We found them and use them to connect to the cube API. This is one of the differences that you're going to see between Azure container instances and replicate. In Azure, we were unable to reach the cube API from within the container. Here we are able to do that. Finally, we listed the host processes and confirmed that some of those processes we could attribute to the host file system uh to the host and some of the processes were from other tenants running their workloads, their AI models. One of the processes that we found and we uh immediately realized that this was interesting was uh this one here. You can see just a process but one of the parameters is specifying a radius URL that in that radius URL and it was reducted of course you see the password to connect to the service and the full uh domain and port. From previous research we knew that uh this is a radius task queue. So every prompt from users goes to this task uh to this task Q and then finally reaches the AI models. The same thing happens with the result the results from the AI models. They go to the queue and then go back to the user. So at this point we could have uh connected to this ready skew and performed different actions such as reading the prompts, reading the predictions from other tenants also do some kind of interference by modifying them, deleting them and so on. And this was possible for both public and private models on replicate. But we were not interested in this uh ready service because that was past research. We wanted to actually connect to the cube API and uh expand within the Kubernetes cluster. We took a break and uh we were figuring out with Eli what to do and at some point we received this. So in our screen when we want to replicate we saw your account has been temporarily disabled. We said a few things that I'm not going to repeat in public and uh then we continued to of course we we paused our research and uh we started to figure out our next steps. 5 minutes later I received an email uh from Replicate asking if we were doing research on their platform. Um, we said many other things that I'm not going to repeat. And uh, then we just stopped our research and send all of the information to replicate. Really great job here, Replicate on identifying our actions in your network and uh, taking your actions to disabled account and reaching out to us. The last case study is going to be digital ocean. In digital ocean, uh, one of the services is called paper space deployments, which is just like ashure container instances. You upload your image and it's going to be run by paper space deployments. The exploitation was successful. No special customization in the exploit was needed and uh we performed our same three steps. The container images running on this host were more than 500. Once again, very promising and an indication that this is a host that's running um models for multiple tenants. Next, we found the cublet credentials. These credentials were going to be later used but the connection from within our container to the cube API was successful. No limitations there. The host processes also indicated that this uh host was running models for different tenants. We used our uh cublet credentials in order to authenticate and run some commands. The first one that we run was get notes and uh this was amazing. So we were able to get a response for that and it was a cluster with 700 plus nodes. This is a pretty decent cluster size. The next thing that we did was to get the information for the pod of one of our test accounts. And uh this also was successful. We were able to retrieve the name of the image running in this pod and the name of the image pool secret on uh paperace. when you provide an image that is in a private registry from the uh UI of paperace you need to configure the secret so that paperace can pull that image from your registry. So that's the name of the secret and with our credentials we were also able to retrieve the value for that secret and uh we could have done this same thing for every other pod in paper space. Um so this could be considered a full service compromise. We started from a malicious container. That malicious container was pulled and uh by the by the cluster by the coordinates cluster a pod was created. Because of the container escape vulnerability, we were able to move into the node and from the node we use the credentials in order to compromise models, source codes and secrets. The secrets in this case could be like the ones that I showed before Kubernetes secrets or secrets stored as environmented variables within the images or configuration files also. So this was uh a full service compromise very different from the other scenarios. Summary and takeaways. So we had one vulnerability affecting the entire cloud ecosystem. This vulnerability was a critical container escape and uh we used it to learn more about how different cloud service providers are building their offerings. This uh research had very different outcomes on every service. For Azure container instances, there was no cross tenet access. We really really tried nothing came out of it. for replicate. We had access to the ready skew and we could have we could have uh some uh modifications to the prompts and to the inference results, but the impact was rather limited and our actions were um flagged by the blue team and we had to stop our research. Finally, digital ocean we achieved full service compromise. The difference here is basically which security controls, guardrails and uh best practices were implemented while developing and imple were implemented while uh creating these uh services. Securing your Kubernetes cluster is a very difficult task and I'm not going to try to uh tell you how to do it in uh one slide and in three minutes but if we have three things that we believe could have reduced the impact of this uh vulnerability. We believe that user name spaces which is a new feature in Kubernetes is really useful to reduce the impact of container escapes. When you use username spaces your pods are going to be run using a different user name space a different user ID than the one that you have on the host. That level of isolation is very beneficial to reducing and preventing some container escapes networking. Um, as we saw in Ashure container instances, it was impossible to reach the cube API from within the container. We believe that this is a a network policy and a restriction that should be implemented in most services. So whenever possible do that restrict access from your application or for the applications running on a pod to the cube API. sidecars and other components in your cluster still need that connection, but most likely the application running there doesn't need it. Finally, permissions. Um, many of the actions that we were able to perform on paper space were because the permissions associated with the cublet were excessive. So for every principle that exists on your Kubernetes cluster, make sure that you use the principle of list privilege in order to reduce the permissions that you assign to those. Thank you so much. Um if you want to learn more about this specific research, we have a very technical blog post on the vulnerability uh which can be found in wis.io/blog. io/blog. Most uh research content is going to go out in the next months. Um so just bookmark that and visit it every now and then. Um if you have questions, I'm all ears. Thank you. Thank you very much, Andress. Um just a reminder that we do have a question thread in Slack. So, if any of you here don't ask them live or anybody watching on the live stream, please put your questions there and uh we'll go ahead and ask them. Uh if you're here and have questions, uh please raise your hand and we'll uh uh try to get to you. So, hey, I understand if you don't want to name names, but was there anybody that was as strong as Azure in this? And was there anybody else that caught you in, you know, in any of those categories there? Yeah, that that's a a good question. Um we saw other uh use cases um case studies which were similar to Ashure maybe not as good and uh no other cloud service provider detected us doing our research. Right. Uh so what sort of takeaways would you have for users of these services and ways that we can try to defend ourselves against this? Is it just trying to vet the providers to make sure that they have these controls in depth? Like what what would be the takeaways for our uh users? That's a really good question. um because you don't really know how these services are going to be built and uh they might have a really large investment in cyber security and uh we saw that for example in some cloud service providers that have two or three different offerings for AI services and one was very much vulnerable and the other one was really well built so it's difficult to really know what's behind the curtains. Other questions in the room. Um I guess sort of one final question for me. Um so the underlying uh container toolkit vulnerability that you exploited like how long did it take these did you observe it for providers to patch that? The their patch was uh actually pretty fast in uh all these case studies. It took them less than two days to apply the patch, which is amazing. Really, really well done in these cases. We didn't track all of the patching cycle for everyone. Uh, but in these three cases, really well done, right? Uh, we had a question from the Slack uh from Jay. Do you think cross tenant workloads should be placed on separate Kubernetes clusters? Oo, good question. So we have a set of recommendations. Uh we as whis has a set of recommendations and a framework on how to do exactly that. The the name for that is uh peach. So basically if you want to uh learn more about how to run different u workloads from different tenants you should follow the peach framework. Uh we believe as we research believe that um containers are not strong isolation. So you could think about using microVMs such as firecracker. Yeah. Great. With that uh out of time so please give one more round of applause to Andress.